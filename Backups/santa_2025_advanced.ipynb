{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e974d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and seed set.\n"
     ]
    }
   ],
   "source": [
    "## 2. Setup\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from decimal import Decimal, getcontext\n",
    "from shapely import affinity\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "from shapely.strtree import STRtree\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seed\n",
    "SEED = 2025\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# High precision for coordinates\n",
    "getcontext().prec = 50\n",
    "\n",
    "print(\"Libraries imported and seed set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde65439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SA parameter normalization helper\n",
    "def normalize_sa_params(params: dict):\n",
    "    \"\"\"Return canonical SA parameters with robust fallbacks.\n",
    "    Supports multiple naming conventions to avoid KeyError.\n",
    "    Keys mapped: T0, Tmin, move_scale, rot_scale, compression, iterations.\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    T0 = (params.get('initial_temp') or params.get('T0') or params.get('start_temp') or 1.0)\n",
    "    Tmin = (params.get('min_temp') or params.get('Tmin') or params.get('final_temp') or params.get('end_temp') or 0.001)\n",
    "    move_scale = (params.get('move_scale') or params.get('move') or 0.5)\n",
    "    rot_scale = (params.get('rot_scale') or params.get('rotation_scale') or params.get('rotate_scale') or 30.0)\n",
    "    compression = (params.get('compression') or params.get('compress') or 0.05)\n",
    "    iterations = (params.get('iterations') or params.get('iters') or params.get('n_iterations') or 50000)\n",
    "    base_seed = params.get('base_seed', 2025)\n",
    "    return {\n",
    "        'T0': float(T0),\n",
    "        'Tmin': float(Tmin),\n",
    "        'move_scale': float(move_scale),\n",
    "        'rot_scale': float(rot_scale),\n",
    "        'compression': float(compression),\n",
    "        'iterations': int(iterations),\n",
    "        'base_seed': int(base_seed)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a8dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded: {'hybrid_search': {'enabled': True, 'switch_n': 25}, 'dynamic_beam_width': {'enabled': True, 'base_width': 12, 'critical_n_multiplier': 2.0}, 'beam_diversity': {'enabled': True, 'max_children_per_parent': 3}, 'multi_start': {'enabled': True, 'n_starts': 3}, 'optimization': {'adaptive_temperature': True, 'smart_move_selection': True, 'batch_moves': False, 'boundary_bias_strength': 0.7, 'reheating': True, 'compaction_pass': True}, 'post_processing': {'polishing': True, 'polishing_iterations': 2000}, 'heuristics': {'corner_filling': True, 'symmetry_breaking': True, 'boundary_alignment': True}, 'validation': {'strict_boundary': True, 'collision_check': True}, 'debug': {'verbose_logging': True, 'plot_convergence': True}}\n",
      "SA Parameters loaded.\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Advanced Features\n",
    "CONFIG = {\n",
    "    # Hybrid Search Approach\n",
    "    'hybrid_search': {\n",
    "        'enabled': True,\n",
    "        'switch_n': 25,  # Use forward building for N <= 25\n",
    "    },\n",
    "    \n",
    "    # Dynamic Beam Width\n",
    "    'dynamic_beam_width': {\n",
    "        'enabled': True,\n",
    "        'base_width': 12,\n",
    "        'critical_n_multiplier': 2.0, # Wider for critical sizes\n",
    "    },\n",
    "    \n",
    "    # Beam Search Diversity\n",
    "    'beam_diversity': {\n",
    "        'enabled': True,\n",
    "        'max_children_per_parent': 3, # Prevent one parent from dominating the next generation\n",
    "    },\n",
    "    \n",
    "    # Multi-Start Optimization\n",
    "    'multi_start': {\n",
    "        'enabled': True,\n",
    "        'n_starts': 3,\n",
    "    },\n",
    "    \n",
    "    # Optimization Function Enhancements\n",
    "    'optimization': {\n",
    "        'adaptive_temperature': True,\n",
    "        'smart_move_selection': True, # Bias moves toward boundary\n",
    "        'batch_moves': False, # Try moving multiple trees (complex)\n",
    "        'boundary_bias_strength': 0.7, # 70% chance to pick boundary tree\n",
    "        'reheating': True, # Restart SA if stuck\n",
    "        'compaction_pass': True, # Pure translation pass to close gaps\n",
    "    },\n",
    "    \n",
    "    # Post-Processing & Polishing\n",
    "    'post_processing': {\n",
    "        'polishing': True, # Run a low-temp refinement phase at the end\n",
    "        'polishing_iterations': 2000,\n",
    "    },\n",
    "    \n",
    "    # Geometric & Heuristic Improvements\n",
    "    'heuristics': {\n",
    "        'corner_filling': True,\n",
    "        'symmetry_breaking': True,\n",
    "        'boundary_alignment': True,\n",
    "    },\n",
    "    \n",
    "    # Validation\n",
    "    'validation': {\n",
    "        'strict_boundary': True,\n",
    "        'collision_check': True,\n",
    "    },\n",
    "    \n",
    "    # Debugging\n",
    "    'debug': {\n",
    "        'verbose_logging': True,\n",
    "        'plot_convergence': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Bucketed SA Parameters\n",
    "SA_PARAMS = {\n",
    "    'large': { # N >= 150\n",
    "        'iterations': 1000000, # 1M+\n",
    "        'step_size': 0.5,\n",
    "        'angle_step': 5.0, # Restricted rotation\n",
    "        'initial_temp': 1.0,\n",
    "        'final_temp': 1e-6,\n",
    "        'compression': 0.2, # Strong compression\n",
    "    },\n",
    "    'medium': { # 50 <= N < 150\n",
    "        'iterations': 500000,\n",
    "        'step_size': 0.8,\n",
    "        'angle_step': 15.0,\n",
    "        'initial_temp': 1.5,\n",
    "        'final_temp': 1e-6,\n",
    "        'compression': 0.1,\n",
    "    },\n",
    "    'small': { # N < 50\n",
    "        'iterations': 200000,\n",
    "        'step_size': 1.0,\n",
    "        'angle_step': 360.0, # Full rotation\n",
    "        'initial_temp': 2.0,\n",
    "        'final_temp': 1e-6,\n",
    "        'compression': 0.05,\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_sa_params(n):\n",
    "    if n >= 150:\n",
    "        return SA_PARAMS['large']\n",
    "    elif n >= 50:\n",
    "        return SA_PARAMS['medium']\n",
    "    else:\n",
    "        return SA_PARAMS['small']\n",
    "\n",
    "print(\"Configuration loaded:\", CONFIG)\n",
    "print(\"SA Parameters loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1181a79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Submission Shape: (20100, 4)\n",
      "      id                     x                    y                   deg\n",
      "0  001_0  s-42.622673518577642  s54.876895664839893   s45.000000000000000\n",
      "1  002_0    s0.543739444537423   s2.200906498783374   s23.629382601527492\n",
      "2  002_1    s0.851933381526672   s2.723825202203245  s203.629391959431871\n",
      "3  003_0    s1.912714425007637   s1.070285672148608   s66.370480999999998\n",
      "4  003_1    s1.320371765357552   s0.974746819501626  s155.134174000000002\n"
     ]
    }
   ],
   "source": [
    "## 3. Load Data\n",
    "\n",
    "# Load the sample submission to understand the required output format\n",
    "try:\n",
    "    sample_sub = pd.read_csv('test.csv')\n",
    "    print(\"Sample Submission Shape:\", sample_sub.shape)\n",
    "    print(sample_sub.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Sample submission not found, proceeding without it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02218ccb",
   "metadata": {},
   "source": [
    "## 4. Baseline Strategy\n",
    "\n",
    "We define the `GreedyPacker` class here. It encapsulates the logic for placing a single tree into an existing configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d7992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPacker:\n",
    "    def __init__(self, n_trials=100, step_size=0.2, fine_step=0.02):\n",
    "        self.n_trials = n_trials\n",
    "        self.step_size = step_size\n",
    "        self.fine_step = fine_step\n",
    "\n",
    "    def _generate_weighted_angle(self):\n",
    "        \"\"\"\n",
    "        Generates a random angle with a distribution weighted by abs(sin(2*angle)).\n",
    "        This helps place more trees in corners (diagonals).\n",
    "        \"\"\"\n",
    "        if not CONFIG['heuristics']['corner_filling']:\n",
    "            return random.uniform(0, 2 * math.pi)\n",
    "            \n",
    "        while True:\n",
    "            angle = random.uniform(0, 2 * math.pi)\n",
    "            if random.uniform(0, 1) < abs(math.sin(2 * angle)):\n",
    "                return angle\n",
    "\n",
    "    def place_next_tree(self, existing_trees, tree_class):\n",
    "        \"\"\"Finds the best position for the next tree given existing trees.\"\"\"\n",
    "        if not existing_trees:\n",
    "            return tree_class(0, 0, 0)\n",
    "\n",
    "        existing_polys = [t.polygon for t in existing_trees]\n",
    "        tree_index = STRtree(existing_polys)\n",
    "        \n",
    "        # Calculate current bounds and center\n",
    "        minx, miny, maxx, maxy = unary_union(existing_polys).bounds\n",
    "        center_x = (minx + maxx) / 2\n",
    "        center_y = (miny + maxy) / 2\n",
    "        \n",
    "        best_tree = None\n",
    "        min_metric = float('inf')\n",
    "\n",
    "        for _ in range(self.n_trials):\n",
    "            # Random angle for the tree itself\n",
    "            angle = random.uniform(0, 360)\n",
    "            \n",
    "            # Weighted approach angle (bias towards diagonals)\n",
    "            approach_angle = self._generate_weighted_angle()\n",
    "            vx, vy = math.cos(approach_angle), math.sin(approach_angle)\n",
    "            \n",
    "            # Start far away\n",
    "            radius = max(maxx - minx, maxy - miny) + 10.0\n",
    "            candidate = tree_class(0, 0, angle)\n",
    "  Backups.optimization_utils
    "            # Move in\n",
    "            current_r = radius\n",
    "Backups.optimization_utilsion = False\n",
    "            \n",
    "            # Coarse search\n",
    "            while current_r > 0:\n",
    "                px, py = center_x + current_r * vx, center_y + current_r * vy\n",
    "                candidate.update_position(px, py, angle)\n",
    "                \n",
    "                query_indices = tree_index.query(candidate.polygon)\n",
    "                if any(candidate.polygon.intersects(existing_polys[i]) for i in query_indices):\n",
    "                    collision = True\n",
    "                    break\n",
    "                current_r -= self.step_size\n",
    "            \n",
    "            # Fine tune\n",
    "            if collision:\n",
    "                current_r += self.step_size\n",
    "                while True:\n",
    "                    current_r -= self.fine_step\n",
    "                    px, py = center_x + current_r * vx, center_y + current_r * vy\n",
    "                    candidate.update_position(px, py, angle)\n",
    "                    \n",
    "                    query_indices = tree_index.query(candidate.polygon)\n",
    "                    if any(candidate.polygon.intersects(existing_polys[i]) for i in query_indices):\n",
    "                        # Collision found, step back once and stop\n",
    "                        current_r += self.fine_step\n",
    "                        px, py = center_x + current_r * vx, center_y + current_r * vy\n",
    "                        candidate.update_position(px, py, angle)\n",
    "                        break\n",
    "            else:\n",
    "                candidate.update_position(center_x, center_y, angle)\n",
    "\n",
    "            # Metric: Minimize the side length of the new bounding box\n",
    "            t_minx, t_miny, t_maxx, t_maxy = candidate.polygon.bounds\n",
    "            new_minx = min(minx, t_minx)\n",
    "            new_miny = min(miny, t_miny)\n",
    "            new_maxx = max(maxx, t_maxx)\n",
    "            new_maxy = max(maxy, t_maxy)\n",
    "            \n",
    "            new_side = max(new_maxx - new_minx, new_maxy - new_miny)\n",
    "            \n",
    "            # Tie-breaker: distance to center\n",
    "            dist_sq = (px - center_x)**2 + (py - center_y)**2\n",
    "            \n",
    "            metric = new_side + (dist_sq * 1e-6)\n",
    "            \n",
    "            if metric < min_metric:\n",
    "                min_metric = metric\n",
    "                best_tree = tree_class(px, py, angle)\n",
    "                \n",
    "        return best_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df33791",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Module\n",
    "\n",
    "Here we define the geometric features of the problem: the `ChristmasTree` class and helper functions for bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6609d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChristmasTree:\n",
    "    \"\"\"Represents a single, rotatable Christmas tree.\"\"\"\n",
    "    def __init__(self, center_x=0, center_y=0, angle=0):\n",
    "        self.center_x = float(center_x)\n",
    "        self.center_y = float(center_y)\n",
    "        self.angle = float(angle)\n",
    "        self.polygon = self._create_polygon()\n",
    "\n",
    "    def _create_polygon(self):\n",
    "        # Tree dimensions\n",
    "        coords = [\n",
    "            (0.0, 0.8), (0.125, 0.5), (0.0625, 0.5), (0.2, 0.25), (0.1, 0.25),\n",
    "            (0.35, 0.0), (0.075, 0.0), (0.075, -0.2), (-0.075, -0.2), (-0.075, 0.0),\n",
    "            (-0.35, 0.0), (-0.1, 0.25), (-0.2, 0.25), (-0.0625, 0.5), (-0.125, 0.5)\n",
    "        ]\n",
    "        poly = Polygon(coords)\n",
    "        rotated = affinity.rotate(poly, self.angle, origin=(0, 0))\n",
    "        return affinity.translate(rotated, xoff=self.center_x, yoff=self.center_y)\n",
    "\n",
    "    def update_position(self, x, y, angle):\n",
    "        self.center_x = x\n",
    "        self.center_y = y\n",
    "        self.angle = angle\n",
    "        self.polygon = self._create_polygon()\n",
    "\n",
    "def get_bounds(trees):\n",
    "    if not trees: return 0\n",
    "    minx, miny, maxx, maxy = unary_union([t.polygon for t in trees]).bounds\n",
    "    return max(maxx - minx, maxy - miny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5699a009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Growing Trees optimization engine from optimization_utils.py\n",
      "Added collision validation function\n"
     ]
    }
   ],
   "source": [
    "# --- NUMBA ACCELERATED GEOMETRY KERNEL ---\n",
    "# Refactored to optimization_utils.py for performance and caching\n",
    "import importlib\n",
    "import optimization_utils\n",
    "importlib.reload(optimization_utils)\n",
    "\n",
    "from optimization_utils import (\n",
    "    get_poly, get_bbox, pip, seg_intersect, overlap, \n",
    "    check_overlap_single_cached, check_overlap_pair_cached, \n",
    "    calc_side_cached, get_global_bbox_cached, \n",
    "    find_corner_trees_cached, sa_numba_growing\n",
    ")\n",
    "\n",
    "print(\"Imported Growing Trees optimization engine from optimization_utils.py\")\n",
    "\n",
    "def validate_no_collisions(trees, verbose=False):\n",
    "    \"\"\"\n",
    "    Validates that no trees overlap. Returns (is_valid, collision_count).\n",
    "    Uses Shapely intersection for accurate validation.\n",
    "    \"\"\"\n",
    "    n = len(trees)\n",
    "    if n <= 1:\n",
    "        return True, 0\n",
    "    \n",
    "    collision_count = 0\n",
    "    collision_pairs = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if trees[i].polygon.intersects(trees[j].polygon):\n",
    "                collision_count += 1\n",
    "                if verbose and collision_count <= 5:  # Only show first 5\n",
    "                    area = trees[i].polygon.intersection(trees[j].polygon).area\n",
    "                    collision_pairs.append((i, j, area))\n",
    "    \n",
    "    if verbose and collision_pairs:\n",
    "        print(f\"⚠️  Found {collision_count} collision(s):\")\n",
    "        for i, j, area in collision_pairs:\n",
    "            print(f\"   Trees {i} and {j} overlap (area={area:.6e})\")\n",
    "    \n",
    "    return collision_count == 0, collision_count\n",
    "\n",
    "print(\"Added collision validation function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52e10be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test.csv\n",
      "Score of test.csv: 74.6568\n",
      "Score of test.csv: 74.6568\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the existing test.csv to verify the score\n",
    "import pandas as pd\n",
    "try:\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print(\"Loaded test.csv\")\n",
    "    \n",
    "    # Parse the 's' prefix\n",
    "    def parse_s(val):\n",
    "        return float(str(val).replace('s', ''))\n",
    "    \n",
    "    test_df['x'] = test_df['x'].apply(parse_s)\n",
    "    test_df['y'] = test_df['y'].apply(parse_s)\n",
    "    test_df['deg'] = test_df['deg'].apply(parse_s)\n",
    "    \n",
    "    # Reconstruct trees and calculate score\n",
    "    total_test_score = 0\n",
    "    for n in range(1, 201):\n",
    "        # Get rows for this N\n",
    "        # The ID format is N_i, e.g., 001_0\n",
    "        # We need to filter by the prefix\n",
    "        prefix = f\"{n:03d}_\"\n",
    "        rows = test_df[test_df['id'].str.startswith(prefix)]\n",
    "        \n",
    "        if len(rows) != n:\n",
    "            print(f\"Warning: N={n} has {len(rows)} rows, expected {n}\")\n",
    "            continue\n",
    "            \n",
    "        trees = []\n",
    "        for _, row in rows.iterrows():\n",
    "            trees.append(ChristmasTree(row['x'], row['y'], row['deg']))\n",
    "            \n",
    "        side = get_bounds(trees)\n",
    "        score_n = (side ** 2) / n\n",
    "        total_test_score += score_n\n",
    "        \n",
    "    print(f\"Score of test.csv: {total_test_score:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not evaluate test.csv: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b775fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the main processing cell first to populate variables\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Compare all_solutions dict vs submission_rows scoring\n",
    "# This helps identify why scores might differ\n",
    "\n",
    "if 'all_solutions' in globals() and 'submission_rows' in globals():\n",
    "    print(\"Comparing scoring methods...\\n\")\n",
    "    \n",
    "    # Method 1: all_solutions dict\n",
    "    score_method1 = 0\n",
    "    for n, trees in all_solutions.items():\n",
    "        side = get_bounds(trees)\n",
    "        score_method1 += (side ** 2) / n\n",
    "    \n",
    "    # Method 2: submission_rows\n",
    "    score_method2 = 0\n",
    "    configs = {}\n",
    "    for row in submission_rows:\n",
    "        n = int(row[0].split('_')[0])\n",
    "        if n not in configs:\n",
    "            configs[n] = []\n",
    "        x = float(row[1].replace('s', ''))\n",
    "        y = float(row[2].replace('s', ''))\n",
    "        deg = float(row[3].replace('s', ''))\n",
    "        configs[n].append(ChristmasTree(x, y, deg))\n",
    "    \n",
    "    for n, trees in configs.items():\n",
    "        side = get_bounds(trees)\n",
    "        score_method2 += (side ** 2) / n\n",
    "    \n",
    "    print(f\"Method 1 (all_solutions dict): {score_method1:.6f} ({len(all_solutions)} configs)\")\n",
    "    print(f\"Method 2 (submission_rows):    {score_method2:.6f} ({len(configs)} configs)\")\n",
    "    print(f\"Difference: {abs(score_method1 - score_method2):.6f}\")\n",
    "    \n",
    "    if len(all_solutions) != len(configs):\n",
    "        print(f\"\\n⚠️ WARNING: Config count mismatch!\")\n",
    "        missing_in_dict = set(configs.keys()) - set(all_solutions.keys())\n",
    "        missing_in_rows = set(all_solutions.keys()) - set(configs.keys())\n",
    "        if missing_in_dict:\n",
    "            print(f\"  Missing in all_solutions: {sorted(missing_in_dict)}\")\n",
    "        if missing_in_rows:\n",
    "            print(f\"  Missing in submission_rows: {sorted(missing_in_rows)}\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Both methods have same number of configurations\")\n",
    "else:\n",
    "    print(\"Run the main processing cell first to populate variables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e333f7c",
   "metadata": {},
   "source": [
    "## 6. Model Training (Optional)\n",
    "\n",
    "For this geometric packing problem, standard supervised learning is less applicable than search algorithms. However, one could train a model to predict the optimal *order* of placement or the optimal *angle* given the current boundary shape. We skip this for the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8dfea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for ML model training\n",
    "# model = LGBMRegressor(...)\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a7494",
   "metadata": {},
   "source": [
    "## 7. Optimization Strategy\n",
    "\n",
    "We define a modular optimization function. Currently, it's a placeholder for a more advanced local search (e.g., trying to wiggle trees after placement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a6546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_packing(trees):\n",
    "    \"\"\"Centers the packing at (0,0) based on the bounding box center.\"\"\"\n",
    "    if not trees: return trees\n",
    "    minx, miny, maxx, maxy = unary_union([t.polygon for t in trees]).bounds\n",
    "    cx = (minx + maxx) / 2\n",
    "    cy = (miny + maxy) / 2\n",
    "    \n",
    "    # If already centered (small epsilon), skip\n",
    "    if abs(cx) < 1e-9 and abs(cy) < 1e-9:\n",
    "        return trees\n",
    "        \n",
    "    for t in trees:\n",
    "        t.update_position(t.center_x - cx, t.center_y - cy, t.angle)\n",
    "    return trees\n",
    "\n",
    "def optimize_packing(trees, params, target_side=None):\n",
    "    \"\"\"\n",
    "    Simulated Annealing with Growing Trees Strategy (Numba Accelerated).\n",
    "    Uses sa_numba_growing: starts at 90% scale, grows to 100% during optimization.\n",
    "    \"\"\"\n",
    "    if not trees: return trees\n",
    "    \n",
    "    n = len(trees)\n",
    "    \n",
    "    # Extract data for Numba\n",
    "    xs = np.array([t.center_x for t in trees], dtype=np.float64)\n",
    "    ys = np.array([t.center_y for t in trees], dtype=np.float64)\n",
    "    angs = np.array([t.angle for t in trees], dtype=np.float64)\n",
    "    \n",
    "    # Parameters\n",
    "    iterations = params.get('iterations', 10000)\n",
    "    T0 = params.get('initial_temp', 1.0)\n",
    "    Tmin = params.get('final_temp', 1e-6)\n",
    "    move_scale = params.get('step_size', 0.5)\n",
    "    rot_scale = params.get('angle_step', 10.0)\n",
    "    compression = params.get('compression', 0.0)\n",
    "    \n",
    "    # Generate a random seed for this run\n",
    "    seed = np.random.randint(0, 1000000)\n",
    "    \n",
    "    # Use Growing Trees optimization (sa_numba_growing)\n",
    "    best_xs, best_ys, best_angs, best_side = sa_numba_growing(\n",
    "        xs, ys, angs, n, iterations, T0, Tmin, move_scale, rot_scale, seed, compression\n",
    "    )\n",
    "    \n",
    "    # Update trees\n",
    "    for i in range(n):\n",
    "        trees[i].update_position(best_xs[i], best_ys[i], best_angs[i])\n",
    "        \n",
    "    return center_packing(trees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb5694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lattice(n=200):\n",
    "    \"\"\"\n",
    "    Generates a structured lattice packing for N=200.\n",
    "    Uses a brick/triangular layout with alternating orientations.\n",
    "    \"\"\"\n",
    "    print(f\"Generating Lattice Initialization for N={n}...\")\n",
    "    \n",
    "    # Approximate dimensions of the tree bounding box\n",
    "    # Tree is roughly 0.5 wide and 1.0 tall (centered at 0,0)\n",
    "    # But effective packing width/height is smaller due to shape\n",
    "    dx = 0.25  # Horizontal spacing\n",
    "    dy = 0.45  # Vertical spacing\n",
    "    \n",
    "    # Calculate grid size\n",
    "    # We want a roughly square aspect ratio\n",
    "    cols = int(math.sqrt(n * (dy/dx)))\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    trees = []\n",
    "    count = 0\n",
    "    \n",
    "    # Center the grid\n",
    "    start_x = -((cols - 1) * dx) / 2\n",
    "    start_y = -((rows - 1) * dy) / 2\n",
    "    \n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if count >= n: break\n",
    "            \n",
    "            x = start_x + c * dx\n",
    "            y = start_y + r * dy\n",
    "            \n",
    "            # Offset every other row for triangular/brick pattern\n",
    "            if r % 2 == 1:\n",
    "                x += dx / 2\n",
    "                \n",
    "            # Alternating orientation (Checkerboard or Row-based)\n",
    "            # Row-based alternation:\n",
    "            angle = 0 if r % 2 == 0 else 180\n",
    "            \n",
    "            # Checkerboard alternation (optional, uncomment to try):\n",
    "            # angle = 0 if (r + c) % 2 == 0 else 180\n",
    "            \n",
    "            trees.append(ChristmasTree(x, y, angle))\n",
    "            count += 1\n",
    "            \n",
    "    return center_packing(trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23543b68",
   "metadata": {},
   "source": [
    "## 8. Submission Generation\n",
    "\n",
    "This section runs the full pipeline and generates the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "492ce042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline from test.csv...\n",
      "Loaded 200 configurations from test.csv\n",
      "Running Optimized Advanced Search (200 -> 1) on 12 cores.\n",
      "Beam width: 12, Branch factor: 5\n",
      "\n",
      "Initializing with Lattice Generator for N=200.\n",
      "Generating Lattice Initialization for N=200...\n",
      "Optimizing initial lattice...\n",
      "Loaded 200 configurations from test.csv\n",
      "Running Optimized Advanced Search (200 -> 1) on 12 cores.\n",
      "Beam width: 12, Branch factor: 5\n",
      "\n",
      "Initializing with Lattice Generator for N=200.\n",
      "Generating Lattice Initialization for N=200...\n",
      "Optimizing initial lattice...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNoAAAHWCAYAAAChceSWAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAA+OJJREFUeJzsnQd4FNXXxl9IQggQei8CCgoiCFIEUVFEsYK9dz5QVGyof1BBwV7BgqIIKlYsiCBWLKA0UURQiqD03kkCpH/PuZNZJpvdZDeZO7Mz+/584lbO3Cm7e+ed95xTLj8/Px+EEEIIIYQQQgghhJAyUb5s/5wQQgghhBBCCCGEECJQaCOEEEIIIYQQQgghxAYotBFCCCGEEEIIIYQQYgMU2gghhBBCCCGEEEIIsQEKbYQQQgghhBBCCCGE2ACFNkIIIYQQQgghhBBCbIBCGyGEEEIIIYQQQgghNkChjRBCCCGEEEIIIYQQG6DQRgghhBBCCCGEEEKIDVBoI4QQn3H22Wejf//+iDXWrFmDcuXK4a233oLXyM7ORpMmTfDKK6+4PRRCCCFEO9dffz2aNWtma0z5/Zd5gMwHCCHEz1BoI6QUE4TffvvN7aGQMOzfvx8PP/wwfvrpJ3jtuKpYsSI2btxY5PVTTjkFxxxzTESxZs+ejW+//Rb/+9//1GOZJEvskv4iEb9EZHJTJNu8eTOGDBmCU089FampqWrc4fazCGMjRozA4YcfjuTkZHX76KOPIicnp8iJRHHbxdwfSUlJuPvuu/HYY4/h4MGDjqwvIYQQb/Pvv//ipptuUr9B8htftWpVdO/eHS+88AIOHDgAv/L4449jypQpiCWWLFmCiy++GE2bNlX7olGjRjj99NPx0ksvwY/s2bMHdevWVXOZTz75pNBrCxYswG233YY2bdqgcuXKOOyww3DppZfin3/+iWoZM2bMQM+ePVGtWjU1L+vYsSMmTZpU5H1Tp07Fcccdp7a7LOuhhx4qMh+LJqY8vvrqq9GyZUu1fjJPJiTWSHR7AIQQYrfQJgKL4LUf3szMTDz55JNlmvQ988wzOO2009CiRQv1ePTo0UhPTw+8/uWXX+KDDz7AqFGjULt27cDzJ5xwQkRCm/wbEafcYMWKFXjqqafUxKpt27aYO3du2PfKBOzjjz/GjTfeiE6dOmHevHkYNmwY1q1bh9dffz3wPjkB6tWrV6F/m5+fj5tvvlmJlDIRN7nhhhuU0Pf++++ruIQQQkg4pk+fjksuuURd7Ln22mvVBbOsrCz88ssvuPfee/H3338X+j3ym9Amotb5559f6PlrrrkGl19+udomTjJnzhx1kU5EHnH8169fH+vXr1dzAxE9Bw0aBL8xfPhwNScOhcyl5MKsHJ/t2rXDli1b8PLLLysxTLZJJBd333zzTfTr10+JlbK/ExIS1DxNtquVr776Sh0HMieX+a0InnLhc9u2bXj11VdLFVP+3e+//47OnTtj586dpdo+hGgnnxASMW+++Wa+fGwWLFiQHwvk5eXl79+/3+1hxBTbt29X++ihhx7KjyXS09NLPK7at2+fn5ycnL9x48ZCr/fo0SO/TZs2JS5j69at+YmJiflvvPFG2Pc888wzalmrV6+Ocg3y1RhkLKVFlinLlvUtDfv27cvfuXOnuv/xxx+rWD/++GOR9/3666/qtWHDhhV6fvDgwfnlypXL//PPP4tdzs8//6z+/WOPPVbktXPPPTf/pJNOKtX4CSGExAf//fdffpUqVfJbtWqVv2nTpiKvr1y5Mn/06NH5scx1112X37Rp01L928qVK6t/HyucffbZ+XXq1MnfvXt3yLmTk2RkZGhfxpIlS9R8cOTIkWo+I3MmK7Nnz87PzMws9Nw///yj5qBXXXVVRPO5lJSU/Ntvv73E9x599NH5xx57bH52dnbguQceeEDNx5YtW1aqmOvWrcvPzc21ZW5KiC6YOkpIGRF3T5UqVZRT5txzz1X3xQUzZswY9bpcuRELtFizxa4ubphQaYOzZs1S7ppatWqp1AK5+rl79+5C7xWHjSzjm2++US6dlJQUvPbaa+q1//77T12ZqlmzJipVqoSuXbuqq6kmW7duRWJiYsDtZUWuFskY5GqW1XJ+5513qrpUcuVRHFJyBSwvL69Iza1nn31Wra+kRsiyzzjjDHX1SZxBjzzyCBo3bqzG2rdvX+zatavI8uVq10knnaS2kdjEzznnHHWlN9R2llQ+uTIm9+vUqYN77rkHubm5gfHIc4Ksp5n+J6mk4Yhm+0c7VkkZkXpp8r6rrroKJXH//ferdRFXW2mQ/S1W/GCHVknIv5H9dMQRR6h9LceZjEUcdibynKznzJkzA9vVdAzKPpX9IC4zWW/ZfmeddRb+/PPPEpctKZ7Lly9XaaElIdtRju+S+Pnnn9WtXDW3Io/lmAyV1mBFPqOyfldeeWWR1+Qqq7gRQh3HhBBCiPD0008rN/n48ePRoEGDIq/LnOqOO+4osX5p8BxG7stzkuInzm1Jr5N5jzi25fdN5l4y15LfYXFtPffccxHVSJMyDMWVYzCR+Z444GWuJPM6SesLTkuUOBkZGXj77bcD8wXTCR+8fJnTytwxFN26dVNzXSvvvvuuWqYsW+YD8rse7HYKhczHJE2yevXqRV6T9MpgZDldunRRc9oaNWrg5JNPVmU5gl3+ElPmTQ0bNsStt96q5s6hSn+I+0piSDyZXwkyx5IUSjkWJIbMt++7775Ccy9hx44dap4Uzp0WCjm2LrjgAjVfDYXswwoVKhR6TrIFZH2WLVtWYvyxY8eq+erIkSPVYznW5fgLZunSpepvwIAB6hzE5JZbblHvtx47kcYUZFuVL08Zg8Q2PEIJsQH5YRBhQb74ZXIlooTUPpAJxZlnnqkmCiJSiVAgAs7q1auLxJD3y4+bTKLkPe+9954SlIJ/ZEQUu+KKK9QJv9jd27dvr0Q0+dEUAU5+vMw6Un369MFnn32m/l29evXQo0cPfPTRR0WWLcKD2LNFqBPkx1zeKxMNGcuLL76oaooMHTpU1akKRsYqEw6x3g8ePFiJMVLr4cEHH8TXX3+t6oXJj+y0adOUIGPlnXfeUWKVCDSyjWSyKD/KJ554YpGJoGzn3r17qwmeTPZkjDKJNFMvZLJp2tBlgiGx5e/CCy8scR9Gsv2jGauIVzJWmcDJWC+66KISx9C8eXO17HHjxmHTpk0oTWqEbBsRdKPh//7v/1SKgaQMSEqpbNcnnniikFAlKagimLZq1SqwXR944IGAyCu1WGTC/Pzzz6uUGBGYJU5J6yHCaevWrdWxZRfmJFUm4lZkgivIhLc44U8+I/J5ClUEWib4ckzItiaEEEJCIfMdEZAiKctQGi677DJ14VMuzB1//PEqFU9+p2VuKBd7ZY4iAo7MueRCol3IvLNDhw5KDJHUPhFPZO5ovbAr8wMRjkTkMecLciEz3HrInFhqhllZu3atSmG0zkNkbitzJBGEZK4hF4O///57JWAFC1zByLxIfvv/+uuvEtdRLtRKiqvUZpX1lMcyv//hhx8C75G5oghrIrDJPFTmeHLhWy40yzzCiqQ2yjmCzNdlH0kKq+w7maPL/PC8885TKZUy55Q5mGwTK3IRXOZJv/76KyJBymbIHEXOR6JB5jZyPmEtKxIOqaMm80EpRyJzQzm/kfmnzIutF+T/+OMPdRssmMp2k39nvh5NTEI8gzavHCFxkjoq1nh57vHHHw88J9Z0sT+LLfrDDz8MPL98+fIiaY1mzI4dO+ZnZWUFnn/66afV859//nngObHwy3Nff/11oXHdeeed6nlJeTNJS0vLb968eX6zZs0C9urXXntNvU8s5cG27p49ewYeP/LII8r2LzZyK0OGDMlPSEhQlm1rKqDY8ffs2RN439ChQ9XzwVbxK664Ir9ChQr5Bw8eDIyxevXq+f379y+0nC1btuRXq1at0PPmdhYbvJUOHTqobVfa1NFIt39pxirbK9rj6t9//1V2f6t1PtLU0RNPPLHQtogkdXTRokXq8f/93/8Vet8999yjnv/hhx8Cz4Wz58v+NI8xE4kvKQjW/RUqddR8LtoUk+JSRz/99FP12jvvvFPo+bFjx6rnjznmmLBxp02bpt7zyiuvhHxdUoDk9aeeeiqq8RJCCIkP9u7dq34n+vbtW+ayCsHzGbkvzw0YMCDwXE5OTn7jxo3VnPPJJ58sMhe1/r6a843g8hHyWxr8mxoqdTS4XInMm+Q31TqHLC51NHj5sq1kriClHazIHEzWZ+3aterxmjVr1PwzuKSDmSIZqtSDlW+//Vb9e/nr1q1b/n333Zf/zTffFJr3mSm95cuXz7/ggguKzGukXIuwbds2NZc944wzCr3n5ZdfVus2YcKEwHMyZ5LnZP5hReYnshzrvN06T5HUzuB9Hmq+E4zsn8MOO0zNw637NTh1NBQyJnnv+PHjS3xv1apV82vUqKH2nZTp+OSTT/KvvPLKInNfc85pnjdY6dy5c37Xrl2jjhkMU0dJrEJHGyE2Ia4gE7GmH3XUUSq9UJxdJvKcvCYOoGDE8SVXz0wGDhyorhTKlZ1g15M4pazIe8TiLs4qE3FdSUxxWonrShBnl8S0ps7J1T153XoFTa6GyZVIscuLZd38k5REcZUFXx2Vq5mSvmAiV1cFSWuwWsXleSkEbHZy/O6779RVSHHoWZcj7jp5748//lhkO0mReisyzlDbM1pK2v6lGavEiBa5Ai5XUsWlF0k6ZfBVU9ln0WCuX7BTUZyJgvUqdTjkyrVp4ZfjQ8Yhx58c7wsXLiz234prTM4l7OxmKum6cvVaruRPnjxZXRkXl5o48GSfFtfpTdJG5Tiwfm6tmNtX9j0hhBASzL59+9StOHKcmHPKPEQcQ/JbKoXkg+eidsyRTKxOcSmvsXfvXjUPK+m3PhxmqQn5jbZmEMg8VUqgSPMCQX7LxdUkv83WOZikx4rDLdQczIo4/aSBkrjIpKyFuL1kLi3uP+mIaSLufFmOuPyDUxMl5dV0XslcVhx11vdIkwVZn+B5k8yRpJmSFZlni0tNHFzW9ZFSM4J1fcQ9J9smkgZf4nAUR52ZnhopkpoqDj1J173uuutKfL+kdcr+F7efuP7E0SeZIJLFI67HtLQ09T5zvhWq+YV0ILXOxyKNSYhXoNBGiA3Ij4VZG8xEhCexPps/zNbnQ9X+komCFREqpK5HcEqiCG3BiJAgk6lg5EfcfF0QO7h0pLSmj8pkRsQHa3rlypUrVcqnrJP1z6z9JZ2CrJgTIes6CmK1D/W8uf6yHEEmFsHLkloYwcsJtZ1F+Ai1PaOlpO0f7Vhlm8r+Lw2Sciupp6Wp1RaunkU45NiQiaLZpdREJq8ySTePneKQSamkO8g2lMmUHGeyXRYvXqwm4U4jx4lMdCXlQCZqIuZJuolMnKWmi+zbUMgk7/PPPw+kJxe3fYM/14QQQoggYougUxgINe+S377gtL9wc87S8sUXXygBTJYlv6dmyY6y/NbLhV6ps2Z2Epd6apLmab0ALHMw+f2VeUbwHEzKfgTPwUIhHSpFsJPtIWmYUrJC9pF0RzUvSMuyZU509NFHh41jzouC591S80wulgbPm0TMC66HJusjdW+D1+XII49Ur0eyPsHIfFU6z0uKbbh5Tiik46iURZFjRWqmiXAbqeAqF5+tyGMRz8yUUPN9wXXnBClxYxVuI41JiFc4ZDUhhJSacD9K4Z6PVgyxElx3Klqk3oVcWVu0aJGqFyGim4hv1smZCCdy9U+KsobCnAiUdf3NmgtSv0OEnWCsbrji4jlBtGO1uryiRSZq4gYUV9uQIUMi/nciDpV2Ql0W4UjqtEgNjRtvvFE1VZDJt6y7XO11q66GFPQ13ZqyTWTSLJ+du+66S9WOC4VcyZb6hMU1rjC3byQ1TAghhMSn0CY1qCKpB1bc76/Z6CkUoeZDkcw5S7MsE2k0JI4wqYkmdXnlYqQ4wN98880ijb6iQWqUSQ1Vsz6q3MocwqwbLMhcQsYuDalCrWc0wpKIXiK6yZ/MZ2VOLA4zaUygg1DzdlkfaSAlteZCEXyhOhLkYqKIeuJ8My8Si4gmbN++XT0nAq11bioCqTgKJWND9q8ct5Eg7xOxUOo/h2osYc6VzEYgkqERvE7ynGTjRBuTEK9AoY2QGEF+XKRAqtVdIz9CkgZXEpImJ00SQlnBzddNpNiqFKU100elc1VwIXrpPinLj7Z7ZbTIcswfUbuWVVrBqKTtr2OsJbnapBmFFDSOFElB+PTTT6NajhwbMuGT9TcdkIIUxJWJl/XYCbdt5QqobDvprmZF/r2bgpSMVwQ3a5qsrGu4/ScpCjJZlxOJcJiNTKzbihBCCLEizYHkYpm4tCQdrzjMkgTBBf0jcZRHS1mWJfMLcbJJ4y1rKqAIbWWZi0mZFdleInaJ8CTzU0lHtYo+MgcTwVCyOoIv9pYFs0i/WapDliPzBLlIJxejQ2HOi2Tebe2YKumkMkeIZI4oy5EUVrnQbZdDft26dVi1alXILq7SKM0Uq8zOq+IoE5FTzgMkHbY4F1+oxlAyb5RSMNblmQ2wzOwTcxv+9ttvhUQ1ed+GDRtU2ZZoYxLiFZg6SkiMIBMya6ciseJL+qBcaSoJEYPEBm/a7gVprS4xJW3O+uMpP7CSGidXDD/88EN1ZU/ENytSA0NiyWQqGJmcybjsQMYhV37FERXcpcm8AhctZmfJkjpQRbv9dYy1pEmYuNqki5V5RbIkZDIvk6ho6rGYQqJ0wrJiXmWVdALrZDjUdpWry8EuTZkwm7X4ikO2pQjC0dajixZJOxDXnVxdDU5LMPefTDSlW615DIVC0llkUlzSiRMhhJD4RTIC5DdTaqnJhatgJEVR6k4JMreQi1LB9W/FNWY35kVD67LEzWZ2by8O+a2X3z+r+01cUuIGDybcfCEckiYqgsobb7yhBKjgzptS3kSWL/W7gucb8lhqwxaH1DwLlU1i1qk100BlPiyOL6kRFuzIN/+9CGkyd37xxRcLxZSLjeIQs86bwiHzbJkjSZf5UPMVmcObSO02mSeJ4744pPPsZ599VuhPsgzM41Eey34RZB/KNpa5vszXipvTyPxMlm+d+5r7x3qBVbaXiK6S1SCimSAXO+UisBxf1uNG5thyLEnabrQxCfEKdLQREiPIlTC5siU/vnKVTCZY0tygOHeNiaQXfvDBB0oUuv3229UP0ttvv62urMkVyOAURvkxExFHliECknl1y+Tee+9VxWHlCuP111+vftzkR3/JkiXKvSQTKzucSjK5lB9bKf5/3HHHqbRWuWIlV+Wkxlb37t1VW/NoLfoiLMoVUbnqKdvimGOOUX9l2f46xloSUrxfUlVlPFZnVjhkcicprCIYWa8SFsexxx6rCt/KJEgmxZJWKaKtHD8y4bS6/OQ4kG0gkzmp6SbuPqlZJ8eJTEol/ULSPuQ4EXdYqKuqwchEU9xhMoZIGiLIsgWpbSLI9vnll18CLkAT2Y9yNVyOBSlMPWHCBCVAyr4KVaBajhcRVotLGzWbYsi+DlfDjRBCCBFBS9IpZb4lv3FSJ1TmITLXmDNnjhI3ZH5lIoKc1GWVW3FZiRAmTiO7kbmE1FiTTIZdu3apOZJcdI3kAqrMMeQinBSnv/LKK1UdsTFjxqj5gNRktSLzBZmLyPvlt1icaGajrHAX/eS3WZoYiaAm9VWDt6f8/su4ZQ4q8xN5v8xzRUCSOY/823AMGjRICVVyMU2EH3M/yG+/XJA2mxXIusjcSwQqcdWJwCfuvQULFqj1eOKJJ9TcT8Yhop9sC5knmvNGSUeV+XVJyFxSLnhLgy8RAWVeIUKUCFryvFzoNt12MreUZcn7imuIYG2IZmLO72Vc1ovq0vBK5vniaJPjQDIorFjXQdbVPKeQbSX07dtXzZlle4gQKHNJEVxlPiYXiK2OR6kbJ9vojDPOUHNnSamWdZJj3ZodEE1M+XyYYrFcKJVzFHN+KKnN8keI67jd9pQQL2G2JV+wYEHgOWlfLm3Mg5FW09JyOhhplX7OOecUiTlz5kzVrl1aW1epUiX/qquuyt+5c2ex/9bKv//+m3/xxRfnV69ePb9ixYr5Xbp0yf/iiy9Cvnffvn2q5bss99133w35nrS0NNUevEWLFqqNee3atfNPOOGE/GeffTbQDt1sSS/tu62EayceavuZ7+/du3d+tWrV1NiPOOKI/Ouvvz7/t99+K3E7m23PrcyZMye/Y8eOatzymrwnHNFs/7KOtaQxBG8XM5a8FupYCkWfPn3yTzvttLCvm63WZd+ZZGdn548YMSK/efPm+UlJSflNmjRR+/7gwYOF/u2WLVvU8ZeamqpimO3U5X2DBw/Ob9CggTquunfvnj937lz1urXlunm8yPoGPyfrGQny3nB/Vp566qn8Vq1aqX0k+1S2yx9//BE2rrSYr1u3bn5OTk7Y9+zZs0cdU2+88UZEYyWEEBLf/PPPP/n9+/fPb9asmfr9kN9P+Y186aWXCv3G7t+/P79fv35qbiHvufTSS/O3bdtWZA5jznm2b99eaDnRzEVlvtirV6/85OTk/Hr16uXff//9+d99952KK3Mca0yZd1oZP358fsuWLdW/ld9Y+T0PNQ9bvnx5/sknnxyYa5q/8eZ8xzoHMZF5l7wmYwvHp59+mn/iiSeqdZU/GcOtt96av2LFivzi+Oqrr/JvvPFG9X6Z48m+kPntoEGD8rdu3Vrk/RMmTMjv0KGDWk+ZQ8h2lG1k5eWXX1bxZN4k23HgwIH5u3fvjuhcQJC5tMxV5HVzOTJ3lfnY3r17A+8zt69130RKuPm4jCvS+ZQ5Dw3eZ3KecMcdd+TXr19fbc+2bduGPaf47LPP8tu3b6/Ws3HjxvkPPvhg4FyiNDHNbRLqr7g5PyFOUk7+57bYR0g8Iy4euZImV8vMq1fEOfy2/aWYrVzxlKuiwZ1USdmQ9Nqnn35apfyUtSkJIYQQQgghxJ+wRhshhPgISXUQe74IQsQ+pDaJpMBIeipFNkIIIYQQQkg4WKONEEJ8xldffeX2EHxHUlKSqsdHCCGEEEIIIcVBRxshhBBCCCGEEEIIITbAGm2EEEIIIYQQQgghhNgAHW2EEEIIIYQQQgghhNgAhTZCCCGEEEIIIYQQQmyAzRBCkJeXh02bNiE1NRXlypVzeziEEEII8QBSjSMtLQ0NGzZE+fK8lhmrcJ5HCCGEEJ1zPQptIZDJV5MmTdweBiGEEEI8yPr169G4cWO3h0HCwHkeIYQQQnTO9Si0hUCucJobr2rVqlqupG7fvh116tSJiyve8ba+8bjOXF//E2/rzPX1PzrWed++fUrAMecRJDbhPM9e4m1943Gd421943Gdub7+J97WOU/T+kY616PQFgIzjUAmX7omYAcPHlSx4+Ugj6f1jcd15vr6n3hbZ66v/9G5zkxHjG04z7OXeFvfeFzneFvfeFxnrq//ibd1ztO8viXN9fy/hQkhhBBCCCGEEEIIcQAKbYQQQgghhBBCCCGE2ACFNkIIIYQQQgghhBBCbIBCGyGEEEIIIYQQQgghNkChjRBCCCGEEEIIIYQQG6DQRgghhBBCCCGEEEKIDVBoI4QQQgghhBBCCCHEBii0EUIIIYQQQgghhBBiAxTaCCGEEEIIIYQQQgixAQpthBBCCCGEEEIIIYTYAIU2QgghhBBCCCGEEEJsgEIbIYQQQgghhBBCCCE2QKGNEEIIIYQQQgghhHiW3FwgKwsxAYU2QjTy05qf8OqCV5Gfn+/2UIhPyMrNwpWfXokX57/o9lAIIYQQQgghJCaYOhVo2hR4MQZOkyi0EWJh78G9aD2mNf5v6v/ZEm/g9IG45ctbMGvtLFviESLH0gd/fYC7vrkLy7Yvc3s4hBBCCCGEEOI6o0cDW7YYf25DoY0QC7+s+wXLdyzHxD8nKudQWUnPTle3U1dMtWF0hACrd69Wt3n5eXjwxwfdHg4hhBBCCCGEuMrChcCsWUBiInDrrW6PhkIbIYVYun2pus3Oy7bFLSRiiDD1n6lMHyW2sGbPmsD9ycsmY8HGBa6OhxBCCCGEEELcZNQo4/bSS4FGjdweDYU2QgqxdIchtAmLtiyyTWhbtWsVVuxcUeZ4hKzeYzjaKiVVUrf3/3C/yyMihBBCCCGEEHfYtAn48EPj/l13ISag0EaIBauLzU6hTZi2YlqZ4xFiOtpGnDICSeWTMOO/GeqPEEIIIYQQQuKNMWOAnBzgxBOBTp0QE1BoI6QASe00U0eFRVttFtr+odBG7BPaTml2Cm7pfIu6P/jbwcjNy3V5ZIQQQgghhBDiHNL44JVXYsvNJlBoI74VzaSDaDRsStuEtKy0wOM/t/xZ5rpq1n8/e/1s7Ny/s0zxSHxzMOcgNqdvVvebVW+G4T2Go0bFGli8dTHeWvSW28MjhBBCCCGEEEeQU+2BA4E9e4DjjgP69kXMQKGN+I7NaZtx6tunovYztTHpr0kR/zvTzSYCRmL5ROw+uBvr9623xdFWNbmqun/H13fghXkv4Lk5z+F/3/0Pg78ZjF0HdpVpGcQ/vPbba7j2s2uRk5cT8vW1e9aq28pJlVErpRZqptRUYpsgHUjTMg8JxYQQQgghhBDiVz7+GJgyxeg0+uabQEICYgYKbS4ibqet6VvdHoav+GnNT+jwWgfMXDtTiRU3fXET1u9dr7b12N/G4tbpt4YVMZbtMOqzta/fHkfXOTpknbb5G+bj61VfRy20nXvkuer2vSXv4c5v7sQ9392Dp+c8jefnPY8hM4aUen2Jf5BjdOj3Q/HO4neUm7K4tNHmNZqjXLly6r6kj7ao2QJb0rfg2TnPOjpmQgghhBBCCHGa7duBW2817j/wANCuHWIKCm0uMvb3saj/XH28sqAgqZiUifSsdPT9sC+2ZmxF27pt0bFBR+zN3IsbPr8BA6YNwMDpA/HKb6/g902/F+toO7r20UpsCxbaRKjrNr4bznrvLLy7+F31nIh2Y34dg8+Xf16s0HZPt3vwWM/HcEunW3BZm8twZdsr0f+4/uq1CX9MUF1JSXyzLWObclEK4cRgU2gT16VJhYQKePTUR9X98X+ML1QXkBBCCCGEEEL8xu23Azt2AG3bAvffj5gj0e0BxDNLti1Rtw/88ACuOOYK1Eip4faQPI2IEPsy96FacjXM+7952LBvA9qPbY/vV39f6H1ZuVmB++8tfg/7s/ejf8f+AUdb6zqtUatSLXX/z61GnbaHf3oYI2eNDPy7/tP64/Aah+Op2U9h6oqpSCiXgL9v+RtH1T6q0LLyYdRoq16xOu4/qeg3gIzxq1VfYeTMkZh4wUSbtwjxEubxJ4QTy1bvWa1um1U7JLQJ57c6Xx33G9M24ue1P6NHsx6aR0sIIYQQQgghzjNlCvDhh0aqqKSMVqiAmIOONhcxT6b3HNyjBBuzIP/MNTNdHpk3MdNwG6Y2RKWkSjiy1pF4+vSn1XPyOLVCaqHtnp2bbbjdvhiAH1b/cMjRVueQo23h5oW4+YubAyLb8JOH4+yWZ6ui9CdOOFGJbEJufi4e+umhImMyl1W+XOiP2iOnPqJuxSFn7XhK4o9l25cVEWiFd/58J+CgtKaOWklOTMZFrS9S999f8r5DIyaEEEIIIYQQ59i1y2iAINx7L9CxI2ISCm0uYu1I+cL8F/D83OfR6uVWOOXtU/DR3x+5OjYvIimjQv0q9QPP3dr5Vky/cjr+vPlPNKnWpJCIIel52XnZxvu+vBU79u9Q94+qdRSOrXdsQNh4feHrKIdyePWcVzHi1BF494J3cUSNI1QccRGNOXuMeu+kvycVqa1lCm1mPa1gOjbsiAtaXaBiiWuOxC+hHG3itrxx6o245rNr8N/u/0Kmjppc0fYKdfvJsk+Ua1OEZKkpGC4NlRBCCCGEEEK8Ql6eIbJt2QK0agU8VNTnEjNQaHMRa3qYOKQGfzsYaVlG18DR80a7ODJvIsXghXpV6gWeE4FLHGhSLN50lZnb3eoaWr5jeUDAqFyhskodbVK1SaAG1seXfIybO92sHkuK7zdXf4PB3QZjTr85qhj95cdcHuj8GI2jTRhxygh1++myTwNdJUPxz85/MOyHYYH1JP4X2kQwM4WyD//68FDqaAih7dRmp6Je5Xqqi+0nSz/B6e+cjq7ju7IGJCGEEEIIIcTzPPAA8NFHRpfRCROAihURs1BocxHzZLrPUX2UYyqxfCKGnjgUSeWTMHfDXPyx+Q+3h+jJ1FERG0IRLLSFqoPVunbrwP0BHQco55qIahcdbaTlmRxR8wg8e8azge6kIpZJnbYv/vmiULMF07VYnNDWtl5bnNb8NDWeV397NeR7xCnXfUJ3PPrzo6p5AvEfpthbSAy2uF7fWvSWapgQTmhLKJ+gGm0I4oCTzrvCL+t+0T52QgghhBBCCNHFK68ATz5p3B83DujWDTENhTYXMU+muzfpjl/7/4qltyzF46c9HhB1xiwwUhKDT7hJ8amjpRHaaqbUVLemcCY8ePKDWHX7KpzS7JQSly314Mz3mU0urMsoTmgTBnUZpG7HLRyHA9kHsPvAbjwz+xk8PftpjF84Hj0n9gyktmZkZcApxGH32KzHlEuK6CMtM001xjAJdYyu3LVS3VZNrooaFUM3TpFutua/E+Fe+Hv731rHTgghhBBCCCG6ePddYJBxuoyRI4Hrr0fMQ6HNRawiTKeGndCyVstAXTGzqPlnyz5Dh9c6oPkLzbF462JXx+vFGm1WxDUYTsQYe85YVZftmnbXlHr5FRMrFhJF5dZMTy1JaDv3yHPRtFpTJWi9OP9FnPr2qbhvxn3434z/4f+m/V8hocua8pqblxtw8ulA6gZKOqw0hCDOuNlKcl2Kmy1czb8ujbood2TLmi3x5ZVfBlKOrZ12CSGEEEIIIcQLvPMOcN11Rn22m28GHixcqSlmodDmIuHcTuJwa1evHQ7kHMCFH12IRVsWYe3etejxVg9V3JyUkDpqqdFmxdzOphBmFTEuaH0BFt28CMfWN5oglAZT/AhVA84U+cIhaX9S600Y8v0Q/Ln1T+XMu6rtVTihyQm4sf2N+L8O/1dk3Ld/dTsaj2qM7//7HjowawZ+vPRjpiA6VJ8t3DFqEipt1HoMzrh2BlbctgK9Du+l3G9S403ENkIIIYQQQgjxChMnHhLZbroJGDNGznfgCSi0uUg4t5OcLN/e5fbAa+JwE7Flz8E96PVOL0z8c6JyMpEwzRBKkTpakuMsEoo0W7Ck+0YSv1+HfgFXXOOqjTHrhll498J3MfvG2RjfdzyqVaxWZNx/bf9LCSkP/aSn5YpVLLzz6ztDCj+k7CzbXlhoC3WMVk6qrG6bV29eYjz5DpG/NnXaqMd/bfvL5hETQogzPPnkk+r77M477ww8d/DgQdx6662oVasWqlSpgosuughbt+pzdxNCCCHEWd5+20gRlVNqcbJJjbbyHlKvjCI+xBWKq991Y4cblbAiNcPkLz0rHX0/7IsfVv+A66Zch2fmPKPSHOXkWxxcF7S6QLmiIkFEOllmuPSzcGzP2I6UpBRUqVAFsbgtzULxJTnaQglhJTnOIqGsQp50On3m9GdUQ4VXz3kVzWs0Lza+9f7s9bPx89qfcVLTk2An1mX9vvl33Pj5japDrnS/FOFXUhIHdhqIe7rdY+ty443lO4tPHZVGG5e2uRRvLnoTHep3iDjuMXWPUY1V/t7GOm2EEO+xYMECvPbaa2jXrl2h5++66y5Mnz4dH3/8MapVq4bbbrsNF154IWbPnu3aWAkhhBBin8h2ww2HRDZxsnlJZBMotLmIeRIdSuQREezioy8OPBZxS2oujZo3Ck/Nfko5VKR+l8kb572Bfsf1K3Z5UkR/8LeDMf6P8coFlZyQrFxUY8451HQhHCKqtHiphXLTSIplrCE1zHLzDZdf3cp1oxLCZPtHKzpGFB/RO+Zu63Kb+oskfvD9J355QpvQJvXjJH357T/fLvIeOQ4PZh/EgFYDbF12PBHO0WZ1vb589svqO6H3Eb0jjms62tgQgRDiNdLT03HVVVdh3LhxePTRRwPP7927F+PHj8f777+Pnj17qufefPNNtG7dGvPmzUPXrl1dHDUhhBBCysKMGUC/fobINnAg8PLL3hPZYkZoGzNmDJ555hls2bIFxx57LF566SV06dIl5HtlwjVx4kT89ZeRCtWxY0c8/vjjhd4vTqWHHnpIvXfPnj3o3r07Xn31VbRsaTQbiBUi7UhpkpyYjCEnDsFNHW9SHUlX7FyBlTtXYv7G+XhvyXvFCm1S5+2KT68oVHQ9MzcTr/72qorZpFqTYpf955Y/sS9zX6DzYazWZ5PuoRUSKkRUQy0gtNmU6B2oAVcgjlhFMB1CXrAr76tVX6n93L5+e9iFuSxxra3buw77svahbd22qsuqbOtZa2dh2I/D8NDMh5B1MAsjzxiJeOWRmY+orrFvnf8WejY3Tv4iQVyBq3atUvcbpjbEprRNRY5R2feVkirh7JZnRzUmcbQJTB0lhHgNSQ0955xz0KtXr0JC2++//47s7Gz1vEmrVq1w2GGHYe7cuSGFtszMTPVnsm/fPnWbl5en/uxGYsrvs47YsUi8rW88rnO8rW88rjPX1/94YZ1XrgQuuaQccnPL4eqr8/HiiwXn1Xmxs76RxnNdaJs0aRLuvvtujB07FscffzxGjx6N3r17Y8WKFahbt6gz6aeffsIVV1yBE044ARUrVsRTTz2FM844A3///TcaNWqk3vP000/jxRdfxNtvv43mzZtj2LBhKubSpUvVv/Gq0GZSI6UGHjzZaLexZs8a1ZF05tqZqkZZcMdNWcYL815QBfblhF5O5N/s+6bqsHnJx5fg53U/493F72LoSUOLXaYpsIWq0SVprUNmDFFdP9/q+xYqVzBqScVSfbbihDA76rPZUaMt0vjWuOay6lSqg+37t+Pijy7G8B7DccUxVyApIanMyzTjS+24UM7Hk5uerMb1wA8P4LH5j2HjwY3qfSIKxRsf/PUB1u9bj3PePwefXfYZzmxxZkT/TkQ2cWOmVkhVtflEaLPrGG1T13C0/bv7XxzIPqBSv+X4sUtcJoQQHXz44YdYuHChSh0NRi7KVqhQAdWrVy/0fL169dRroXjiiScwYsSIIs9v375d1XuzG5mEi/NOvm/Le/EyfJTE2/rG4zrH2/rG4zpzff1PrK/zvn3lcO65tbBnTyI6dszCI4/swo4dsbe+aWlGs8CYF9qef/559O/fHzdIEi6gBDepuzFhwgQMGTKkyPvfe++9Qo/feOMNfPrpp/j+++9x7bXXqg0pYt2DDz6Ivn37qveIA04mYFOmTMHll1+OWMEUTMoiwkgHwi6NuuDXjb9i8rLJgc6Vwt6De3H5p5fj61Vfq8fntzpfpZhKLTDhhvY3KKFN0gHF1Vbcybc450IJbeKUEcHOdMqdfvjpGNDR+RRCEfmKq89WXOqoNqHN0khAR7MF633Zf0/+8qQSVMwafr/c8EuggUJpiWQb3X/S/Ugsl4ihPwzFW3++hYVbFmLaFdNwWLXDEC/IZ1lSawWpYdfngz5qG/RuUXKap9kR9KjaR6labHYeoyI810qphZ0HdqrP6JTlU5Trbk6/OcV2LyWEELdYv3497rjjDnz33Xe2XRwdOnSouqhrdbQ1adIEderUQdWqVWE3MrmXOZXEj8WTGbuJt/WNx3WOt/WNx3Xm+vqfWF7nfJUmWg6rVpVDo0b5+PzzRDRoELoclNvrG+ncxFWhLSsrS6UAyATIRDaCpAOI/T8S9u/fr1IIatasqR6vXr1aXdG0phRIoVxxy0nMUEKbaykFVsGkDMu59OhLldA26a9JuLnjzYHnxakmIltKYgqeO+M5DDhugDrYzGVd2OpC3PbVbSoFdd6GeTi+0fEligFW++XmtM3o+kZXZGRnIKl8ErLzsjHu93H4vw7/57hNdUvaIUdbuOWUL2iym5Obo94jt+r5cuXtGVv+oWYTEq9QZ9j8su3jQvHzjfiCeQwdWfNI/HPbPxj721g8M/cZJYC+/OvLGHpi8U7FkjDXQerYFTf+u7vejeYVm+O2n27D4q2Lcfknl2PmdTMjbtDhNYKP6Z37d2J/9n51//yjzseUFVMw/MfhSnguibV71gbq4JnOzOBj1Pq5jRap0zZr3Sy8/vvrGPv7WPXc7HWzcVjVw3xnN7cTrq//0bHO8bT9dCHzwm3btuG4444LPJebm4tZs2bh5ZdfxjfffKPmj1IaxOpqk66j9esXdvWbJCcnq79gZM6p62RDvrd1xo814m1943Gd421943Gdub7+J1bXedQoYMoUICkJmDxZxLZyMbu+kcZyVWjbsWOHmjyJ28yKPF6+vHAXvnD873//Q8OGDQPCmpk2ECpmrKUUHDh4QD1OT0tXk8rS0qNOD3Ur7rTFqxejfmVjorl993Z1e3bzs3FBkwvU+gRzVrOz8OnKT/HavNfQ/KTCXS6tLN++/FB3z4Kxfrv6WyWyNa7SGO+d/R56fdILv23+DT8s/QHH1D7GUZvq6u2r1W1qudSw2zIn2xAt9uzdo96zI21HQEQqy/Y3ycrMUrd70/aqeLv37A68Jttemk+UhQP7jeMlY39GYLyZ2YZAvG/vPhzcexDXt7weVfKrYNCPg/D83OdxRfMrVBqnOJpEDK2aHN2Ve/P4z0g/tMxQyD4+pvIxmHreVJw++XTV6XLkjJG4tf2t8CPBx/Ti7YvV83Ur1cX9He9XQpt0aV29cbXqDFwcyzYbjRBqJ9XGhuwNhY7R7Xu2l/kYPbzK4ZiFWQGRTZCxRxsv1u3mdsP19T861jnSdAISntNOOw1Lliwp9JxkPUgdNpnziRMtKSlJZTJcdNFF6nUpN7Ju3Tp069bNpVETQgghpDTMnQvcd59x//nngTCl+j2H66mjZeHJJ59UdTykbltZ0gvcSilIqpAUcNyFqkcXKfJvuzXupsSNmdtnYlDzQer5SpWNOlmVUyqHjd+/S38ltE39bype7fuqarhQZLz5eVizb03gvhmr6i5j2xxW/TCceNSJKjX146UfY/Layeh5dE9Hbapp+cbJTfM6zcOuq3k1u0pqFfWe9KR09VhcV2XZ/iaVUgq2d2Vje+/J3BN4rX7d+mWumZZaJVXdJldMDow3IcFwjNWsUTPw3IDaAzBq0Sj8t/s/fL7hc+VouvCjC1Gnch0sv2V5VOMwj1H5HBS3jcx93LJOS4zqPQr/N+3/8PRvT+PS9pcG6oT5ieBjOn2XcSxJOmbHIzqqtFlpHrEqcxVOb1S8q21HtiH4tm7QGsv3Li90jO4uv7vMx2inpp3w1tK3Cj1nxveL3VwHXF//o2OdY6kOrFdJTU3FMccculhn/q7WqlUr8Hy/fv3UvE2yGeT3adCgQUpkY8dRQgghxDts2ABcfDGQkwNceqk0QoJvcFVoq127thIKxO5vpTj7v8mzzz6rhLYZM2agXbt2gefNfycxGjRoUChm+/btYyqlIA9Gikli+cQyL+eyNpcpoe3zFZ/jjq53FKoRJifp4eL3OryXKsC+Yd8G5cK5ou0VRd6zce9GVXfKjCljt9ZzM7eT1GYToe39Je/j2TOeVU4qKcAuXSm/WvEV3r3oXXRo2AE6a7TVT60fdl0Dda7KFVg+C1ZB3EJ27GczTVK2kTW+kJhQ9n0ciG9xX5ipo9b4FcpXwJDuQzDgiwF49OdHVbMKaYSRsScDP6z5AWe1PCviZZrHUCTHqGnNvbHDjfhs+WeYvnI6rplyDX66/idUr1i4aLUfsFqRpQmCKTrLY2kSIanbv6z7pcQ6bea/bVq9aeBzZcY2jyE5dkt7/LSt1zZwX5paqM+y+Rnwid1cF1xf/2P3OsfTtnOTUaNGqW0tjjYp/SENr1555RW3h0UIIYSQCElLA845B9i0CWjTRmrvy7wMvsHVGaF0jerYsaOy/1uvMMvj4uz/0lX0kUcewddff41OnToVek26jIrYZo0pDrX58+fHXEqBHc0QTI6uc7S6lRRBk0gKqYt4Y9ZUe/HXF4vtOBoYd5iOiD2b90Tz6s2xN3Mvrp58NZ6b8xzav9Yez819Dkt3LcXDMx+GLramFwhtQV1Xi+vaqasZQnB8u5YR3DXVej84/rXHXotGqY2w68AuJbJVS64W6IwZDaXZRnLi+vp5r6tOqH9u/ROnv3M69hw85O7zI+JeM+usCScfdnIgnTuaGm06GnZIs5QTmpygOtGe0uyUQnGFb1Z9g1cWvFKomy0hhMQSkrkgja6szsExY8Zg165dyMjIwOTJk0u8QEsIIYSQ2CA3F5DS+YsXS4kvYPp0cbTDV7h+6VWs/+PGjcPbb7+NZcuWYeDAgWrSZHYhlU6i1mYJTz31FIYNG6a6kjZr1kzVXZO/9PT0wEn+nXfeiUcffRRTp05VdT4khtRxO//88xFL2Cn0FNeRsqT4N3e6GRUSKqiGCNJUIVzH0eC4wfHldmCngeq+OJru+e4e1URBGhQIX6z8IiBIWJEi8m/+8Sa6T+iO3u/2LtLZNKquowXLiomuoxbhoriOrqWNX9w6SArwiFOMuoPXt78eU6+YGtgv4jKMFDN+tONvmNoQM66doTpe/rbpNyW2ibPOr6zbZxzXZqdVcbQJ8pnKzDnUaCUYcZeZx678Wx3HqLjYZt84G+9f9L5yJlrjCjd9cRNu/fJWzFk/J6q42zO248pPr1TrSAghhBBCCCGR8NBDwJdfAikpwNSpQFPDq+ArXBfaLrvsMpUGOnz4cJXauWjRIuVUM5sZSHHbzZs3B97/6quvqm5TF198sUoNNf8khsl9992n6nUMGDAAnTt3ViKcxIy12ikBEcOaY1hKyiK01atSD5cfY3RjfXH+i0og+mDJB5j458SQjrbiRIA7u96JTy75BPefeD/6HNUHdx5/J/4e+DdObHiiev9rv71WqKOldMZs/Hxj3Dj1RnWi/+2/32JbRpRF2qVBQ8G/kXWJVgizS2gz92Pw9rFj/1rFrkj3cb/j+mHHvTvwZt83ceJhJyohR8SuL1d+GfEyyyL0tKvXDj9c90NAbJOOtH7F6koTjqx1JOpWrovM3Ews2LQA7y1+D3WfqYupKwzB02T9XiNtVNKsa6bUDCu02XUMhfqeMAVQSTuPhgl/TFAOybu/OVTfkhBCCCGEEELC8dVXwGOPGfclXdQvzQ9iTmgTbrvtNqxdu1bV2ZAUz+OPP75QusBbbx0q5L1mzRolkAT/Pfzww4UEiZEjRyqnm3RNlDpuRx55JGINO4Wesghtwh3HG3XdPvr7I5z13lm4cvKVuG7KdUr8ikZok0L7Fx19ER477TF8fvnnGHXmKNRIqYHr21yvXh+3cJxy+Ijw0m18Nwz6ahB2H9ytUk6D40eKpEfm5BkdRUXciBlHW5i0TrviRyLE1KpUK/BvL29zedTpo2VdBxHbhvcYru5/uuxT+BXTqWk62uQ7yHS1vfb7a+g/rT+279+Otxa9FTblVNWKcuEYMpcRLAKWxKpdq9StONrM1G1CCCGEEEIICcW6dcDVVxv3b7kFuPJK+JaYENrilVhJHRWOa3Cccj1l52Xjm3+/CTwvIkFw6mhpapz1btZb1QwTseGkN09C53GdldOnanJVjDl7DFYOWomk8kmlEtrMk3xxBEkKbKSOsNKmRcaKkBftMsxGF9KkYF/mvoiWacc6XNj6QnUrou3mtEPuVL9gTf+UhgYmJx12krqVpggHcox03dnrZxdKKS4i0BUIpuHqIOo8hlbsXIEVO1ZEHOvf3f8Gxjrtn2m2jI8QQgghhBDiP/btA/r2BXbtAqTM/vPPw9dQaPOZ0GY9iY82/gMnPaDe26ZOG4w7b1zA4WY6V4LjRhNfakMNOG6Aui8CmwgK17S7BstvXY5bOt9idEYNIQLYVZ/NOk6nRIxYE9qOrXcsWtVupYShLuO6YOxvY/H1qq/x+u+vKzEoVDF8O9ZButoe3+h4td2lRpzfMNM/KydVRo2KNQLPm442QRpDiAgsKc7/7f4v8PzavWsLCW1uHkNCNIKZdT2iTTslhBBCCCGExAdZWcBFFwGLFgF16wIffwwkJ8PXUGhzETvTwkqTVhjMmS3OxIa7NmDRzYvQr0M/tK3bVoky4nKzjrG0IoA0XZDuh32P6quWMfGCiWiQ2qDYdYjG0VZcfbZQ8bV1HS3Yr+at3Y45a9fRaNZBxvHM6c8oF6G4lwZOH6jShKUY/jWfXYPHfi5Ilrdg1za6qPVFMZk+Kt02zVqEpcUUy8TNZt3X8vlpUMU4vt8+/210bNBR3bc2HQjuVqr7GA045kII8tGkj0on2/X7DIFRmPHfDGRkZcDPyDYb9OUg3PvtvezQSgghhBBCSATk5QH9+gEzZgCVKxtNEJo1g++h0Obj1NHSCHkifIn7TASDmzreFHi+WfVDn4bg+lGRCnm1K9VW3Q+nXD5F1e6KZB10ONqccgs54UaKts7fuUeeq8TU0b1HK4ebiEGnNjtVvTbsx2H4bNlneoS2ow2hbeaamdixfwdigQ37Nqhum9dPub5MNcaC0z9NxKX543U/Ym6/uTir5Vno3qR7IH003L9129EmY7PuHzm+Qm2bNXvWqH8nTRzku0EEeWlk4mfkeHl5wct4du6z+H3z7yW+X5q9XDDpApz7/rnqPiGEEEIIIfFEfj5w663Au+8CCQnAJ58AHQ3vge+h0OYidtYIK2taYSiubnc1UhJT1P2jah1VJK4TIkA0HR8bpjaMKr7uQvO6u5pa70ezjNTkVNzR9Q7lKlw8cLHqDDqoyyD1mjjblmxdUiR+WbteHl7jcLSv3x65+bn4fHnRNENpZvHA9w/gyV+ehFOYtQflOJAU2tJiHn+HVS0stAlH1T4KXRt3VffFzRnsaIs0dVRXHUHrfekOK/dNV5uIQ5d+cinqP1e/iIhmpo3Kfj3/qPPjIn10c/qh+oKSbl0SH/71IaYsn6JqIgY3lCGEEEIIIcTP5OcDd90FjB0r5zLA228DZ56JuIFCm4vEUjOEUFSrWA1XtjVagYhIEqtC2+Jti9XtMXWPiUqosj0tL7jZAmKrRltxPN/7eZzW/DRkZGfgxfkv2h7fmj4aLMjIMm78/EY8/svjGPr90EK1v3RiFvMXvlz1ZanjrNu3rkgjhFB0a9JN3f617S/sPbhXrbdZ3838t+YxZIq0dou1xR1DZtOKO76+QzkP7/3uXnyy9BP13Bf/fFEozr+7jG13RI0j0LdVX3VfBDrzeT+yJX1L4L507k3LTAv7XhGOR8wcEXi8bPsy7eMjhBBCCCEkFsjPBwYPBl54wXj8xhvAVVchrqDQ5iKxLrQJo3qPwitnv4J7T7g3JoU2ESL+3PKnui+pkJHED+6aWla3VnD8Im4kTfHtdDxJurDUzhPSs9MDz9sp9HSo36GIM0ji3zr9Vryz+J3Ac9P/mQ4nsIpC36z6RokjdqaOBlO/Sn3lABMH3fyN81VjhMzcTLVtpSOv26mj8hnv2bwn0rPS0eudXhg1b1TgPdLAxIophorQJt2K5Xb3wd3oNK6TY/vPTaFNtpE41sIhzUWsLrZlOyi0EUIIIYSQ+KjJdvPNwKiCU4lXXgFuvBFxB4U2F7FTxNAltEma4cDOA1EjpUZMCm0i2uw8sFP926PrHB13Ndq8INYWF1+aI4z9fawSI3sd3ks998XKwu4pJxxtezP3Yu76uWVKHTUbGhRHoE7butkBgU5SnpMSklw/huSzPv3K6aqOnyk6SlMUYdGWRcjOzS6y7UQ4FJH2p+t/Uimyew7uwbkfnItZa2fBb2xOMwTiiokV1e24hUZn5mAyczIxcubIQuLr0u1LHRsnIYQQQgghbnUXveYa4PXXjXTR8eOBgQMRl1BocxE7T6KD0xbtjm+NY9Y2iwWhbfHWxYEacilJKVHF17Z9zLQ/TTXgQnWM9KrQ9s/Of9TtFW2vwEtnvaTu/7Tmp2LT8uzCdGVJkw5BamlFi0r/LOi+WZKjrVCdtg1zQjrh3BZrRUSafOlkPNTjITx3xnN4/bzXUS25mmp28Pf2v4sIbUfUPELdNq7aGDOvn6kccYKknvrV0Xb9sdcjqXyScvmJAGny68Zfcfknl6uadqv3rFbNWZ447Qn1Gh1thBBCCCHEz+zZY9Rge/99IDHRuI1HJ5sJhTYXsTO1ULdI4kaNMysLNy/EqwteVW4RK2baaKgupsE4LWI4WV/Lq0Kbeb9yUmUllkoKYlZuFmb8NwO6McWiAccNULdfrvxSHU/SEOLKT6/EQz8+hB9X/1hsDOnIKeNV6Z9VjfTPSBxtEnfMgjGuC21W0dZ8Tdx1D5/yMO7udrd6rmPDjoXSR+XfWJshmFRIqIAjax5ZKL6f2JKxJfBdc3bLs9V9a5OIflP7YdLfk5SrTxpLjDtvHDo2MLbd8h3LfblNCCGEEEIIWbsW6N4d+PFHoEoV4IsvgMsvR1xDoc1FvCSSuOEIM+8/O+dZHP/G8bjly1tw65e3hmyEUFJ9tlBCnm4hzImOkXa65nTWgCspvrwmy5C0RUF1aty5EieMPwHDfhhW5mWLI+vijy5Gp9c7ISMrA7sO7FKCiCCp0bL8JduWoMNrHVR9LSl2P3LWSPSc2BOT/poUNu7WjK3qtm7luiqFsiSkYcd1x16nuq+Kcy845dTpOn/WfRFuGZ0bdla3v236LbDO+7P3q1jNqjcrNr4fU0cbpDYIfN+YnWsl1VbENOGrq77C1nu24ryjzlOOP3G/yfYyG18QQgghhBDiFxYuBLp2BZYuBRo2BH7+Gejd2+1RuQ+FtjgQ2nQJPU64ba749ArV/dCsGTX+j/EY9/u40jnaCg53bY48Fxx/XhJrI4lvCm1Tlk/BSW+ehLkb5qpupGYdtNIgtcUu+fgSVQ/u982/44fVPwQaITSo0kClPXZr3C0gXF7W5jI83evpgGvptq9uw/aM7SFji2AmiJgS6T6c0HcChnQfEnjO6mgzjyFTQLU7/TjcMVrcMoKFNtMJ2KRqE+ViKy693I+po9LUomWtluq+2fBgzZ416jtKUm/POOIMJJRPUM+L+Gq+l+mjhBBCCCHET3z5JXDyycCWLUDbtsC8eUD79m6PKjag0OYiXhJJQi1Dd/xVu1apVKyEcgl49ZxX8XjPxwPCx/wN81UaqekiObb+saWuMadLiNRVo83PQtvJTU9GlQpVVIML0y0m73llwSulWqb8W0kF/eKfQw0WpFB/cI2xF896UaWQzr5xNj68+EPc2/1efHbZZ2hbty127N+BO76+I2x86/gjQd77RK8n8GbfN9HnqD64qPVFhV6zxnWqzmJxy+jcyBDaxPF3IOdAyLTRcOP3CyL6FxLaahYW2kxnmzwfvB1b126tbpdtp9BGCCGEEEL8wWuvAeedB2RkAL16GU62Jk3cHlXsQKHNRXSk/VmdJLpTI834uoQqSfczi9Xf3OlmDDlxCC5odYGqiXXtlGtVIXJxFNWoWAONUkuuj+XnrqO66vw5lZpqviYOKdNJJk6qt/q+FejwKCmf0SLpmSLWiuNMUjaFn9f9HBCLpCaccFyD4/Daea8FmhWYYxH3mYxNUkmnrZhWJH5Z9vH17a/H55d/jnpV6rnnGrV8X4RbhjjXJDVWPmtLdy4tsu2Ki+8XJM04MzeziKNtU9omdVyaTT3M562Y3ZDpaCOEEEIIIV4nLw8YOhS4+Wbj/vXXG862atXcHllsQaHNRZysf2UHbgtVsp3EBSTd/OTEduD0gQE3WyTb0Gm3EJshlC7+6N6j8dq5r+H7a7/H1e2uVs6p3Qd3470l7ynh7KKPLsIHSz6IaJniRhO6NemGEaeMUPclfdTsVhvKlWWlU8NOGNxtsLr/9Jyni7zul89YccuQz5ZsB2HR9kUBN2A8OdpMN1v1itVVemjNlJpK4Bdke5jONtPpFsrRtnT7UkfHTAghhBBCiJ1kZgJXXQU8+aTxeMQIYMIEICmyKjpxBYU2F/GSSBJqGYH4cE5kqFaxGp45/Rl1/48tf6jbdnXb2RZfx/jtLmRvdSHZuQ5O1YCzNrsIFV+KzQ/oOACpyamq1tVtnW9Tzw/+djBOfftUTF42GVdOvhJ3fX1XoHZfOMz4kn7ctHpT5c6SfzN1xdSwrqxgbj/+dnU7e91sbNy3Me6ENmudttcXv666s1rTbouL7xesaaMmgTptO1cGHG1H1jK6rlppXacgdXTHskLHPiGEEEIIIV5h1y7gjDOADz8EEhOBt94Chg+Xc0i3RxabUGjzs9AGf4gAwfHF5dS9SffA40jqs4XsOqq5hhprtNkT/8YON6JyUmWkZ6Ur0VKKzQuj54/GOe+fU6zYFqoGnCC1xsKJRcFIswQ53mR/SkOF4uKXlUAzhAJBxqmGJtbXQtG1cVd1uy5tnXIXJicko0ujLiXG9wub0ws6jlZpEHjOWqetOEfbUbWOUvtVOt1u3x+6qQYhhBBCCCGxyurVQPfuwKxZQNWqwNdfA9cZVXlIGCi0uUgsiBjRYJ7sB4sATgttMo6Xz3458Hz7+u1L5ajymhspuGOk3empbtSAi2QbiYvx9fNexxXHXIEF/Rfgm6u/weRLJyvx7dt/v8Wbf7xZsqOtoAvkSYedVOj1SBxtwqVtLlW3H/39Ucj4uusguu1oE3HzqdOewn2d78Nnl36GtXeuRbPqzUqM72tHW4Go9te2vwJdcUM52lKSUgLbig0RCCGEEEKIl1iwAOjaFVi+3Gh28MsvwGmnuT2q2IdCm4s4XWje7mW4GV/EtXcueEd1Iu1Qv0NMpHYGO+acbIbghWOoLPGvbHsl3r/ofXRs2FE9vqD1BXis52Pq/vCfhiu327q969BtfDfc8dUdJTrahNQKqarRRiSYnUFnry+cPuqXz1hJy5DX7jnhHtx13F2qU6q1gUNx8eMhdVTEXvkul+NJmkaEwmyIIKIcIYQQQgghXmDqVOCUU4Bt24D27YF584C2bd0elTeg0OYisSJilHYZbscX8WXoSUMjTqtzrUabprQ/J44hpxxzpYk/sPNA5UgTEeS+7+5Dz7d7Yt6GeXhn8Tth47eq3Sogrkkx/0j3TaOqjXDiYSeq+58s/cSW8XtJaCtt/HhIHTXTQcXNFu546ta4W6B7rt+2DSGEEEII8R9jxgAXXADs3w+ceaaRNtqwoduj8g4U2lzETkdVqLRCv4gAXo2vvQZcfn5gGV7oXGt3/AoJFfDEaU+o+6/+9mqgG6a1WURuXm6h+LKdzPTRSOqzWbn06IL00aWH0ke9foxa94WuY+jvbX9j+I/D8cTPT+C1317D1vSt8JOjLdzjYFG4SoUq+HPrn5i2YprGkRJCCCGEEFI2xo4FbrsNyMsD+vc3nG2pqW6PyltQaHMRHSJGyELqNne99KoQFixG6hq/KfToqq9lxrUKSrEohDkR/+KjLw4U6pfUPau4Fi7+tcdeq27PbnF2VMu6sPWF6nbO+jkqVdWO8Yetg1iwb71eZ1G4+9u78cisR3D/D/fj5uk3o/+0/vCD0Fa9YvVCqcdH1ixan82kZkpNDOoySN0fOWukq91HS+rUSwghhBBC4rsm2x0FlXiGDQNeew1ISnJ7VN6DQpsPhTavCmFOxdcthPkp7S/WhTYRjiaePxG3db4Nn1z6SUTxz291Pvbfv191M40Gq8iSmZNpy/gjdUV6VSwX9h7cq25NQfT71d8jOzcbXhTaGqQeSh0N7jJanKNNuLvb3aqBx8LNCzF95XS4wV1f34X6z9ZnUwZCCCGEEFKEnTuBiy8GsrKMtNERI+R8y+1ReRMKbT4W2nQLSXamLTrpOHOqhloenGm2YF12vAltpsDx0tkvBTo+RhJfOkFGu98pZkcf33p/6IlDUSulFvZn78dvm36DV8jKzcKO/TuKiK3B4lqojqNWxP12a+db1f2nZj8FN5ixegZ2HtiJJ2c/6cryCSGEEEJIbLJ7tyGurVsHtGgBvPkmRbayQKHNRXQUmvfDSbqutDm/pKYGC6l2LaM4kSQea8AFY90GXv+MFRGbNTnmrPcTyyeiR7Me6v5Pa36CV9iWsS0wfkkBDetos9wPx+XHXK5uV+5cCTc4mHNQ3X6w5ANsStvkyhgIIYQQQkhs8d9/wAknAD//bNRi++QToFo1t0flbSi0uYgOEcMJocqrQpjjzRAcSk21axleF8J0xw+1DMYvPn7wMk5peoq6/9PanzxZny14O5nimjj1aqTUKDFWUkJSke3jhtCWnZeNMb+OcWUMhBBCCCEkdpg/H+jaFVi+HGjcGPjlF+DYY90elfeh0OYiXqvR5lZqp92F4P3SddQJoc3OdaDQFvlnzKlmBbrFYOv9hHIJOKWZIbT9su4Xz9Rp25y+OWTaqHBS05NQo2INVfevtNvHDaFNGPv7WGRkZbgyDkIIIYQQ4j6ffgqccgqwfTvQoYMhurVr5/ao/AGFNhfxmtDmN7eNLiFMt0gS7Ci0K/XP60KYCDk64/vxM+C0o61N3TaqVpmX6rSF6jhq0jC1IbbesxVv9HnDE0LbgewD6rZqclXsOrALb//5tivjIIQQQggh7iGnq08+CVxyCXDwIHDOOcCsWUDDhm6PzD9QaHMRCm3uxHfKLaS92QJTR8MKkbr2cbCY55TjTPcx5JTQJn89mnqrTtv2jO3qtl7lesWmg3rJ0TbguAHq9rPln7kyDkIIIYQQ4g5paUZn0aFDDcHt1luBKVOAKlXcHpm/oNDmIjrS8vwghPmlELzdjrlwqbt2LcMvQpvAOoIlxIc7Qptgpo/+uOZHeKXrqJCckKzFdekUOXk5yM3PVfdPbX6qul22fZnj4yCEEEIIIe6walUCunYth8mTgaQk4LXXgJdfBhIT3R6Z/+AmdRE62hjfjvh2LcNPQpvElcd+2cderbNYnNA2e/1sJWJVSKiAWEb3Z8DptFGhff326nZj2kbsy9ynUkkJIYQQQoh/EdfaddfVQnp6OTRqZHQWlSYIRA90tLmInY6q4oQ2r6adeS2+080WnBTavFQDzho3EB/eOoa0dw52KH061DLa1Gmj0jClTtuM/2Yg1jHHn1DecKN5VWizNkKQenNmzbnlO5Y7PhZCCCGEEOIMWVnAvfcCF11UHunp5XHyyfn4/XeKbLqh0OYidp7kWsU0p+tHec7NA4fra2lOTbVLTPWbo83u+NY4jB9ZfMFMVzRfk2P18mMuV/etxfil6cDeg3sRa/jF0WYKbeIglHG0rt1aPWb6KCGEEEKIP1m1CujeHXj2WePx//1fBr79Nh/1QpceJjZCoc1FdKeFOdVVU5eQpNvNo0so9JMbyc5tpLtZgZNCmykeeb2OoBuONuG6Y69Tt58v/xy7D+zGyp0rceRLR+KECScg1tBxjJrHjxtCW8XEiurWFNroaCOEEEII8R/vvgt06AD89htQsyYweXIeHnkkTdVmI/qh0OYinnOEuZQa6dX4uoROp0QYrzUr8KOjzaljyGmhTWqEHVP3GGTmZuKjvz/CoK8GIS0rDUu3L0VmTiZiCdMB63VH24GcA4WFtjoFjrYddLQRQgghhPiFvXulFhtwzTVAejrQowfw559A375ujyy+oNDmIl4XAew8AQ0Z3ym3kEdq2DktdHpNCPOj0Ob1+OGWIcey6Wp78McH8c2/3wRe275/O2IJv6WOpiSmqNtWtVupWwpthBBCCCH+YMYMoG1bYOJEoHx5YORI4PvvgcaN3R5Z/EGhzUU3m+lW8YrQw/ili+/V/etloS03z9nUTl3p314bfzTH0FVtr1LP7di/o9Dz2zNiS2gzjyXbXZ0Fjma3U0f/3fWv6v5KCCGEEEK8iTjXBg4ETj8dWL8eOOIIYNYsYNgwIKHs/bxIKaDQ5hLWQvZereHltfi6HWFFRBKPO/6s9+1stmCN6yUhz9E6gprr/DklBhe3Dg1SG+CMI85Q95tWa4oWNVuo+8HCm26kAcPQGUPR+93eWLVrVZHXdR+jTnEgu3DqaMPUhkitkKrqxUmNPEIIIYQQ4j1mzgTatQPGjjUe33qrkSoqTRCIe1BocwnrSZbXRADdQphuodApEcPUUj1bH8ziutQlMuiO77XPgN/iW+8nlC96OW3kKSPRvUl3vHPBO2hStYnjqaMT/piAli+1xJOzn8S3/36L8z44r0jnU78IbcGONvm+Neu0sSECIYQQQoi3kFPmUaOAnj2B1auBww4zUkdffhmoXNnt0REKbS5Boc39+F4tNG+OW3czDbtdl7qFMKsrS9c+MMUiqxjpiOPMZ11HTTo36oxfbvwFJzU9CXUq13E0dfTVBa+i39R+Stg7qtZRaJTaSAlOV06+MpAuah1/Qrmy++6tYqNbQltKklGjzZo+yjpthBBCCCHeISsL6N8fuPtuIC8PuPZaYMkS4LTT3B4ZMaHQ5sPUUb8IYV6NH3DkwdsiibWGlBeENmscv8S3W8gLTp/WLTZb75e0jNoptR1LHZ3x3wzV6VQY0n0Ilgxcgs8v/1y5vb5c+SWemv2U7xxtwV1HBTZEIIQQQgjxFnJ6cNNNwPjxRsMDcbW99RZQtarbIyNWKLS5hPUkS7cQY3d8p+tH0S3kbny7lkGhLXbiO5Y+HcUyAo42zamjUo/sko8vUbXJrm53NR4/7XEkJSShY8OOeKqXIbB9teqrqMfvtdTRQo627RTaCCGEEEK8wLhxhrAmItuUKcCdd8rFdLdHRYKh0OYSdruFrHGcKvbvVNqctmYFXnMjhRm/7q6m1tdiPrWzIL3Pq0KYU3UKYyV11EqdSs4IbXd9cxf2HNyDro27Ytx54wodl9KUQQiVOuoXoS0l0ZI6aqnR5vR4CCGEEEJIdCxYAAwykjLwxBPAeee5PSISDgptPq7R5lj9KM1Cj1dFBq93HaWjjfHLGj+aZdSuVLrU0Z37d+Ljvz9GVm5Wie/9bdNvmL5yuhrLxPMnFnJ3lXX8kWCNIY46tx1th9c4HEnlk1Ra6YZ9GxwdDyGEEEIIiZxZs4BzzzXqs51/PnDvvW6PiBQHhTaXYDME9+LrcgsVqX/lMcdfcUKbHem11hheP4YYP7L4VndYxKmjUTZDeHTWo7j0k0sxYNqAEt87cuZIdXtV26vQslbLyMZfIIh53dF2ILtojbbE8oloWt1w8a3Zs8bR8RBCCCGEkJKRU8rnnjO6i27bBhx7rJE6ynTR2IZCm4+aIQQLPV4/SfdaaqdTjj+n0wrtWoZsB6fTd3XFN8UX7cdowfeEV12jTqSObtu/Td2+/efb+PCvD8O+b+HmhZj2zzQ1jgdPftD28dshtFlLCjiROio0q95M3a7es1rbsgkhhBBCSPTs2QNcdBFwzz1Abi5w9dXAnDlAtWpuj4yUBIU2l6CjreT4djpJ/NgsQnfHSHbGZfyyxi9N6uiuA7sK1UgrCeuybv7i5rDOLNPNdsUxV+DIWkcWO35rWqcZP6G8Uf/PblenVWTr/W5vHPHiEUjLTIMTqaNCs2qG0LZ2z1rbl0kIIYQQQkrHwoVAx47AZ58BSUnAmDHAxIlApUpuj4xEAoU2lyiUludRtwrjuyuEec3RVtwyGL+wEOOXzr6lEdrk/bsP7o54mQEhrFwC9mbuxU1f3FTkPdNWTMPnKz5XY3jgpAfCxjLFNF2OtlCuTpMFmxbgu/++w3+7/8Ps9bNhN1KHLaTQVuBoW7OXqaOEEEIIIW4jCQ5jxwLdugH//Qc0awbMng3ccgvTRb0EhTaXsLqF7HZUeVVkKA/NIobHU2vdGr8Ty9Ad3w43UnHxvb59YkFoS0pIQvWK1aOu02bGH9TFaMH03b/fYWv61sDruw/sDohvg7sNDnTatHv8kRJqGcJbi94K3J+/YT6ccrQ1r9Fc3dLRRgghhBDiLmlpwFVXAQMHGk0P+vQxnG2dO7s9MhItFNpcwpq2aHsNL81uGKdrhNk+fqdSOwu2k24hVXd862u6lsH4zsR3ukaefAaicXaWpvOomWYq6aCdGnZSy/viny8Cr9/5zZ3YnL4ZR9U6CiNOGRHV+HVso1CuORHBPvjrg8DjXzf9Cm012pJYo40QQgghJNb46y9DUPvgAyAhAXj2WWDKFKBGDbdHRkoDhTaXsPvkrVghxiYhz+uOMN2pnU6JGE6nFXo6vRnePka9LpZbnbuS2lkSpWmIYF2HPkf2Ufen/jM14G6b+OdE9dpb579VRGQKN35rjTgnHG1TV0zFnoN7kJyQHHC02d0YIWyNtgKhbcO+DcjJy7F1mYQQQgghpGTefhvo0gVYsQJo1AiYORMYPJipol6GQptL2C3y+EkE8Et8XTXaVOz8fG3jD45vl2POD11BTbHISUdYPLki61SuU+rUUXGK9TmqT0Bg25+9H8N+HKYe39b5NnRt3DXq/euU0Gamjd5+/O2okFABOw/sVLXanKjRVr9KfSXwyWdmc8ZmW5dJCCGEEELCs38/0K8fcP31wIEDQO/ewB9/AN27uz0yUlYotLmE3W4zPwhJ5rZw2rFle+quZreQGVur0Ab7hTw/iqm66vB5PT27tEJb7ZTaUTvarN2J29Vrh6bVmipR6b7v7sP8jfOVsHT/SfeXavxWd5uubbQpbRO++fcbdX9AxwHoUL+Duv/rxl/1pI4mphQZT9PqTdX9dfvW2bpMQgghhBASGnGvde0KTJgAlC8PPPII8OWXQB3jujPxOBTaXMIJEcOprpdeaebgdH0qXdvHjG2KMbqEPLvjW2M5tQ90iyR+EcJiRWgzHW3harT9vPZntHmljUq1NLGugwiVpqttzIIx6vbmjjejXpV6pRq/zvRj89iUdZLldWzQES1qtkCXRl3U8yISOpE6ak0fXZ+23tZlEkIIIYSQokyaBHTqBCxZAtSrB3z3HfDgg4bgRvwBd6VL+EHEcCp+JLWdoonvmFvI5hN0q2Cn29GmI76fj1Gvxtcl1pZaaCumRltGVgaunXItlm5fik+XfRp4Pvh7whTaBEmJvK/7faUef6j4ZSV4GVm5Weq2VqVa6vb4RsdrEdoOZIdOHRWaVaPQRgghhBCim/R0oH9/4PLLjfs9ehipoj17uj0yYjcU2lyCIob/4+t2nFFoi4H4bLYQUfxou46GqtE2/MfhWLNnTZG4wevQo2kPVEuuFkjFbJDaIOqOoGY6aqj4trsuC5ZlCnmmo+2PzX8ERDjdjrbmNZqr23VpTB0lhBBCCNHBggXAcccBb7xhNDm4/35gxgygQeRTVeIhKLS5BJsh+E8IC+7K6pTQZltXWc2OOT8eQ4xffHxr986ypI7+tuk3jJ4/OvC4uBpqSQlJGHnqSJza7FQ8cNIDZRq/G/tA0kdrptREZm4mFm9dDNtrtIXovMrUUUIIIYQQPeTmAo89BpxwArByJdC4MfDDD8ZziYluj47ogkKbS+gQMZyqQeZ0swLGLxxfV7MCPzjaTEeS7n1giju66gianzHz1u6GHbHqaAuVOvrXtr9w2SeXqViVkioVEfBCrYN07/zhuh8irs0WbvxOpteaz8u+DtRp2zBfe9fRQkJbOoU2QgghhBC7WLsWOPVUo/5aTg5w6aXA4sXAKae4PTKiGwptLmF3oXw/CUm64gc7zuzeB0WEyIK0QrtFEqdTR73cGZfxI/sMxGLqqAhck/6ahOPfOB7/7f5PiUGDuw0OX0OtQGS1c/xaxOCCFNFg1591/NIYQdDhaAuZOlrdSB3dkrHF1nRVQgghhJB45f33gXbtgJ9/BqpUAd5+G/jwQ6BGDbdHRpyAQptL2F3byc9dO70anzXawi9Dt9ipfR/DH8dorAltZuqopE1+vPRjXP7p5difvR+9Du+FBf0XoEGVBlqFsGDHovW+k/vgiBpHqNvVe1bbskz5LgqkjiYWTR2tW7muEuBkLOv30tVGCCGEEFIWpP7aVVcB+/YB3boBf/4JXHutUZuNxAcU2uKg66jnRAymvobE6iyT2HbX+QsW2lhH0L/xnf4MROqMrJxUOeC4uumLm9Rtvw798PVVXyu3W0AIszQrMO/bsQ7BbjO39rHZnCCU0Cbvf37u81iwcUHEy8zJywksJ5SjTfaN2Xl0zV6j4QQhhBBCCImejz8GnnjCuP/QQ8CsWcDhh7s9KuI0FNpcQqdbyBTxvCoC+CW+biHMy442HqPxFd8UsEpCBB8zfXTPwT1oWq0pXjjzhUBape7UzuLi25GaGknXUeHwGsZsbO2etYXcdcLkZZMx+NvBuOaza6KuzxZOaBOaVm+qbs3OroQQQgghJDqWLQNuuMG4f999wMMPs+FBvEKhzSX84BZyuki4V4QwXY5C3V1Bgx1zTqaOevUY0rUPzM+WU8doLMU3GyIIL5/9MipXqBy22UVpxLxoxu/WNmqU2ghJ5ZOQnZeNTWmbCv37Gf/NULcrdq7Axn0bI1qmmTYqJCcmh3yP2RBBxD1CCCGEEBIdCxYAffoAGRlGAwTpKkriFwptLuFEoXmnhDCvd0S0a/y6a9jJOK3LsFvIC95GFNrc62oaz+nf9avUV7cXtr4Q5x55bsj4JXUdLev4rampuraRuYxQ8eU4O6zaYSHTR39Y/UPg/o9rfoxKaEtOSA67Ho1TG6vbTemFhT1CCCGEEBKe9HTgrruArl2BVauAJk2ADz6gky3eodDmEhQx/N/MQadrUURU3ceQbiHPi8cQU1/1x3/w5Adxc8eb8eo5r5YY3+5mBcU55nRto1BdR6112qTjqok0Kli5a2Xg8Y+rIxPaDmQfKDZtVKifagicwQ46QgghhBASmi+/BNq0AUaPBvLyjAYIv/0G1Kvn9siI21BnjYNmCBQZnB2/rvhmLHHCOOE4oxgcP/FjyZV6QpMT1J8bzQpiJXVUOLy6Uadt9e7VRRxs4kyTzqw/rDnkbovE0Vac0GZ2dN2SviWq9SGEEEIIiTe2bAHuvBOYNMl43KwZMHYs0Lu32yMjsQIdbS5BEaMopnPK60KeLhHDGotCW/HxTZeQX44hxi85tdOOZgWuCm1BP8ehOo+aaaP/d9z/IbF8ompcEEnzAlNoS0lKKVFo25y+Oep1IoQQQgiJB+Q0dfx4oHVrQ2RLSADuvRf46y+KbKQwFNp82gxBxCq7l+F06qXX4+t2LXo9vh/2MdObi49vCmK64mur0WapAad7HQJdR4NTR6sXFtrk82gKbX2P6ovODTtHnD5qdh0tztHWMLWhut2esR3ZudmlWDNCCCGEEP+yYoXR5OD//g/Yswc47jijAcLTTwOVD/XuIkRBoc0ldNe/Mk+grc/bGd96a3t8+ENk0C2m0tHmX8eceezYLXZ6XSwPVUPNTiHMTE1Vlyk0OWsjbagRcLQVpI7+u/tfrN+3XnUj7X5Yd5za7FT1fCTpo5GkjtauVFu55GTdt2ZsLeXaEUIIIYT4i6ws4JFHgHbtgJkzgUqVgOeeA+bPBzp0cHt0JFah0OYSOkSMUGmFdi7D6yKJdfvoEDvDxfeqEEahLXbix1INtViIr7vrqFXsDKSmFohwdi2jRKGtwNG2MW2jEspMN1u3Jt1QKakSejbvGXC0maJgiamjieFTR2X5dVLqqPub05g+Stzj1VdfRbt27VC1alX1161bN3z11VeB1w8ePIhbb70VtWrVQpUqVXDRRRdh61aKw4QQQuxnzhxDTBs+3BDczjwT+Ptv4O672VWUxLjQNmbMGDRr1gwVK1bE8ccfj19//TXse//++281oZL3y4nnaGnvEcTDDz+sXrP+tWrVCrGGUyKJwJP0wvGDT6C9Ej94GTpqwFFoi4/4fqmDaLcQZh2nU+sQ6DoaNH5xmFWpUEXdX7tnLb759xt1v2czQ2CThhEVEiooIU7cbmV1tAl1K9VVt6zTRtykcePGePLJJ/H777/jt99+Q8+ePdG3b181BxTuuusuTJs2DR9//DFmzpyJTZs24cILL3R72IQQQnzE3r3ALbcAJ54ILF0K1K0LfPCB0WVUGh8QEtNC26RJk3D33XfjoYcewsKFC3Hssceid+/e2LZtW8j379+/H4cffriagNWvXz9s3DZt2mDz5s2Bv19++QWxhlNphV5OvfSqCGAKGbrTg50Sa3UJeZ7cx/D4+MPFh0dSRx3qOhoL+1g+d6ar7deNv2Laimnqft9WfQONDdrUaaPuL9+xvNhlHsguuUabUL+S8bu6KW1TKdaMEHs477zzcPbZZ6Nly5Y48sgj8dhjjynn2rx587B3716MHz8ezz//vBLgOnbsiDfffBNz5sxRrxNCCCFl5bPPgKOPFoe10fzgxhuBZcuAyy+X+ZnboyNewVXDo0yU+vfvjxtuuEE9Hjt2LKZPn44JEyZgyJAhRd7fuXNn9SeEet0kMTGxWCEuFtAtklhTiTwvAngsvhmbNdqKj++EEOPVY4jxI+86ajrC7BbaYqFzrdRpW7JtCR79+VFk52WrBgjt67cPvN6sejP8seUP5Xiz1dHG1FESI+Tm5irnWkZGhkohFZdbdnY2evXqFXiPZC0cdthhmDt3Lrp27RoyTmZmpvoz2bdvn7rNy8tTf3YjMdUFNw2xY5F4W994XOd4W994XGeuL7BxIzBoUDl8/rmhprVokY+xY/NVAwTj38DTcB/bQ6TxXBPasrKy1IRp6NChgefKly+vJk8yWSoLK1euRMOGDVU6qkzMnnjiCTUJC4cbEzDryZVdyzCdU3ISmpObc+iF/MgPiEjiKxHGsg75ecUfwJEe5IHx5+Wq95onuvK8LduoQHsMjN8S0874gmx/q6PN7n0s8XPycmyPb57sS/yAgIGSj9FI93Egfl5OoWPItmO04DKT7viybYLX1474pkBuxg4ISjaNP/gzEPiMlStnzz4Oim9+D0kDADu/S62f30LutjIuw+o+lbFbnal2xLe6B9V3hGyjgs9xqN+CZtWM3IR/dv6jbvt16FfoPYdVOyzQmbS4sVkdbeHeJ8+bQps42uJhEqZjAhYP280JlixZouZvUo9N3GyfffYZjj76aCxatAgVKlRA9erVC72/Xr162LJlS9h4Mg8cMWJEkee3b9+ulmE3chyI+06OL5nb+p14W994XOd4W994XOd4Xl+Znb3zTgoeeywVaWnlkJiYj1tvzcAdd6QjJQUIk2znOeJ5H5e3cX3T0tJiW2jbsWOHulIpkyMr8nj58uLTYIpD6ry99dZbOOqoo1TaqEysTjrpJPz1119ITU2NmQmYuYNyc3LDpspGS3ZWtrqVA2rrtkOFgXds31GikyESDuw3TtYy9meoMZsn0Xt278G28tvKfJBnpGcYyzl4QMWXW/V8hrG8srJ79251K+OWeAczjX2bkWZP/LSsQx+6Ldu2BOJLyrNd+9gUMnbs3BEQhHOycrTE33VwV2D/lRQ/0n2ck2McM7v37FYxTRFj185dSDqYVObhy7YQZCwSPys7Sz3et3efLdso86AhyKelp2HPnj04kHnA1mNIPkvWY9S8AJCelm7PZ2Cf8RkQgU3iyXqY62XHPt55YKe6VZ0rt27Fjl07jMd5+fZ8xvYa45X9asYzv4f27t6LbQllW4bp/BLkOzT3QC6ycrIOxU8q+zrId76wa88utQ7pGenGsg8cLLKN6iQazQmEykmVcVrd0wq9p1ZCLXX7z9Z/it2+2/dsN+5kI+z7ZP9WRVV1f83ONfZ9p8TZBCzSyRcpHpnDiagm++eTTz7Bddddp+qxlRa5qCulSkzk97NJkyaoU6eOarig49iSCxgSP15OZuJpfeNxneNtfeNxneN1fXftqoOBAxPw88/Gxdbjj8/Ha6/lo23bSgDkzz/E6z6uY/P6ipkrEnzXK+Oss84K3JeuVSK8NW3aFB999BH69esXMxOwShuMD26FpAqoK9UVbSClotFRrnJqZdSqbZyACfXr1VdFs8uKXFUWkismG2MuMH/UrlW72HWI9CA3t3VShSQVT26FaqnVbNlGO8sZIoCMW+IlVjAO/6rVqtoSv1LWoS/j2rVrIynJGH9qlVTb9nFigjHm6jWqo/L+yoEPu474OfsNAaNCYsnHaKT7uGIF44sptaqxTUyhrW6duqhbpa5tX3zyGZD45ROMsdSsUdOWbVSlkvEZSKmUohwV5j6uVs2eY7R2fu3Cx2hSoq3xD1Q4EBDCJJ6sh1C5krG9yrqPE/YfKuhfp24dVM8xXCdJicZnuqzUTK+pbssllDsUL8LvoUjIyjVENUG+Q7PSsgIuRulwaMc6JFdILvQZqJhiHLNVKlcpEr9tk7aB+1e2vRKHNz680OttGhk12rYc3FLs2BIrFnyuU6uHfZ/s32a1DQfdrqxdtn2nxNsELNLJFykeca21aNFC3Zc6bAsWLMALL7yAyy67TGVEyIUOq6tNhP3iSoYkJyerv2Bkv+s62ZBjS2f8WCPe1jce1zne1jce1zme1jc7G3jhhSoYPToBmZnlULky8PjjwK23lkNCgn8LscXTPta1vpHGck1oEyEiISGhSEv2kiZL0SITMSmmu2rVqpiagOUhz/ZlSHpWgHKFxRM7lmHGl5N0iWeKJJHEj+QgTyyfWCi+WeNMlmvH+E0RyXQvmGl6slw7t4+i3KF9bNf4ramX1gYFOuKr46dgEZEeo5HsY2sdKus62HaMWjo3RnuMRvsZUOmWBfvYrmPIPEZl3NbPgO74st3s2Mdm/CLHULnyWsYvmOm1duzjRMtPYn65wvs4KSHJlnWwxih0jIbYx0fUPCJwf0DHAUVeP7ymIbyt3bu22LFl5hrOyEpJlYp9X73K9QJdRzkBKx3xst3cEEXF4Suim1zg+P7771UXemHFihVYt26dSjUlhBBCSkISbM49txxmzDCy3c48U2rFA02buj0y4icS3bxaKRMmmSydf/75gYmUPL7ttttsW056ejr+/fdfXHPNNYglTJHHy11HvVpI3ZGuo/KfQ/vY6/HtXIZfjiFd8U1x06mGIF7rOmoVamNhH7eu3RoXtr4QtVNqo2ODjkVeb1rNmBFuy9iG/dn7lZBWlmYI9SoZQtvWjK0qvbjQxQNCHEKyDCQ7QWrrSiru+++/j59++gnffPONcvdKdoJkIdSsWVM54QcNGqREtnCNEAghhBAr998PzJhRDpUq5anOotdcU57dRIntuJo6KhMlqbvRqVMndOnSBaNHj1b1uMwupNdeey0aNWqkaqgJki6wdOnSwP2NGzeqGh6S0mimGNxzzz2qNbyki27atAkPPfSQcs5dccUViCWc6kgZXODbrvheFBn82nXUrv3rVnzr83bGt956Jb65rc1jx6nxW92FdsQ3YzvRddRchlUkKy3W7RALnXFF6Pr00k/DxqpesTqqJlfFvsx9qvNo6zqtMWvtLBXzlGanhGyGUBwi6KmmDPl5SrxrkNqglGtJSOmR+oAy/5M6uyKsSRkQEdlOP/109fqoUaOUc1AcbeJy6927N1555RW3h00IIcQDfPop8Mwzxv3Ro/fi6qurUWQj/hPapNaGNBwYPny46hbVvn17fP3114EGCZIKYE3DEOGsQ4cOgcfPPvus+uvRo4e62ils2LBBiWo7d+5UdVdOPPFEzJs3T92PJZwUSew+ifaqiBEc324hLJzIYOc+tjqS6GgrPr71lvELx9ct5JmxA51rNW0f6307l2Een6rjruZ9bIqGpXGPyfeBuNqWbFui0kdFGDvjnTPUMvYM2ROozXkwNzJHm6Sv1q1cF1vSt6j0UQptxA3Gjx9fYh28MWPGqD9CCCEkEiTR6IMPgJtuMh4PHpyP884zSmsQogPXmyFImmi4VFFTPDNp1qxZIB0vHB9++CG8gFMihl0iW3B8rSJAwT5m2lz4ZTgR30/pzXa4nazxTXHE7m1URAwuiO81sdyMbbsbrECMsh47OsQ8UyR0Qmgra/xm1ZspoW3NnjUqhlmPLTMn85DQVpA6mpJoNL8ojgZVGhhCW9pmgDobIYQQQjyONFIfOBCYPNl43KuXND7Ix65dbo+M+BlW7XUJHSKJ6ZzSLcKYbhi7RQanhLBgN4/dqbVOpY76pQacncsIFsJiXSRxK74gx48uoVB76miBuGYuR7Crnlg0qZ2xIAabddokdXTu+rmB563ptZGmjgr1qxjNiDalbSrVeAghhBBCYilVtE0bQ2RLTARGjAC+/NK4T4hOeIi5hHYRRnN8L4sM2uprBdV30i12et0xx9RR9+Kbsb0aX3fqqDWu7vRXUxAri6NNWLN3DXYf2B143rqNIm2GIDRMbahuJXWUEEIIIcSLiFtt0CDg/feNx23bAm+/DZhVqPIOTZMI0QIdbS7hRTeS1TFnvfVaWpuu+IWK2efna0/fpdBWfHzrrVfim8eKrvRprwttobqOllWo8voxZAptq3evxrwN8wLPW11/gdTRpMhSRwWVOkoIIYQQ4jGmTweOOcYQ2aTcu3QZXbDgkMhGiBPQ0eYSXhdhvHAC6nR8M5ac+PsptdNWITJEMwc7l+H1Y4iOttJ3HY3V1M5I05tLm/ratLqROvrbpt8KbZdCqaM5pUgdTWfqKCGEEEK8Q3o6cPfdwLhxxuNWrQwXW5cubo+MxCN0tLlEHjSmFVrcVBTawjvydAthOtJ3Q9Xh0965VrMYbH3ezvjWW6/Gd6ozru70abtFqlCpo7qFsFhNTTUdbVZhzRo/2tRRux1tQ2cMRcuXWmLn/p22xCOEEEIICebXXw3HmohsMh0VwW3hQopsxD0otLmEF91IhUQkKaSuSQRwqhC8112Ffokfy/WvYkXI09Www2vHUHDXUWsXal3LiHWxtlZKLVRKqlTk+ZCpoxF0HTVrtNnVDGHS35OwatcqzN1wqFEDIYQQQogd5OQAjzwCnHACsGoV0Lgx8P33wHPPASklT3sI0QaFNpfw2glucHxTZPOs0OajZgJecuSFE9rsEjt1d4x0qyOl3a5Orx6jAbdZgYhkdXF5ZR/YnZoqnx3T1WaHo+3wGoer241pGws1VygtezP3qtut6VvLHIsQQgghxOS//4AePYDhw4HcXOCyy4DFi4FTT3V7ZIRQaHMNv9TvsnMZXq9PZY2lO7VTh6PQGsuLIkxwfOutV47RQDONgn3rl8+A7u3vxDJ0OebscF02rWbUabMeQ4VqtGVHXqOtdqXaaFGzhbo/f+N8lAX5ntp7sEBoy6DQRgghhJCyI6fRUnutfXtgzhygalXgnXeADz4AatRwe3SElKEZwsqVK/Hjjz9i27ZtyAvqjTtcJGVSIk6JJLpEHi+e4DohMliL/VMIcze+9daLx6iOWoted3WGcxSWpZlAca65/HLeSJE3HW3NqzfHjv07kJaVVmpHm3BCkxNUuuec9XNwZoszSz2u/dn7A4LflvQtpY5DCCGEECLs2gXcfDPw8cfG4xNPNES2ZkXN/YR4S2gbN24cBg4ciNq1a6N+/fqFUr7kPoW2yNAphNHRVnx8J7aROj33iWvRq/GttzxGnYlvjSWxddXIC25UoEvMM5vWOJI6WgahsE2dNur2lGan4LPlnxVKr5XvoUCNtqTIipV0a9wNE/+cWOa6ambaqEBHGyGEEELKwowZwHXXAZs2AYmJwIgRwP/+ByTYc62VEHeFtkcffRSPPfYY/idHNSk1XhQxCrm1NBQh156WF9QR0YuOM+s+8HLqq9eFNt0dKc3Ydou1RYQ2eMzRFpx2aSn4b/v3kPyn0TEXvA5lGX//jv2VW61vq76YumJqoWM0Kzcr8F0XjaNNmLdhnhpfadfdTBsV6GgjhBBCSGk4eBB44AHg+eeNx0ceCbz3HtCpk9sjIyQ8Uc/sd+/ejUsuuSTaf0aC0F7I3iE3lR/S8rQJSXCmqym7poaOb7314jGqYx8Hi81eE1OdrtHmpKOtLPErJFRAv+P6qfpqwfFNN1s0Qps45FIrpCI9Kx1/b/+71OPac3BP4D6bIRBCCCEkWpYsAbp0OSSySdrowoUU2UjsE/XMXkS2b7/9Vs9o4ggdThLzZNnp1FG7TtK9npZnjeWH+F5PfbXexrJIEokQ5tV9oM1RWOACK+Q4K2XXzuKWoUXIQ5jUUZvGb7rPzG1kFdqSE5IjjnF84+PVfanTZkfqKB1thBBCCIkUKQM/ejTQubMhttWpA0ybBrz6KlC5stujI0RD6miLFi0wbNgwzJs3D23btkVSUlKh12+//fZoQ8YnBZmXXjnBDRffzmVY65uZy9ER3w9ClRcL2QeLDBTawsfXvY/N7e81MbW4Zgg60lN1psibQpiuOnbBjjZxs0XjjDyh8QmY8d8MVaft5k43lzl1VEQ3GUukrjpCCCGExCc5OcDFFwOff248PuccYPx4oF49t0dGiEah7fXXX0eVKlUwc+ZM9WdFJvEU2iLDL2l/di7D6sjzqtBWyFXosfpXfovviNBm8z6m6zK6ZiOmSKVjGV5NkTfFSHPbHMg5oG6jFbi6Nelmq6PNTB9tWr1pqeMRQgghxP88+KAhslWsaKSMSrqojafMhMSm0LZ69Wo9I4kz/CLCeCotz5Liqt1xln+o66iu+lc6xu9G+rGO+MGphV45Rh0Xgz22j8NtHzsvWrj1PWd3s4Xg1NFohbaujbuq21W7VmF7xnbUqVynTI42s/MohTZCCCGEhGPyZOCpp4z7b78NXHqp2yMipHSU6czBKiaQ6NB5gqs7rdAa385l+CUtz6siht/iW2+9eIx6tTNuqPh2iUjWOIXi21TfrJBQlZ+r3TFnV9fR4jqzZuZkRlWfzaR6xeo4us7R6n7fD/ti+j/To/69D3a0sU4bIYQQQsLxww/AddcZ9wcPpshGvE2pZvYTJ05U9dlSUlLUX7t27fDOO+/YPzof43URQ7eTRIujLVzHRR3pu9AjkjjlRirUldWj6c0662s55br0WldQ3fHdSq3VKYTpTh3NyctRt4nlozaw48GTHlQdTaVO27kfnIs7vr6j1F1HBXYeJYQQQkgwmZnAvfcCvXoB6enAqacCTz7p9qgIKRtRz+yff/55DBw4EGeffTY++ugj9XfmmWfi5ptvxqhRo8o4nPjBqSLkXhVJdBQJ97rIEC4+RZji43tJJCnkurSInbrFVK/sY2sc5Tiz2Q0W3HDBFMsdSR21uWtq8PdoaVyFV7S9AqvvWI1+Hfqpxz+s/iGqf09HGyGEEEKKY9kyoGtX4NlnZW4KDBhgdBdNjP76ICExRdSH8EsvvYRXX30V1157beC5Pn36oE2bNnj44Ydx11132T1GX+LFlDCrm0r3+K23di/D3D5eExmcFkm8eIx6vY6gW44tr8S3ilFeHH9wfB0XFExBLdDVtOC2tEJew9SGGNxtMMb/MR4b9m0oVY22upXrYlvGNlWjjRBCCCFERLVXXgHuuQc4eBCoXRt44w2gb1+3R0aIPUQ9s9+8eTNOOOGEIs/Lc/IaiQy6kYqPb731YuqlNXXRq65Cv8S3cxlupTd7fR/oFiLtqgHnh+85HfEbV20ccKilZ6VH7Wg7qtZR6paONkIIIYRs3Qqcey5w222GyNa7N7B4MUU24i+innm3aNFCpYsGM2nSJLRs2dKucfkeupGKj2+91eUI01JDrUAo0RXfyyKJE/GtqZ1eFNqCl+H1+HandhZKHc3L1Tp+3fEDjjbTcWZXw4igGm1lSR01SU1ORdXkqur+xn0bo3a0HVnrSHVLRxshhBAS30ydCrRtC3z5JZCcDLzwgnG/QQO3R0aIy6mjI0aMwGWXXYZZs2ahe/fu6rnZs2fj+++/DynAkdB4/QRatyPPeuvVdfCNmArN8eloK7IMc/xO1XL0SvzgrqM66jiGEmu9dEHB7tRRk0apjbAvc59KHz2qtuFQKwk62gghhBAi7N4N3HEHYPZPbNcOeO894Jhj3B4ZIXqIemZ/0UUXYf78+ahduzamTJmi/uT+r7/+igsuuEDPKH2IdKX00gmu0/Gtt15dBy2prwWOOa+On0JbyZjCrwipATGY6cfFdmX1ophtioS6U0ftcLRZ00ejqdNmOtpMYY5dRwkhhJD446uvDEFNRLby5YH77gPmz6fIRvxNqfp5dOzYEe+++679o4kjdDhVnBJhnCjEb731igjgN8ecp0UeeFdo87IQpju+HIvynwhgIiDZ3bFTd+prcPxCQlg5TamjdjnaqjZStxvTIksdld8J09Fmpo6mZaVhf/Z+VEqqVKaxEEIIIST22bcPuPtuYPx447FUmXr7baBbN7dHRkiMCG379u1D1apVA/eLw3wfKZ5AoXyPizAU2oqJr9G16HW3k3X8dLS574r00jEkzqycvBztx6gS8qBXyLPexvox2jg1OkebNE0wl92kahNUTKyIgzkHlauteY3mZRoLIYQQQmKb778HbrwRWLdO5plG2uhjjwGVeK2NxAkRzbxr1KiBbdu2qfvVq1dXj4P/zOdJZHjxBN0PQpuTqZcFWWee3Qd+qQ9m5zL8ILS5El9TnT/djjMtQmGBaOdYjTaXUkdNN5usrzjY6lepr7VO264Du9BlXBeMmjtKS3wSGVlZWVixYgVycnLcHgohhBAXEE/OLbcAvXoZIlvz5sBPPwGjRlFkI/FFRI62H374ATVr1lT3f/zxR91jigu8WKPNydRUcznaliH/6UjfDVFfy0v72I/xvSy06W544eXOuNauoHZ17AwWwnzRddSl1FGzPlv1itXVb0e9yvWwZs8abZ1Hf177MxZsWqBcc3d1u0vLMkh49u/fj0GDBuFtyQkC8M8//+Dwww9XzzVq1AhDhgxxe4iEEEI088UXwMCBwIaCa3Jy/+mngSpV3B4ZITEqtPXo0SNwv3nz5mjSpEmRVCMRLdavX2//CH0K64MVH99669kaZPBeWp4f49u5D5x0XequheikGOxVIUy3q9N6q0sMdtvRVq1iNXXrhKNNEKGNOM/QoUPx559/4qeffsKZZ54ZeL5Xr154+OGHKbQRQoiP2b7dSA394APj8eGHA+PGAT17uj0yQtwj6pm9CG3b5dMUxK5du9RrJDK8fgKtu5mD9dbrQo8Ox5zX3Uh+iG+9pWDuv/hWx5yXjqEiqaM2OdpMoW1bxjZk5WZF7GirlmwIbeJo09l51BTaDuQc0BKfFI90oH/55Zdx4oknFrqw0aZNG/z777+ujo0QQoge5HRQ+iO2bm2IbNJR9J57gCVLKLIREvXMXgSWUO6Q9PR0VKxY0a5xxQ10O4WOb7316jr4pcaZF+PrEkl019fy0z7Qvo/zcwNuLS3jL/jPa11HdX2P1kqpheSEZHV/U9qmEt+/5+AeRx1tuw/uVrcHsim0uYFcgK1bt26R5zMyMmyd5xBCCIkN1q4FzjkHuOYaYOdOoF07YP584JlnWIuNkIhTR4W7pTdvgTA0bNgwVLJ8gnJzczF//ny0b9+eWzVCKPIUH996a2famdURpl1k8GAdPj+6qbzUMTJ4GYxfFPP7QNc+tsbX3WxBi6MtuEabTamj8tsvddr+2/2fSh9tVr1ZZKmjBY62w2scrm4/WfYJhvUYhoapDWEndLS5S6dOnTB9+nRVk00wxbU33ngD3bp1c3l0hBBC7GTVKuD44yWjDahQAXjoIeDee4GkJLdHRogHhbY//vhD3YqAs2TJElSQT1UBcv/YY4/FPeIVJRHhxRPcUKmpuhx51lst66C5/pVusdM6fq8LYV6Nb73VuQyd8XWPX6vjzEGx3E6x36kabXanjgqNUg2hbeO+jZGnjhY42i475jKMnj8ai7YswlWTr8KMa2bYul2tjrZwznuij8cffxxnnXUWli5dqjqOvvDCC+r+nDlzMHPmTLeHRwghxCYyMoALLzRENvHYSMpoq1Zuj4oQDwttZrfRG264QU2gqlatqnNcvsfrIoMfRAynapx5VQhjDbjw8UW8MIVUu5dhHi+mSGV3fD/tA52OM+s+1pX6ai7H1q6jFkeedTl2xA/XEGH6P9ORkpSCns17FnG0VU+urm4rJlbEpIsn4bjXjsNPa37Co7MexUOnPAS7HW3yvZWdl40KCYcuBhL9SG02aYbwxBNPoG3btvj2229x3HHHYe7cueoxIYQQ7yPTogEDjBps9eoB06cDDe01qBPiG6I+exg9erS6WhmqGcK+ffvsGpfv8fIJrl+ENh1CmLVjJIWq+Iivaxk5eTlahTyv7gMnu4568XuuSOqojY62UEJbelY6Lph0Afp80KfQMRvsaBOOrHUkXjv3NXX/0Z8fVf/WLnYfMBxtAuu0OUt2djZuvPFG9fs3btw4/Prrr8rN9u6771JkI4QQn7BjB3D99cD77wMJCcCkSRTZCCmOqGf2l19+OT788MMiz3/00UfqNRIZXhRhnDxBt97qdJPoiu+HfcD4kcXXtQyraMGmKcXHtzMF0RSknEgv15k6qiO+pI4KG9MOpY5mZGUoB1lGdkagAUKoGm0mV7W7Srnb5PjesX8H7Ha0CazT5ixJSUn49NNP3R4GIYQQDeTlARMmAEcdBUycaDw3ejTQo4fbIyMktol65i1ND0499dQiz59yyinqNRIZZkqSV9MKvXgCahUsdLmFHE3t9HjDC8+7nQrqd+lahh+OUd1iuRfHHza106aGDmb8QI02zamj1osWVldZQGizONpMUiukqtu0zLQyjym4RptAR5vznH/++ZgyZYrbwyCEEGIjf/9tCGr9+hk12cSkPGcOcNttbo+MEB/VaDPJzMwMmToqqQMHDnByGyl0O0XWbEHXMsyTUCeEMLqRfBhf/rMcozo6m+oW2nQ7tnTFL9QV1OPNFpxwtNnaDKFqo6JCm+W71Cp2me62YEebkJqciu37tyMtyx6hTda1UOooHW2O07JlS4wcORKzZ89Gx44dUbly5UKv33777a6NjRBCSHTs3w888gjw7LOAnPZXqgSMHCnf5ewsSog2oa1Lly54/fXX8dJLLxV6fuzYsWpyRSLDdMNorz/mQZHHCSHMCbeQzvRgqxjJYyh0fN0ijLZmCAXb2wkx2NNipwOOOd0p/o7UaNPgaNucvlkdnxIzrKMtRI22YEfbvkx76rpKHHNfCXS0Oc/48eNRvXp1/P777+ov+DuNQhshhHiDL78EBg0C1qwxHvftC7z4InDYYW6PjBCfC22PPvooevXqpbpLnXbaaeq577//HgsWLFBdpkhkePkEVLeI5IQQ5tUabUxNjb34fjhGvZjCLkJPoEabBkehEms1djUt4jgrryl11EZHW/0q9dX45ft5W8Y2NEhtENbRFug6WtHoOhrsaLMzddRan004mHPQlrgkclavXu32EAghhJSBDRuAW26pjunTjXlKkyaA+GpEaCOERE/UZw/du3dX7dqbNGmiGiBMmzYNLVq0wOLFi3HSSSeVYgjxiZdFDN0pZ04IbboKzZuCBR1n8RPfkWNUkxDm1PeQnUKY7jp81tRU03msS8hzInXUzviJ5RNRu1JtdV9SP4USHW0hUkerJldVt3aljgYLbUwddRfVddvi+CWEEBK7SGroCy8AbdqUw/TpFZGQkI977gGWLqXIRoijjjahffv2eO+998q04HjHdITpEEnUJNeDXU0LpY5qdvM4WWjeTpHEPF78JFR5KX4oEcYPx6hX42txnMHbNdrCpo7aJBYmlU8qdHyGcrTJOplpoU40Q7AKfAJTR91h4sSJeOaZZ7By5Ur1+Mgjj8S9996La665xu2hEUIICcGvvwIDBwILF8qjcujYMQtvvJGI9u3tO3chJF6JWmhbt25dsa8fxgTuiPD6Ca7OlDOdIoM5XkdEDIfq8Hk1vpfFYKcdbV7fRrpSa51KTdWaOmqzEOZ4amoIR1t6Vnrg8x2yGYIptNHR5huef/55DBs2DLfddpvKfBB++eUX3HzzzdixYwfuuusut4dICCGkAOkgOnQoMG6cGDSA6tWBJ57IQ58+u1C/fl23h0dIfAptzZo1K9aFlZt7aNJNwuPFE1zdbiqraKe9RpsDhebN2tyeFaooFEYUX1f6MYW2CFyFZmqqTSKS06m1OhpqBBxtQUKYXUJeEcec5bvUFLzMjqPifquYWDFsjTa7miFYa8MJdLQ5jzTIevXVV3HttdcGnuvTpw/atGmDhx9+mEIbIYTEAHl5wFtvAf/7H7Bjh/GcmI6feQaoUwfYts3tERISx0LbH3/8Uehxdna2ek6uZj722GN2js3XePEE18nUUTZDcLdOntcL5Xu1WYQ1nlXAsFXIczD9WHfnV6eEPN2OOTuXEXCcBQlhjjjaCgQva8fRUMeu3amjbIbgPps3b8YJJ5xQ5Hl5Tl4jhBDiLn/+Kc0OgDlzjMdt2gCvvAKcfPIhEY4Q4qLQduyxxxZ5rlOnTmjYsKGqzXHhhRfaNTZf43WRwetCmxOF5k1HGJsV+DOtUIeIVGgZmuN7fh9r2gdOujqtIpVdQpjjNeBC1GgzO46GShst1HU0S1ONNqaOOo40xZIGWffff3+h5ydNmoSWLVu6Ni5CCIl39u0Dhg83OoiKmFa5MjBiBHD77UCSUXaVEBIrzRBCcdRRR2HBggV2hYufZgh0CxWJ77TQpiO91smuo15ybLGZQ/THqJ3fEX7YRrq7jlqFTh3NFkKN31OpoxHUaNuavlXd1q0cus6L9q6jTB11nBEjRuCyyy7DrFmzAjXaZs+eje+//14JcIQQQpxFpvEffgjcfTewZYvx3CWXSE1NoHFjt0dHiP+JWmjbJ7K4BTkZl7QAqcHBq5aRo7v2j5fT/nQ6zkyhp1B8XZ1fuY8ZvwzLMI9RXfH9IAYHarTZJCIFxzcviOiqAadDaAt2tNmeOlqwrc3j0/pdajraNqcbqYINUhuEjGF719GC5crYRPijo815LrroIsyfPx+jRo3ClClT1HOtW7fGr7/+ig4dOrg9PEIIiSvkdP2ii4AZM4zHcor+8svAGWe4PTJC4oeohbbq1asXESZEUGjSpAk+FNmcRISXT3Dl5FOniBTs5tEhhOlwqgTH19GswBTVvCok+Sm+jv0bSgxm6qiz6cFOOeZU6qgl7dJ2x1m+HkdbYvnE8KmjBY62LenGpfMGVRo4kjpqOtpE2NuwbwMdbS7RsWNHvPvuu24PgxBC4ho5Rbv+ekNkq1gReOAB4N57geRkt0dGSHwRtdD2448/Fnpcvnx51KlTR9XnSEy0LRPV9+io/eP1tDyroKZbZNAd3+vNFhjfnfhOisFe3UbWZgU6tpEb28dTjrZiUkdFOJPv1s1phqOtfpX6xTra7O462ii1kRLa2AzBeb788kskJCSgd+/ehZ7/5ptvkJeXh7POOsu1sRFCSDzx1FPAZ58BFSrIeTvQtavbIyIkPolaGevRo4eekcQZXjzBdUpkkNhOCWG6hTxdqZ26XYWqDLwHjyGnOkY6IrSxGULcxNdZo83JZgjCnoN7DqWOluRos7nraMPUhuqWqaPOM2TIEDz55JNFnpffKXmNQhshhOhFTgsmTjQcbII0P6DIRkiMC21Tp06NOGCfPn3KMp64QXf9Lt2pnTpFACeENt2F5q0nn14XAdg11bn41nhMHS35Mxao0aahhpruzrW6uo7qTh0tztFmpo+WVKPN7mYIZsoqhTb3WLlyJY4++ugiz7dq1QqrVq1yZUyEEBIvzJtnND2YO9d43K8f0L+/26MiJL6JSGg7//zzCz2WE29TyDEfm+TmFp50k+JTR70kYlhFqYBQZeP4nRAZzHVg6ijjx2J8azzdnzFdYmqoOoJ2CmFO1lDTkeIfztFml+ivPXW0BEebpHGaqaNhHW0FqaPpWelqnGXZvpk5mcjIzigstLFGm+NUq1YN//33H5o1a1boeRHZKleu7Nq4CCHEz6xZAwwdanQXFSpVAv73P3EZy9zO7dEREt9ENLuV+hrm37fffov27dvjq6++wp49e9Sf1OY47rjj8PXXX+sfsU/Q6TjT3U3QFzXUNNW/ChbyvCwk6Sj273UhrJDIo0GEcVJs9uo+COUI05V+7GTnYLvEzuDUUacdbdsztmNbxrbiu44WpI4KGVmGSFbW+myyDc2acHS0OU/fvn1x55134t9//y0ksg0ePJiZDoQQYjN79xpiWqtWhsgmU4gbbxR3MTB8uFGfjRDisRptMpEaO3YsTjzxxMBzUvy2UqVKGDBgAJYtW2b3GH2Jl09w/SC0ORXfdrdQiIYXXnJF+im+DhHGKoQ50QzBy2KqNbXTLhHJCSHPOn673WYhHW35ehxt5ndcsKNtxc4V6riScdSpVCdkjJTElECZAGmIYBXeSps2Wr1idVROMpxTdLQ5z9NPP40zzzxTpYo2btxYPbdhwwacdNJJePbZZ90eHiGE+IKcHOCNNwwxbft247mePYHnngPat3d7dISQMgltcrWyevXqIdMG1oh/lUSEl0UGLzvO3BDavLiPHY0PzfE1uUadaqjBGm3uNLwoVANOk5Cnc/uYx6fdy0gsnxiyBpzJsu3GxbZ6leuFFffk4oCkj+7N3FvmOm1mI4SaKTWRkpSi7rPrqPPIHHDOnDn47rvv8OeffyIlJQXt2rXDySef7PbQCCHE88i1XUkcu+ceYOlS47mjjgKeeQY491ymiRLiC6Gtc+fOuPvuu/HOO++gXr166rmtW7fi3nvvRZcuXXSM0Zd48QTXGsvrQphukUR3MwRd6cF+Siv0vNjsg66jOr4nvH4M6RYKw9VQ05Y6GuRoW7rDOAMw0zjDIS42JbSVsfOomTpaI6WGcsoJTB11BxFQzzjjDPVHCCHEHpYsMQS2b781HteqBTz8MHDTTUBSktujI4SEI+rZ/YQJE7B582YcdthhaNGihfqT+xs3bsT48eOjDRe3mClbdna9DFf7x+74fmhWoL2ZQ75+R5uX0/68Ht8PYrOkvnp5H8j217EPrHX4dNdos7t+WnB8namj4RxtS7cvLbY+m92dR62OtoqJFdV9po46x9y5c/HFF18Uem7ixIlo3rw56tatq0qKZGZmujY+QgjxKlu2AAMGGCmhIrKJqCaCmzRyvu02imyE+M7RJsLa4sWLVXrA8uXL1XOtW7dGr169bBct/IxTRba1iQweTZszj1FHa7RpFlN1H0NeH782R1u+Q2KwjdvfD/sgVNdRL6V2OpY6mueOo23PwT3FdhwN7jxaZkdbQY22GhVrBFJH6WhzjpEjR+KUU07BuZK7pJwXS9CvXz9cf/31am74zDPPoGHDhnhY7BeEEEJK5MABYNQo4IkngPR047mLLwaefBI44gi3R0cI0Sa0WdMDpPZGcnIyBbZS4MUTOCfT5rxaAy64kL3dy/C6COCX+E6IwU40Q/DyPrA2Q9AWX0NnWSe3vxuONpMShbaCBgi21mgzU0fpaHOMRYsW4ZFHHgk8/vDDD3H88cdj3Lhx6nGTJk3w0EMPUWgjhJASyMszOohKN9H1643nOncGnn8esPQgJIR4hKhn93l5eWpS1ahRI1SpUgWrV69Wzw8bNoypo1HgxRM4q6DqdaHNqfpaXq6h5mXXpbWQPVNH/Sm0+SG+jq6jwUKY3etQkqPNpKTUUdPRJl1HbanRZnG0sRmCc+zevTtQr1eYOXMmzjrrrEJ1fdebZ4yEEEJC8ssvQNeuwFVXGSJbkybAe+8B8+ZRZCPEq0Q983700Ufx1ltvqVbuFSpUCDx/zDHH4A3pN0wiQnche531u/wghDmaOmqj45OpnbER38nUUa+JwU6lRlprnOmK7+VjNOBoszt1tCCOeXya+8Csjxa1oy1Tg6Mt50DgQgHRi4hs5gXXrKwsLFy4EF3lbLGAtLQ0JLGQECGEhOTff4FLLgFOOglYsACoUgV4/HFgxQrgyiuB8vZOAQkhDhL1x1eK3L7++uu46qqrkJBwaOJ+7LHHBmq2kZLxolvIKujorh/llMjg1fF7vZC9k/HtdAs56WjTVQcxuE6hH1yLOvax7mPIsRptdqeOlg/d1bR2pdqlcrSVNXVUOpcK1SpWCzjaZLtm52WXKS6JjLPPPhtDhgzBzz//jKFDh6JSpUo4Sc4YC5CavkewqBAhhBRCesTcey9w9NHAJ58Ygpo0PpBGB0OHAinGzxkhJJ5qtEl3UWmIECqlNDubE9toU0d1u53sTh2V/8Qt51XHmeNdTT1ayJ5CXvHxPe1oK7i+4nWhzevpx7q6jhapoWazoy2xfGJIIa9OpTrYsG9D4H31q9SPrOtoGR1tWblZ6jY5IbmQq07qtFVIOOS6J3qQUiIXXnghevToocqJvP3224WyHaRTvdT0JYQQYiBei1tuke9H47F8RT73nGSHuT0yQoirQtvRRx+trlw2bdq00POffPIJOnToYOfYfI0XHW1mPDmx8qrQptstZD1J1xnf6zXUvB7fSTHY7mYzodKbtbsKNQhJ1hpn2o8heFPIs95qa4YQxtFWktBml6PNFNpEVBOxzbwYJOmj1VCtTLFJydSuXRuzZs3C3r17ldBmzXQQPv74Y/U8IYQQA+kVIyKbuNik+YGkjhJC/EfUQtvw4cNx3XXXKWebuNgmT56MFStWqJTSL774Qs8ofYgX3UhmPDnB8noheK8LeU4KVV5yXVpFGK83Q9B9DHnd0aa766hXxeBwqaN2LSMgtAXFr1WpVuA90pgguGabrq6jAUdbotEBXZYrIhs7jzpLtWqhRc2aNWs6PhZCCIlV5swBBg0y7j/2GEU2QvxM1DPvvn37Ytq0aZgxYwYqV66shLdly5ap504//XQ9o/QhXjyB85MQps2NFFT/Sld8R7qOamyo4VURw0yf9nTqaJDr0g8NNXQ55nR0hw7lyNNVA05LM4QwNdrEUWamg5ZUn83OrqOZOZmB5QvsPEoIISTW2LoVGDgQOPlkaRwDXHAB8L//uT0qQkhMOdoEKXT73XffFXn+t99+Q6dOnewYl+/xohvJD0KYU6mdutP+vCpUeT2+n1ydTnQd1dIV1CHXolfTs4ukdtrdDCGMo02eFyebCGcldRy1s+uoNXVUsHYeJYQQQtxEype/+CIwYoR0YTaeE5HtrbfkHMHt0RFCdBL17D49PR0HDhSewC5atAjnnXcejj/+eDvH5mtMt5CXnCRW4ciptDbdjjOvCpFy/HgxtdPr8a3xzK6GXheDPZs6mper5XvI66mpRVJHNTnazOPHvFVCW0qNiB1tgWYINtZoE8yUVaaOEkIIcZMffwTatwfuuccQ2cSLMnMmMHkyUNX4CSSE+JiIZ/fr169Ht27dVB0O+bv77ruxf/9+XHvttUpgkzTSOZJ4TiLCi04JP9U483p8rwpVXo9vjacrdVR3+rFfhDYn4usU8nR1HXWrGYLEF0ebEJGjrYImR1tB6igdbYQQQtxg40bg8suBnj2BpUulaQwwfjwwf76ROkoIiQ8iTh299957cfDgQbzwwguqAYLcSvdREdn+/fdfNG7cWO9IfYYXT0D9lDrq9dRUr4sYVkeel8ZvjedUMwQ7Xa9+ENpMwahQjTYba5zpju9W6qhdy0gsnxg2dbRO5TrqfqPURiXGsbsZQpHUUTratDN16tSI39unTx+tYyGEELeR2mujRwMjRwIZGUZXUanL9sgjQA3jOhQhJI6IWGiT9u0isHXt2hWXXnop6tevj6uuugp33nlnmQYwZswYPPPMM9iyZQuOPfZYvPTSS+jSpUvI9/7999+q+cLvv/+OtWvXYtSoUSGXH01Mt9BRZFt3IXtfCGFwVmjTJZI42azAS+nNViHP6zXU6GjzZ2qnk/GdbIYgz9/T7R5UrVAVV7S9IqpmCOLwLm0KfKDraEKyumUzBOc4//zzI3qf7Nvc3EM1IUviiSeeUPPN5cuXIyUlBSeccAKeeuopHHXUUYH3yIXfwYMH48MPP0RmZiZ69+6NV155BfXq1SvVuhBCSFmQ0uXSTXTFCuPxCSfI+aiROkoIiU8int1v3boVzZs3V/fr1q2LSpUq4ayzzirTwidNmqRSUB966CEsXLhQiWIyWdq2bVvI90uq6uGHH44nn3xSCX12xHQLL57AWeMFapzZLCSZJ1vZudnerAFndqTUHN8PIoDO+H5Ib3aqTqH1Oc+ldmoQUwuJ2ZpT/HV0HS1So83BZgidG3XGuD7jULdy3YgdbXIcZuYanUNLA5shuEdeXl5Ef9GIbMLMmTNx6623Yt68earxVnZ2Ns444wxkiEWkgLvuukt1u//444/V+zdt2oQLL7xQw1oSQkh41q0DLr4YOOMMQ2SrW9dodPDzzxTZCIl3ouo6Wl48sJb7FSoYE9vS8vzzz6N///644YYb1OOxY8di+vTpmDBhAoYMGVLk/Z07d1Z/QqjXSxNTkKuh8meyb98+dWtOEu1GYponcHJr1zICMZGPnNxDQpid61BEaCtXcnxzfSMZh+k4sxaat3P8gWYO5gk6bI4fQmjTdQyZIoYY2+xehlVkMJdnyz4uCGkVGWw9Rg8NOSDW6voMBI5RTceQVUTSEd8qZkcSP9J9HBi/xXFm/nu7XaPW9Fq7jyHd8WXbBL4nbDyGConxeZF/ziLdv9bfAHlvab/rKiVWCtzfe2AvKlQu3XzCFOkSyyWq5ZvNEDKyMmz9bYoUHd/38cbXX39d6PFbb72lLvBKNsPJJ5+MvXv3Yvz48Xj//ffRU4ogAXjzzTfRunVrJc5J5gUhhOhEThufew547DExggAJCcBttwEPPwxUr+726AghnhLaZDJ65JFHBoQK6T7aoUOHQuKbsGvXrojiZWVlqUnT0KFDA89JrF69emHu3LmRr4ENMSVNYYT0XQ5i+/btKj3BbtTJSYEQtnfPXtvcdjsP7Azc339gv7rNSM+w181XcJKYccC4spx5ILPE+LK+MjGWYyj4eCny3lzjJCUt3ajbk5uTa+v4Dx4w9ufBTONW9oOd8cV1qeJnFRw3+bA1fvq+dHWbmZUZEDF27dyFCgfLJnqb7NtriMyZ2ZnIysk6dIym2LOPd+/drW7l5N88RmWb2bWNJA3NZE/aHnWblZll6z4whYyM/cZnICfH3mMo86AhHBzIPKD1M2B+hkUgiSR+pPt4f0bBfj2wH9k5hpi3d/debEu2Zx0O7jfGn5aRhv2ZxrIO7D9g2zZKTzv0GTtw8EBgm9kVf1eG8RspIt6u3QX3c+3bx4HPcJbx3Wzug3179hX7OY50/8q2FtL3p6v45nd1JL8FwYj7TJxnazavQX5Vi0peCkfbvt37sC1rG8rlGJ/P7bu32/rbFClp0louThHHmbjL1q1bp+ZjVm6//fZSx5V9JNSsWVPdyjxPXG4ytzNp1aoVDjvsMDXXCyW0uXVBNV6E13hb33hc53hb3+LWWa4F3HlnOaxcafzenHRSPl58MR/t2pn/Dp4k3vZxvK1vPK5znqb1jTRexEKbXC20kx07dqiTi+B6GvJY6nI4GVOEOUk3tU7AmjRpgjp16qCqhv7LsnPKlTe+nGvVrKWu1NpB4oFDuzOhgpHeU61qNdviq2UkGMtISDLiS7fZkuKr9S1XTm3Pkk5mKiQZglFSxSR1m1wh2dbxV65UWd2WSzC2f3KSvfGrVik4Xsof2l52xq++07hMlpiUGBDa6tapi7pV7FlGzXTjRCYhIQHl88pHfIxGuo/Tk9IDrssKyRUC28yubZSSaaSNCckpRs2mSimVbN0HZgpeYgXjs1CxQkVb48t4hfIJ5bV8BqpUrlLoMyxCWyTxI93HVVONz4Ds38D3XC37vufM8VdMqYiD5QzRzc5jqMYuo2JxQmJCwLUty7Qrfm6a4TCTz29q1VTbj6Fa6bUC33EqZsGuKulzHOn+rZZaTd0mJSepeObnLLVKatTrUDW5qhLaKlSpUKr1F8He/B5sWK8halWqhepVCr4jKyba+tsUKRUrGo66eOOPP/7A2WefrS6ciOAmopjMyaTMiOyH0gptso+kFm/37t1xzDHHqOek/q58NqsH2UZkrievxcoFVbtF3Fgm3tY3Htc53tY31DqvX5+A4cNT8fXXxvd83bq5GD48DRdeeBDiQ4mxKkVRE2/7ON7WNx7XOU/T+kZ6UTVioe26666DX0lOTlZ/wcgO0XUQmicHIsTYtQxTBAuuH2XnOljTwsy6PJHEl5OZSLan+bo1bc7O8ZsiSU5+jtb4gUYCBettF2bHv+COiLYdQyHiy3N27WPzGLU2c7B1/CE+A5Eeo6Wu0VZe7zFq9zEUOEYtDVkijR/JPjbjWzvLJiUk2b6P1RUq3Z8B2B8/KdG4iKDGXlAaz87vIetnTGKa36WR7IOIPsOW7SPvi/Z7IrhO29aMrcjIySjV+ltru1VMqqhiVEoyhOqDuQdt/W2KlHiYuIZCaqadd955qlxHtWrVVApnUlISrr76atxxxx2ljiu12v766y/88ssvZRqfKxdUbRZxY5l4W994XOd4W1/rOqem1sFzz5XHk0+Ww8GD5ZCQkA+5djB8eLmC7w/7v0PcIN72cbytbzyuc56m9Y30ompUNdrspHbt2so1I00WrMjjcI0O3IipC92F4HV3XNTe0dEihGmJ7/FC+WZ8u5fBZgjRd1306jGqO76fuo7a1bHTGl/X92igWUG+nmYIptAWqutotFg7j5YlbVRITmTXUTdZtGgRXnvtNTWRlXmYpGlK86qnn35aXagtTaOC2267DV988YXqet+4cePA8zKfk9TUPXv2FHK1FTfXc+OCqt0ibqwTb+sbj+scb+srfPddRYwYkYD//jOujJ1yCvDyy+XQpo08srchXCwQb/s43tY3Hte5nIb1jTSWa1tYbP8dO3bE999/X0h1lMfdunWLmZi6MN08XhMZ/CIC+CW+EyKDFztS+mkfezW+k2KtnV07nTxGzYYausZfSAizSSwM29W0FPHNzqNpmWllFtqSyicV7jqaza6jTiLuNXPiKamiUqdNEHfb+vXro4olblUR2T777DP88MMPgY73JjLPk+VZ53orVqxQy4y1uR4hxJuIZ+O888rhuutqKJGtUSPgww+BH35AgchGCCEx6mgTxMYvVzo7deqELl26YPTo0aq2h9kx9Nprr0WjRo1UbQ1BrmAuXbo0cH/jxo3qKmqVKlXQokWLiGLGCgERw8arIRQZStfR0db4BaKUtvEHdU21exl0tEW+DGtnXC8do04KbTodW7qOIWv6txeP0WAhzG4xsohjzgZHW1pW6YQ2M3VURDbzu9F0tEntN+Ic0hxrwYIFaNmyJXr06IHhw4erGm3vvPNOoLZaNOmi0lH0888/R2pqaqDumoh2KSkp6rZfv35qvie14CR1a9CgQUpkY8dRQkhZkb4p558PzJtXDklJ+bjzTiNNtIpRIpYQQmJfaLvssstUIVqZkMlEqn379qqtu9nMQK5OWq15mzZtUpM5k2effVb9yaTup59+iihmrCBXbLWmjjKtrdj45vax060Vavx2Cqmh4lufszO+tYaal4Q268m+9mNId3q2Q59hXceo9tTRvFwt28ip1FRtQluY1FG7lmGno02aIdjhaKuQcKjrcsVEo24GhTZnefzxxwPFgR977DF1oXTgwIFKeBs/fnxUsV599VV1e4rkaAU15br++uvV/VGjRqn54UUXXaTSVHv37o1XXnnFtvUhhMQvIqzNmwdUr56PyZN3okePmihf0NyJEEK0C23iKFu9ejWOOOIIJCaWXq+T9AD5C4Upnpk0a9YsIFCVNmas4MUTOGs8CnnFx2eNNnfiW0Uj7ceQQ3UE7RbCnHLMOZo6qqGGmm6xWcjOzdY6fi2po2GEPLN2W2mEtrLWaLMKbUwddQfJIjCR1FG5wFlaIpnnSSHiMWPGqD9CCLGL114Dxo6VC/HAu+/mo3XrQxe2CSEkGqI+e5DW7WLZl5btbdq0CdThENv+k08+qWOMvsTa8c/LqaO6HWFeFdr8VKPNS65LOR5NIcnrjjavH6PaUzvhTTHY6dRRu5shBLriFoy9LKmjNSrWULe7D+62T2hjMwRX6Nmzp2pOEIx095TXCCEkltm0Cbj0UuDmm43HI0cCZ53l9qgIIV6mfGlapP/555/KbWZtbdqrVy9MmjTJ7vH5Fi+ewFnjUcSIjfh+aFbgWbHW451xvSq0Wb8jtAh5mmvAFXK0aajz55ijzYbU0ZopNdXtrgO7yiS0mR1HCznamDrqKDInlEyHYA4ePIiff/7ZlTERQkhJ5OYCYoxt3Rr4+GMgIQEYMgS4/363R0YI8TpR53pMmTJFCWpScNZ6gizutn///dfu8fkWMzXCTpHB0bQ5j8YPbiagWyj0co02L4vBcvLv1WPUL2KzE6mjOrZRofgaa8AVSh21setocGqn3c0QzBRRO5oh1EipYYvQFsrRxtRRZ1i8eHHgvjSrMhsXCLm5uSqFVJpaEUJIrLFoETBgALBggfH4+OON1NFjjzUe5xk/n4QQ4ozQJo0GpP5GMNLZ025nip/RkTpq3f66hSrfNEPQVP9Kd9dUJ4U2dsYNHV9351qv10HUljoawnFmp1Bl/Y7QEd/q/NKZOhpwtMVwM4SyOtoyc4yuo2yG4B7ScEql7JcrFzJFVLqEvvTSS66MjRBCQpGeDjz8MDB6tOFoq1oVeOIJ4KabDEcbIYS4IrRJwdvp06ermmzWk8I33nhDtVYn7nUdNePJCZZX0/Kcqq/l+WYIBSe5di/DL442R+J7PHVU9zGqPXVUc1dQ1QxBc3donamjASFMdzOEMjja7EodZTME95CmWPI5Ofzww/Hrr7+iTp06gdcqVKigLswm8MyVEBIjTJsmTfOAghLjqi6bCG4NGrg9MkII4l1okxbuZ511lkoRyMnJwQsvvKDuz5kzBzNnztQzSh+i4wQxlNDmNTeP11NTnRZJvCwyeH4fOPQZ8JpY7mTqqNZmC7o644ZwHtvZdTRcV1C7myHEgqOt2NRROtocoWnTpuo2jzlWhJAYZuNG4PbbgcmTjcfNmgGvvMKGB4QQfUR99nDiiSeqZggisrVt2xbffvutumI5d+5cdOzYUc8ofYhOoc0PIoZf4usWSXSldtLR5n9Hm5NCm61CkkUI01lDTY1fQ4q/7n0QavvHuqNNuo6awr5djjZ2HXUeqdMr2Q7SHEv+br/9dtbuJYS4iqSGSva6NDsQkU0Mtv/7H/D33xTZCCEx5GjLzs7GTTfdhGHDhmHcuHH6RhUH5ENf6qgjxf49Xj9K9/ZxSkgVkc1LXUf90LDDFBq8On4/dh21U8gLFV+X89hshqDDkWcdv/V5u+Kbx09ZXHmm0CYx0rPSkZqcWrquowmWrqNshuAK33zzDfr06aNqtnXv3l09N3v2bNUoa9q0aTj99NPdHiIhJM5YuNCou/bbb8bjrl2NZgft2rk9MkJIPBDV7D4pKQmffvqpvtHEEToKzftJCAucvNlYhNy6vXVvn3CPveKY84OjTUf9K0eaIRQco153vTriOPNgaqru7zlrswgdtRwDjrbg1NFSrIO4z0yRrDTpo8XWaMs5UCqXHCkdQ4YMwV133YX58+fj+eefV39y/84778T/xD5CCCEONju4+26gc2dDZKtWDRg7VsR/imyEEOeIeuZ9/vnnY8qUKXpGE0fobIbgBzeM7vgmuoSqcI+90vXVD0Kb14W8cI+9cgzpatgRquuol+oUOpU6KgKYtZajXa6/xPKJoVNHSxFfjsWy1GnLzA3fdVQ5Bgs+o0Q/y5YtQ79+/Yo8f+ONN6o6voQQ4gSff26kiY4aJbUjgcsvB5YvN5xt5e39KSeEEHubIbRs2RIjR45UKQFSk61y5cqFXpeaHKRkvO5WsaYu6ozvVRFDV3wnU18DrksNqal+ENoCbi3oiR/usV3xndr+WoUkDZ+DQo6wMhT6d0ustaaOWsVO7c0QShlfhLbN6Zvtc7QVpI6a6aPW14g+pNvookWL1BzRijwndXwJIUQn69cbzQ5ML0jz5kazgzPPdHtkhJB4JWqhbfz48ahevTp+//139WdFTsgptEWGF4tsW4U1r4sk4R7bHV+XEKm7xp9y82hYRqiOi149hhxzXWoWs51o2KE7tdPO1EurkOdY6qiGGnPy+dXhaAvbDKGU8cviaAsltEkqqnxmZP2lIUI1VCvVuEhkyIXXe+65B/3798eAAQPw33//4YQTTlCvyQXZp556CndLDhchhGggJwd4+WVg2DAjZTQxEbj3XuDBB4FKldweHSEknolaaFu9erWekcQZfnG0eTV+uMeM70x9Kont1Tp/TnUdDffYK65IXUKbteuoX2q06XDkCWazhVh3tNkptIlwLOmjUqNN/oheRowYgZtvvlk1yUpNTcVzzz2HoUOHqtcaNmyIhx9+mBdgCSFakPprkhIqTQ8E0fil2cExx7g9MkIIKYXQZsWsX2O3IyIe0LXt/CKEaXPkOZTa6fX4Xhbagrsiek2ociz92KGGKXYvw7r9tTdb0OQ8NsUwHamj1ljWGmV2LcNuR1uNlBplFtqsXUfN9FEltLHzqKNzGWmGIH9paWnqORHeCCFEB2PGGKmiUoetenXg6acBKRPJOmyEkFihVF9HEydORNu2bZGSkqL+2rVrh3feecf+0fl4Yqo79U93DbVwj+2KHxBJPFr/yqvxTZHHmnLmNTHVL/HDPbYrvtdrtOkSg62OOd3NEEzHmZ2po1ZnmdXRZpvQFiRkl9nRVtFeR1tw51Gin+CLAyKwUWQjhOjihx8OiWxms4P+/SmyEUI87miTlu2SInDbbbehe/fu6rlffvlFpQ7s2LFDXc0kxWOKbDprqOlOeQr32CtuGxPdQqTdjsXg8equ32V9TtcyvBrfqWPUq9tHW+poiK6jOmqciVPL66mjphBlq5BXLih11MUabZk5RbuOCsmJhsNNarQR/Rx55JEl/tbt2hX9/iWEkFBNDy67zBDZrr8emDBB5tpuj4oQQmwQ2l566SW8+uqruPbaawPP9enTB23atFG1OCi0lYx58ublk3Td8b3qFgoWvrw2/lDxvNpZ1uvxwz32Snwnuo56vUab7tTRgNBmY53CxPKJhVNH7arRdtA+R1tS+aQixyDRW6etWjU2nSCE6OWff4BLLwV27ACOO87oKkqRjRDiG6Ft8+bNgY5SVuQ5eY2UDIW2kuNTxIiN+DqX4XUhzKlmCF5P/9YphOmok1covkN17OwUwgqljubpS02NBUdbOKHNFAMptDnD5Zdfjrp167o9DEKIT8nKAp55BnjkESAzE6hVC/j0UyDFqBJACCExSdRnDy1atMBHH31U5PlJkyahZcuWdo3L15h1f7x8Eq07vl+ENl3NLsI9tju+zmV4XWjT1gxBsyvSKdelVeTQISRpq9FmSU01v6u1da7VcIxaY5k12nRsn2BHmylulVZo231gt32OtgQ62pyCzbAIITqZPx/o1Al48EFDZOvd2+g22qyZ2yMjhJDiSSxNisBll12GWbNmBWq0zZ49G99//31IAY74y9GmuyOiKQJo6zrqcGqn14VUncvwqtDmVFfTcI+9Et8qctj5OQhVQ81OIczR1FGNQlghR1t5Bxxt5V1wtOVlFarJZmKKftZmEET/hUNCCLGL9HRDXHvxRfmeAWrXBkaPBq68kumihBCfCm0XXXQR5s+fj1GjRmHKlCnqudatW+PXX39Fhw4ddIzRd3hZaHPKseX1Zg5+ia9zGV4V2vzmutRdf0xENju/J3QLYWYsaVrjWOqojamdsq3lPxm/1mYIwTXaYih1lDXanCNPKpITQoiNfPUVcPPNwLp1xuNrrpFmfIbYRgghXqFUuR4dO3bEu+++a/9o4gQ/CW1ej+81x5luR6GTQpvu+H6p0ebV+NochZbUTh1CmDWW7n2s8xiVbWM6unQ42syx2+VoO5BzAAeyDyAlKaXMXUcDjrYCsZcQQkjss20bID313n/feCzpoa+9BpxxhtsjI4SQ6Il6dv/ll1/im2++KfK8PPeVXIIgJUKhjfG9UgMu3HN2LsNLaXlO1GjzyzGqvUae5q6jTuxjHV1HraKXVkdbnj2OtqrJVQP/dvfB6Oq0sRkCIYR4H0kNfestyZAyRLby5YHBg4G//qLIRgjxLlHP7ocMGYLcXGNiHVynQ14jJUOhzb/xnXacOeFo82pDB10ihm5HW/D29ur21y20WVNHdXTVdKLOnw7Hme7PQKBGm5k6WkZHmxzfNVJqlCp9lM0QCCHE2yxfDpx6KnDDDcCuXUD79kYDhGefBSpXdnt0hBBSeqKefa9cuRJHH310kedbtWqFVatWlWEo8YPUznHqJNqrJ+l+EcK8lprqJ0cba7S5G19XnUXdXUcLpY569BgyhUcdQp7pFrPL0SbUqGiv0MZmCIQQEtscOAAMGwa0awfMnAmkpABPPQX8+qvRZZQQQuKuRlu1atXw33//oVlQX2UR2Srz0kNE0NHG+F6J7wexkPGLj+/V/StCj47UTmusQNdOGx1zfkkdlQtG4mQvq6OtLA0RzPVLTijcdZTNEAghJHb57jtg4EDg33+Nx+ecA7z8slGTjRBC/ELUs/u+ffvizjvvxL/mt2OByDZ48GD06dPH7vH5Xmjz6kmuX+LT8Vd8fLs7RvpxGzG+s/GtzRACqaM2OrYK1WjzYNdRa/yAUKihGYK5fexwtJVVaGMzBEIIiX22bAGuvNKouyankY0aAZ9+CkybRpGNEOI/oj57ePrpp5VzTVJFmzdvrv5at26NWrVq4VlJqCclYp4celHECBYGvXaSzvjuxndiGYwfH/F1pY5aBSOvp47qdLQFuwrdcLRl5hbfdZSONkIIcZ+8PGDsWCkzBHzwgdHs4I47gGXLgAsvlIvebo+QEEJiJHV0zpw5+O677/Dnn38iJSUF7dq1w8knn6xheP5EV+0iPxbj93p8u4VU3UKn7vihYuoSGXTF97qY7ZfvCCe6juoW2nR3xtURP7hZhB2uPLsdbWyGQAghscHixcBNNwHz5hmPO3YEXnvNuCWEED8TtdBmnqidccYZ6o9Ej9S10SHC+FGo8pyI4bDjz+7UYzkm5T+zYYcXhTa/xWd6efhmCDpSOwsJbZo6yxZJHbW562igq6mO1NFyYVJHbXC07T6wO6p/x2YIhBASm2RkAA8/DIwaBeTmAqmpwGOPAbfcAiTY+5NHCCExScRnD3PnzsUXX3xR6LmJEyeq1NG6detiwIAByMw00jiIe442igCMb+cyKAYzflnj2y0ihUodtTM10nrMezV11IynJXU0yNEWeN4OR9tBmxxtbIZACCGuIaeDvXoBUlFIRLaLLzbSRAcNoshGCIkfIp7djxw5En///Xfg8ZIlS9CvXz/06tULQ4YMwbRp0/DEE0/oGqev8JPQxvj+ih8ck8eo/1JT/RJfaoPp+i61LkNnfG1dRwuOUdPRZafYabrFrEKX611HEwt3HWUzBEIIcY/bbzdSRWvUAMSj8fHHRuMDQgiJJyKe3S9atAinnXZa4PGHH36I448/HuPGjcPdd9+NF198ER999JGucfoKCm3x48jzmuMvOCaPUcb3QtdRXcswU0d1ufJ0dR0tkjpqY3zrti4ktMVQjTY2QyCEEHcYPx54/XWjwcH77wPnnOP2iAghxB0iPjvZvXs36tWrF3g8c+ZMnHXWWYHHnTt3xvr16+0foQ9xUmjzmtDjl0LtXo0fHJNCG+PHanxVo02z40y3kOfFZgihupra5WjbuX9nVP8uMyd011GmjhJCiPM12R54ABg40Hj8yCPAmWe6PSpCCHGPiGffIrKtXr1a3c/KysLChQvRtWvXwOtpaWlISjImt6R4zELzdotgfjqJZnx3hMjgmBTanI9PsTn6rqO6HGfhHnsldTRQo01TswVT6LIuszRUr1hd3e45uCeqpkLm9mMzBEIIcQfp7yapoa1aAY8/DmRnA1deCQwd6vbICCHEXSKe3Z999tmqFtvPP/+MoUOHolKlSjjppJMCry9evBhHHHGErnH6CqaOuhefIkZ0y+Ax6n58uxtS+KnrqO4abeEe2x1fl1CooxmCDkdbteRq6nZf5r7APi0Ja/21Io62BDraCCFEN0uXGk0PLr0U2LABaNoU+Owz4N13gfL2Tx8JIcRTRPw1+MgjjyAxMRE9evRQddnkr0KFQ5PbCRMm4IwzztA1Tl/hZaEt2IXn9RNcCnnFL8OLxyjjx0d8SRs13cG6xLzgZXpNjAzUaNPkaLOrRlu1iobQJvszIysjon9jXXZYRxubIRBCiO3s3QvcfTdw7LHADz8AFSsCDz1kdBY9/3yjPhshhMQ7h9qHlUDt2rUxa9Ys7N27F1WqVEFCUH/mjz/+WD1P/C20MX5suZHsjh+8DKY3+y++X8Ryq1vJq+ugK36RrqMOONrKsg4piSkqpqQD783ci9Tk1BL/jXXZyQmFu46yRhshhNhPXh4wcSIwZAiwdavxnAhrzz8PNG/u9ugIIcSjQptJtWrGledgatY0ihmTkqHQFjvxvZaWp1skCY7JY5TxY1ZEsriV7BaSHE8d1TR+XTXgTMeYNTW1LN+l8m/F1SZdR/ce3IvGVRuX+G/MZcu6BTv26GgjhBB7WbgQuOWWmliwwPg9adkSePFFNjwghJBwMIPeBSi0MX5ZTkitYhuFNv/Fd9px5jWx2YxnreXltX3sVOqo7mYIdsY367SJo60sHUetQhsdbYQQUja2bAH69QO6dCmHBQsqoHLlfDz5JLBkCUU2Qgix1dFGyo50S9OV9seTdH+nzZnb3DyGKLQxfrzH17GMYIeZVx1zTjVDsCO+WadNHG2RYC47lNDGZgiEEFI2Dh4ERo8GHnsMSE+XZ8rhggsOYPToZBx2GIuwEUJISVBocwE62uInvq4aZ04dQzxG/XcMFRk/vOnW0rkMx1NH7XacBaXX6nK0ZeZmuuZoK05oC6SOFtSoI4QQEhlyHXfyZODee4HVq43nOneWOmx5aNFiL+rWrev2EAkhxBMwddQF8kChza/x/VZDjcco45c1vm43mA4hyev7INB11KFmCHY62vZl7iu7o43NEAghJGr++AM49VTg4osNka1hQ6P5wbx5wAknuD06QgjxFnS0+dzR5rXUTt+5hTwuhOnuahrqcVkJFl10pxV6/TPg9fhOLMNrx6juZghaa7RFmToa3HFUYDMEQgiJHOkg+sADwIQJhqOtYkXD0XbffUCVKoc6jhJCCIkcCm0u4OXU0WDhxWsnuF6PHxxTV2pqqPs64vvBUcXPmLPxQ+1Pr62D7mO0iOOsfOw72qomV7U9dZSONkIIKb4O2wsvGHXY0tKM5y6/HHjqKeCww9weHSGEeBsKbW42Q9AskoR6HO/xnRYxdDvCvO6Y07EMxo+v+F52nOmOryt11BSy3HS0mfXh2AyBEEKi55tvgIEDD9Vh69TJaH7QvbvbIyOEEH9Aoc0FvOxoY/wo42sog2gVaCm0+T8+07+Lj+/EMrwWX0dqZ7Hx7ew6ymYIhBCilblzgfPOA7KzgQYNgCefBK6+Gihv/5SPEELiFgptLkChzb1mAl7fPsExvR5fxzKCxU2v7WO/uTq95jYLFVN3swVd8QNdRz2Qmmpn11E2QyCEkNBs3gxcdJEhsvXtC7z77qE6bIQQQuyD1y5cgEKbf+PrFjGCY3o9vo5lMD7jx/oynKpjZzq6dImdmTmZMdl1lM0QCCGkKBkZwCWXGGLb0UcD77xDkY0QQnRBoc0FKLTFTnyvpeUFx/R6fB3L8Hp8ujrdje+HzrI6UjsLxS+IZ9ZJc7XraGLRrqOs0UYIIYeQ0tCffgq0agXMng1UrQp89hmQmur2yAghxL9QaPO50GZ3wwWn43vtBFe3kBe8DN3HkI6GHRSS/B1f9/4NFo3sFpH81Bk3kDoapzXaKLQRQuKdlSuBM88ELr4Y2LABaNYMmDYNOPJIt0dGCCH+hkKbC+TDma6jEt9rji2viwBOO9q8LuTpWAbjuxtfjkmdDTucTs/2cuqobkebnTXaqiZXja7raEHaKpshEEJIUfbvBx58EDjmGODbb4EKFYBhw4C//wZOPtnt0RFCiP9hMwSfOdqsJ6Fa4ntcqPJ6/OCYXo8v0BXpsitVk1ibm58bcnl2xfdyCr5Tqamm0GS3o80Usmx1tFmaIeTn55d4XLIZAiGEhE4TnToVuOMOYO1a4zlxtL30EtCihdujI4SQ+IGONhdw6gRRt9sp1GPG1y+06RZTnU5NpevSX/GDY+quoWa3iBQqpvbUUU2pnWbqqPYacOXtSx0VcexAzoES389mCIQQUpj//gPOOw84/3xDZDvsMGDyZODLLymyEUKI01Bo85vQZtmlXnR6+C2+7vRgL4okXk9N1S3C6E6NdFpo0yGEef0Y8l3qqA3xq1SoEvi+jKTzaEBoKx/C0cZmCISQOEsTffhho5Po9OlAUhIwdCiwdClwwQUyr3B7hIQQEn8wddTHjjaegDovhOlOrQ2OScdc8fF1LIPxo1uG1+PrbFYQ7nGsp46a8cyuo6aDrKxjljptkjoqddrqV6lf6q6jrNFGCImXNNFPPgHuuQdYt854rlcv4OWXgaOOcnt0hBAS39DR5gJSf0ZgIXvn4/vNLcT4xcfXsQyvx/eDGGwVjhypI6g5vdl2x1nB9jHr5HmhGUK0nUfZdZQQEs8sXgz07Alceqkhskma6EcfGY0PKLIRQoj7UGhzATraGN+uZXi9zh+PUf/FD46pPTXVZhHJj8eoLkeb3amp0XQeZTMEQkg8snMncOutQIcOwE8/ARUrAg89BCxbBlxyCdNECSEkVmDqqAtQaPNv/GBhSrdQxRpw7hey157e7LFmEcExvRjfKhx58XtUd1dTbY42S+fRkjDTVtkMgRASD+TkAK+/DgwbBuzaZTwnwtozzwBNm7o9OkIIIcFQaHMBCm3+jW/G1LmPreKa148hHqPOp3b6QWhzUghzpAaczY4z3ampppBlt6MtkDpaVkcbmyEQQnyEONduvx1YssR43LYt8OKLwCmnuD0yQggh4WDqqAt4WWjT7djyukgSHJPx3Y2vYxmMH90yGL/4+DqWESx8aU8dtdnRFlXX0RJqtJk1UQkhxGusXWu41k491RDZatQwGh0sXEiRjRBCYh0KbS4KbV5P+/ODW0hHaicdZ7ETX8cyGD/GarTZLCL54RgN3ibamyGUcz51NNB1NCE5bI02a0MIQgjxCnJ94OmngVatjK6i5csDt9wCrFxp1GdLZD4SIYTEPPyqdoF85PviBFfHMvzWFdTrYqrX4+tYBuPHVh0+3fF1NFso4jizeRm6U1PNeJk5mXq6jpYxddR0tJmuNutjQgiJdaQW2//+Z9zv0QN44QXg2GPdHhUhhJBooKPNx6mjukWeUI/jPX5wTMZ3N76OZTB+dMtg/OLj+6IZgouOtpKEtuxcNkQghHiHuXOBQYOM+488Avz4I0U2QgjxIhTa3Ewd1ZG2aInpxRNEr8cPjqm766jXxVoeoyXHt3sf6HaNBsdk/OLj61hGEUeb7tRRmxxtVZOr2tJ11GyGILAhAiHEK6xaBVx8MZCdbdw+8ID8Zrs9KkIIIaWBQpsLeLkZgt9EDC/uA6tQ4nUhjMeoO0KY1+sIWoUjHamdTh+julI7dceP1a6j1vFk59HRRgiJbXbvBu65Bzj6aGDTJuN2wgSKbIQQ4mUotLkAhbbI49t9gqjbLRS8DK/vY6/H92Jant/qCHpRCLOO2Q/HqFccbXZ1HRWx2tp5lBBCYhFxrr30EtCiBfDcc8bjM84AvvwSSE11e3SEEELKAoU2F5sheD2tUMcy/OAW8lPqqNfj61iG1+MHx2R8d+N78YKFKWJpc7RF03U0MbnYMVJoI4TEYlfRqVOBY44Bbr8d2LULaNMG+Oor4JtvgKZN3R4hIYSQskKhzQXoaPNvfKfr5DF+8fF1LMPr8YNjejG+7q6jjqeO2u04CxLWbHfMldfraCtr6qhVaGMzBEJILPHHH8BppwF9+wL//APUqQOMHQssWgSceabboyOEEGIX7HnvptAGD9YHC0ozszvtjCIG49sZX8cyvB4/OKbX49vt1vLjMaordTQ3P9d1R1s4oS2pvNEQgY42QkgssHEj8OCDwNtvG4625GTg7ruBIUOAqkYfGEIIIT4iJhxtY8aMQbNmzVCxYkUcf/zx+PXXX4t9/8cff4xWrVqp97dt2xZfSjEDC9dff71yFVn/zoyhy0R+crTZnRrpOxHDg2Kqn1JfPZneHCxme3wfeD2+biEv1GPba7TpbrZQzl5H28GcgwEhLRyZOeG7jhZytLEZAiHERfbtMwS2li2Bt94yRLYrrgBWrAAef5wiGyGE+BXXhbZJkybh7rvvxkMPPYSFCxfi2GOPRe/evbFt27aQ758zZw6uuOIK9OvXD3/88QfOP/989ffXX38Vep8Ia5s3bw78ffDBB4gV/CK0USQpeRk6RBKvd4zkMeRu/OCYXoyvu1mBk6mpwcuLZSEsXDy7xp+afKj6d0npoyU62hLoaCOEuEdWFvDii8ARRwCPPQYcOACccAIwbx7w/vusw0YIIX7HdaHt+eefR//+/XHDDTfg6KOPxtixY1GpUiVMkL7WIXjhhReUiHbvvfeidevWeOSRR3Dcccfh5ZdfLvS+5ORk1K9fP/BXo0YNxAr5cjnLAaeK150wOpbhN7eQ18dP16Xz8YNjMr7z8XXXUPOqkCcutMpJlSPqPBppjTYKbd5j1qxZOO+889CwYUP1/T1lypQic6jhw4ejQYMGSElJQa9evbBy5UrXxkuIlbw8MREArVsDd9wB7NgBHHUUMHky8MsvwPHHuz1CQgghvq/RlpWVhd9//x1Dhw4NPFe+fHk1aZo7d27IfyPPiwPOijjggidiP/30E+rWrasEtp49e+LRRx9FrVq1QsbMzMxUfyb7xOetfizz1J/d5OblBtIK7Y5fyO2kIX7wyVwk8eU9MjGOaCz5RR/bug664wcLYSjnufjWYyjS+NHs42BHntbPgIb4RfD4MSrYvY+d/AyIyOO179EitS1t3sdFUtYjiB/V/g2Kb+fnTOq0ZWRnYPeB3cXGNIW2pHJJId9nCm2SYhouTlS/TRGi/fsmDsjIyFDZDTfeeCMuvPDCIq8//fTTePHFF/H222+jefPmGDZsmJoHLl26VJUUIcQtfvwRuO8+4LffjMf16wMPPwz06wcksio2IYTEFa5+7e/YsQO5ubmoV69eoefl8fLly0P+my1btoR8vzxvIo43mZzJBOzff//F/fffj7POOkuJdAkJRa+8P/HEExgx4v/buxMoKapzgePfLMzCMgPDyCa4gIALIuLCwYgrTzFx4xlFohHzDARUBI1oMCCKCyhuARWSHDeOcYm+qC8xD6NGSaIIohLUIC6PRaPsDPvMwEy9c6ustrqnZ+hm6lb1rfr/ciZM97Rf1e2qrr799XfvvbXB/evXr5fq6mrxm5vIU4nGxobI7qudO3Ymdfj9jr9t67akD1uZxFf7sWXLFvsDjUqkNqVqc1XS7c2bNsu6unW+JzldW6q2yLpif58j7wct9Xz5fQz27P6uQmPXzl2+x6+tqU363e9jrPY58d/V+X+O7ti+IykB4Hf8rVXJlTabNm2S0tpS3+Lv2vPd8+O+JtYV+tsGr+3btvt+jNVxdVXvqvb9GHjjq9eD76+B6lqt11HvOaps2rBJaouanpNsX98H7O1t27HXNmRzfNUxTX2+/HqOWhU4FW2r1qySrgVdG31cTV1N4vWY7j0iz3KSmes3rpd1Reua3eZMbdv23Xsk9o3qr6mfdNSxeuCBB2TSpElynlq2UUTmzp1r9wPVF64XX3xxTnyhqiOJm8vi1t7UNn/4oVrUIE/mzXOuO61bWzJhgiXjx6vf3ceL0eJ+jOOA9kZf3Npcr6m9mcaL5Pcr3o6WWiyhb9++0qNHD7vK7XS1pnYKVVHnrZJTHbBu3brJfvvtJ2UaZilt2bql829JS7vqzk9lbb7b36LCIt/jt6v6bgiu+mCSSXx1MqrhH+r53NuHmUqrMun2fpX7SYd2HXwftutqX9He9+fInRtIaVve1vf4JcXffWPfpnUb3+O3LG2Z9Lvfx1jts6uohf/naPm/nQnV3Wonv+NX7KxIut2hsoN0KPNvG2oieK/K9pW+t8E79C/TczSbY6yOq6t1q9a+739xi+Kk14Pf8Vu1dJI9uq6j5WXfnaNKxw4dpVVRK23x27Vtt9c2ZHN8y9skx/fzOtS+VXv5rOozySvNazLm7jpnkYMuHbpIhzYNH1fSwrlOti5r/PzLps2ZoqJKrxUrVthfrKqRD67y8nJ7IS31ZWpjibagv1DVkcTNZXFrr9vmTz7ZIePHl8tzz5WKZeVJYaElP/7xTrnuuh1SWVkvO3eK/RMFcT3GcWoz7Y2+uLW5XlN7M/1SNdREW2VlpV1htnbt2qT71W01r1o66v5sHq90797d3tbnn3+eNtGm5nNTP6nUAdF5EuqInzS3kIb47nAcd1uZxlcfZjLZn9S5fwoLCrUeAx3xU1csNC2+9xhkE39fjnE255Duc3Rf4us4h1Q8L93nkGqP38dY+2vAE0/HMdZ9juq+zqXGV8n/TOJnenwbvAayOIf2prSFUx1aW1/baExVmVxn1SUSauke537hUS/1Te5bpm3OVBw6rmFyRy/sbWRD2F+o6kji5rK4tbeuTmTKFJH778+T6mqniu2HP7Tk9tst6dlTXcP8q3LPFXE7xnFsM+2Nvri1uV5TezP9UjXURFtRUZEcc8wx8vrrr9srh7pPiLp99dVXp/1vBg4caP99vKrH/tarr75q39+Yr776SjZu3GhPnJsLTF511DuxvImLIbjzI1nfToTFMdj7/Fo645v4/LMYQvjxvZPv+z3Rf7pEoc74Oraha1XQIFY1LSksaVDZef+C+2XZhmXy67N/bV//3PnZMlkMwa18Q7yF8YWq30ncXBen9t58s6qSdH4/+WRL7r47T44/XvWX/O8z5ZI4HeO4tpn2Rl/c2pynob2Zxgr9GVbfMP72t7+1J7VdtmyZjBkzxp4IV61Cqlx22WVJiyWMGzdO5s2bJ/fee689j9stt9wiixcvTiTmtm/fbq9I+s4778jKlSvtpJyax+OQQw6xJ8vNBW6ShyRG0/GD2AbHIE18MXz/I7ZipInPUepiBTrjm/786NiG7nNUZyIvXaLt1vm3ym/f/618uvFT+3YmibYW+U5FG6uORos7eiHbkQ2AX9Taa3fc4fw+Y8YWef11S44/Puy9AgDkmtATbcOGDZN77rnHXqq9X79+smTJEjuR5g4LWL16tXzzzTeJx59wwgny1FNPyW9+8xt7Varnn3/engC3T58+9t/VUNSlS5fKueeeK7169ZIrrrjCrpr7+9//nvbbzDCYXNFm+gfQ1JjEJ35z4uvYRmry1/TnyMT43kSSiedQg4q5PLMr2nbudiY5UquRKtlUtJFoixa1yJVKqKkvUb3DQBcuXNjkyAbAD++/rwoAnN/HjbPk0kt3iWeQAQAAubUYgqpGa2yoqFrAINWFF15o/6RTWloqr7zyiuSyqCTadFeDpbtt3HOkoQdGRV7uxNexjdRzxvRzVHdVoekVczq2oX3oaAAVbTV7ahLzse2u352UcHNXHFXJtMbO38TQ0W//W5hDjUxQc+p6F0BQX8JWVFTIAQccYE8dcvvtt0vPnj3txNvkyZOlS5cuiSlIAL+p7/vVcNFHH3VWDz3pJJG77rJk8+aw9wwAkKtyItEWN1FJtJn4ATRqc6gRP9z4OrfBdYL4fsTXkYxMXQzBz/jFBcVJFW1uUk3ZtXtX0r8tW3y3QnJjiyFQ0WYeNR3IqaeemrjtLmIwYsQIefzxx+WGG26wpxgZNWqUVFVVyYknnmiPhGDFV/ht+3Y1PFTknnu+Wz30ggtE5swRafHdAvMAADRAoi0EQX2ANr3aKYhtEJ/4zYmvcxvudcLEijPihzs0tcHQTt2LIWicoy3dEFL336YSbSyGYK5TTjlFLMuZyzYddU2cOnWq/QPosGePU72mqtjc6QDVyGSVcDvhBOe2qmwDAKAxJNpC4HYg+QDddHxd2/A+7xwD4jcnfhDbIH64c6jpjq9j6GtqTO1DU/OCSbTt2rMr40QbiyEAyJbqnr/8ssgNN4gsW+bc16OHyPTpTiUb87EBADJFoi0EDAnLLH4Q2zCxqtAb08T95xzNbhtaks0BnkPaE1U+V2ulxjT9+QlkMYQcrmgj0QYgE++9J3L99WpuaOd2+/ZORdvo0SJF6ddcAQCgUSTaQmByoi3ID+hBbIP4xG9OfF2JniglwkyPb+JiC9qHjmqsaCsuLE6amy2pou3budmyGjrKYggAmrBihcikSSJPPeXcLi5WK4qKTJwo0rZt2HsHADAVibYQmJxoo1qI+MTnHA07fpQqznQkaqloYzEEAE3btEnkjjtEHnxQpLbWue+SS5z7Djww7L0DAJiORFsISLRlFj+IbZhe9cccc+HGD2IbDN8lfraYo43FEACkV10tMmuWyJ13ilRVOfedfrrI3XeL9O8f9t4BAKKCRFsIEqsJ6pi/y5N4MfEDYhBJDBJh0Y4fZLWTQiIs2osJmPga1j101E1i5XRFG4shAPBQq4Q++aQzTPTLL537jjxSZMYMkTPOYKEDAIC/SLSFwBIrEh9wTU20US1EfD/PHxMTMabHZ9XRzOPr2EaDRJ6fc7QVND5Hm5tgcyvbWAwBQCZefVVkwgSRf/7Tud21q8jtt4tceqlIgf+XYAAASLSFISpDR3UnGNLd9nsbJsaPUtWiifF1J3midox1z0Fm+vMTxNBR7YshBFTR1mAxhMK9V7SxGAIQX0uWiNx4o8hf/uLcLisTuekmkWuuESktDXvvAABRRqItBFFJtOlOIunahunPUZSSqaY/P7oSbaa3gfgxWwwhoDnadu5JHjpa2qLxT8pUtAHxtX27yNVXi8ydK2JZIi1aiFx1lcgvfylSWRn23gEA4oBEW4QTbaYnSRSGdqaJL2bvf5Bz5JkYP4htmB7fm+jRMkdbgPGDmKPN723kTEUbiyEASKESa5dfLvLf/+3cHj7cGSbavXvYewYAiBMSbSEwuaItyCFtuua/CroNOuObeA4RP14Jc5LN4cbXMnQ0JV7q4gh+JNpq9tQ0bzGEAhZDAOJIrR6qkmyqim3ePJHTTgt7jwAAcaTnUyIyWgyBJEzw8YPYBok84pt0jnIORXvVVB3b0Bm/uLC48Yq2PftQ0cYcbUBs/OEPzhxsyqxZJNkAAOEh0RYCkyvaTI+fGpdqoWhPxG9i/CC2YXp83UMvo/T8pLvdXKkVbEENHc2qou3bxRCoaAOiT60mOmSIyAUXiNTXi1xxhcioUWHvFQAgzki0hYBEW3jxg9gGiTDix30euyDPUVY1bTp+lBZD2Jc52ki0AdG1erXIiBEiRx8t8sorznDRceNEHnpIvQ+FvXcAgDhjjrYQkGgLL34Q22BoJ/FzfRvEb5o3cWRiojA18RWVxRCyqWhj6CgQXZs3i9x5pzM8tMaZzlGGDRO54w6RHj3C3jsAAEi0hYJEW3jxg9hGlOKTKAw+fhDbMD1+kIkw0+MHsRiCr3O0FThztNVZdXY1GoshAHBVV4s8+KCTUKuqcu479VSRu+4SOe64sPcOAIDvkGgLMdHG/GDBD5kLYhumHwPihxs/NS7JTuLnUiIsqIo2d+XRZi+GUEdFG2A6Ne/a734nMmmSM1xU6dPHWWFUzc3GMFEAQK4h0RYCy7KMTVTpTgCo5KP6n1qZ1dRqIRJ5xM/1bWiP75n+08RVO4OcAy6QVUdNqmj7dtVRRSXZWAwBiLelS0VGjxZZsMC53bWryG23ifz4xyIF/l8+AQDwhZ5PiYjs0NEgKs7cuFGoFtJdtUhVZPTiKyx4Ee6qo9rjBzj0NXV7uZ7IU5VobjVaaqJNJc1UhRqLIQDRt327yIQJIv37O0m21q1Fpk0T+fRTkcsvJ8kGAMhtVLRFONFm4pAzN66an4dqofSomAs3vu4kTNSSqborzkw8h4IeOur3Ntwklq5jrOZpUwmymrrkoaPu8FEWQwCi7aWXRMaOFfnyS+f2BReIPPCAU80GAIAJqGgLgckVbUEOmyPRRvw4TpTvjcs8hdGPr3toaiBDR32O7115NDXRtqN2R2KuNhZDAKJl1SqR884TOf98J8l20EEiL78s8vzzJNkAAGYh0RbmYggGVpxFIsnAsLzYVFOZGN8bl2Tz3pOpuhNVpj8/6W7nevymEm2bqzcnfi8tLG00BoshAObYvVtkxgyRww8X+Z//ESksFJk4UeTjj0W+//2w9w4AgOwxdDQEaqJ/Uz/ABTk/VRSG5ZFMJX5ztqHj/PHGT/2d+NGIr3voaJAVbW71mmvjzo2J30tbNJ5oYzEEwAxvveUsdvDRR87tQYNEZs8WOeKIsPcMAIB9R0VbCBg6mtk2SGJEP76JicggFgRxt0FFW/SHHwcxh53fXygEVdFWs6fhHG0bdm6w/y0qKGowV5wXiyEAuW3jRpGRI0VOPNFJsrVvL/LYYyLz55NkAwCYj4q2EJBoy2wbpsZnaGruxNcxrFAdX/U/VZlq6jlq+vBg73HVnagycTEH3YlC3RVtxYXFjQ4ddRNtTc3P5p2jjcUQgNzz9tvOXGwbnJezXHGFyF13Ock2AACigERbCEi0ZbYN0+Pr2oY3MWJiRZjp8YNcGdfUc5T4uRNfR6IwzDnaNu7amFGijYo2IDd99ZXI0KFOkk1Vrs2Z41S1AQAQJSTawlwMwcBKkigk2hiWR3w/tkGijfi5WnWZVPGnOb6ObaRLtJUXl8uWmi0ZV7SxGAKQe2pqRC64QGTdOpG+fZ3Ktlatwt4rAAD8xxxtIbAsy9hEW1I1lYb9j1pFm4nHmPiZb8P0+Lq2EaWhkSY+P7or2lLnRguioq2itCKrijYWQwByb062Sy8VWbRIpF07kRdeIMkGAIguEm0RHjpq4rBCb1xj43teVqZ/SDfxHIrCOeomaHUns1N/NzG+joqtIOeY052I1FLRlqd5jrYCZ462mrrvFkNoV9rO/jfbijYSbUC46utFHnlEpHdvkeefF8nPF3n6aZHu3cPeMwAA9CHRFgLmaMtsG7rjk8QgfnO3YXp8XdsgfshzqOlO5KUk1ppa/bM5FW3ba7cnEmWJiradGVa0sRgCELoPPnDmX/vpT52KNjUn25tvipx5Zth7BgCAXiTaQlAv+hJtpq946Y1ranzTjwHxM9+GqfGDPEd1zxFm4jlkfCIvoMUQtlRvSdzXroSKNsAUVVUiY8eKHHusyIIFIq1bi9xzj5N4GzQo7L0DAEA/FkMIARVtmW3D9PgKQy+jF9+b6InCOWriMQhyDjgdiapA919DolMlatX/LLG0LoZQVV3V7EQbiyEAwVFTEM+dKzJhgrPggTJsmMi994rsv3/YewcAQHCoaAtz1VGSMKFW85iaxPCeNyYeY9Pje+OaHl/XNkyv2IrSHHM6np8GK5vm6ZmjTa0y6ibN2hS3SUq+sRgCkFuWLi2UQYPyZMQIJ8l26KEir70m8swzJNkAAPFDoi3EVUdN/ICrO8njjWt6fF3bSKqYY1XTwOMHkgz+9nWmbf8NX7BD96qjpp+jSUkwDYk83VVzqRVt6rabWHOr6FoWZljRxhxtgFYbNoiMHp0nQ4a0lwUL8uyVRKdNE/nnP0VOPz3svQMAIBwMHQ2ByUNHvUOGTE1iRCnRRvzg43vj6o6vo+rVGz/1d+JHI77uRKSbyFKrgqZuT3eizZXpYghUtAF67NkjMmeOyOTJak42571q+HBLZszIo4INABB7JNpCYHJFm5tsU20gidF0/ChUnJm4/7onyvfGNT2+rm0QP3eGvgYydDSAirbSwtKkx5S2SL7d1GII6v1K1yrTQBzNn+8sdvDhh87tfv0sueWWTXLOOe0kP5/XGgAADB0NgckVbZEYNhfQ/uvaRpSSDCbG98Y1Pb6ubZg+NNL0VU3dyuPAho7qWnX02zna9qmi7ds52pQ6q87X/QPi6ssvRS6+WOSUU5wkW0WFyMMPiyxaZMmAAQzTBgDARaItBFFJtOmuCDM9kadrG8TPIr6mSxyJNuLncnxvXBMr2ooLixtWtKVUsGW66qjC8FGgeaqrRe64w1ng4NlnRfLzRa68UuTTT0XGjBEp0HOZAQDAWAwdDYE7mbPuVUd1DZWJSpLB1CSG97iamAQwPb43rs7h2TrjJ10nDFz92PTFEHQuJOCNW1dXF0hFm9/PkVvRtrVm6z5XtHkTbbvrdidiAsicmunkj38UufZakf/7P+e+QYNEZs5Uw0XD3jsAAHIXFW0hiEpFG/Gbjq9rG8QPN743rqnxSdZmHl9HoiqIqks3EabrHHKfFx0Vc6lJsXRztGW6GIJCRRuQveXLRb7/fZHzznOSbF26iDz1lDM/G0k2AACaRkVbCEi05UZ83RV/gVQtGliNpL3iT/MceUGeQ4GcowYueJGUCNOQ6DG9Yi6QoaPfxtWRiEyXaMu2os3bbhJtQHYee0zkZz8T2b1bpKhI5LrrRH75S5HWrcPeMwAAzECiLQQk2jKLr2vIU5AVbaYnMUyeCF4N0Tb9NWB6fF3bMH2xAt0Vc964uuK7QzN1JPKKC5w52pqTaFPXAbWPKsm2u55J2oFMvfWWyKhRInv2OBVtDzwg0rNn2HsFAIBZSLRFLNEWZDUP8cONr2sbplfMuXHVSoPa4ks0zqEgzlEdiRjTk8GRqGjTmMhLO3Q0y8UQFDfRRkUbkJmvvxb54Q+dJNuwYSJPP62S1mHvFQAA5mGOthATbSZWOwW5aqepE80HOZG9iUmAKFRduskFU/c/Sslg4gc/tDMpfkBztGVb0eatulOLIQBo2scfi5x9tsiaNSJHHinyyCMk2QAA2Fck2kJgqWWcDP4AF5UkgPb5uzRUg6XGNfEcitQ5qukSanoy2/RVO70xdXwh4o2pq+IsqGRzEBVtaiGEbBdDUFrkOwsiUNEGNK6qSmT8eJGjjhL54AORdu1EXnhBpFWrsPcMAABzkWgLAXO0Ed+P+Lq2YXp8b1zihxtf1zZMjx9oIkz3Ygg65mgrbP4cbd6KNhJtQEP19c6iB717i/zqVyJ1dSJDh4q8/75Ijx5h7x0AAGZjjrYQkGiLdsVZkCtGmngOcY7mzjlKom3v8XVWnKn3AmOHjhowR1uLAqeijcUQgGTvvisydqzIwoXObZVsmzlT5Iwzwt4zAACigYq2EEQl0WZsEiCoiew1vby8+23iPH8k2uITX9c2TF91NMjVlXVXtLlVY7oTbWoYqLctVLQB2Vu/XmTkSJEBA5wkW+vWIjNmiCxdSpINAAA/UdEW5mIImld0NDYRpjl+UIsVRKGizcREnjcu8cOdA87UirMgzlF3v02cQ82bxApqMQR1LVJVbdtrt6d9TFNztLEYAuJOrSL68MMiN98ssmWLc9+ll4rcdZdIly5h7x0AANFDoi0ElrAYQpTjRymJYWJ8b1xT4weVDOYcTc+bPDL9GJiYyEuXaHOr2FSiTd3OpF1UtAEib7whcs01Ih995Nzu109k1iyRE08Me88AAIguho6GICpDR4kf0oqUniozE8+hQKqFvv3wb/w5ZHh8UxNhgQ4dNXCxAt3xiwsaLobgHS6aybBRb6KNOdoQR6tXi1x0kchppzlJtooKkdmzRRYvJskGAIBuVLSFwPREm+5qm6hU85iaxDA9vjeusfElmOHHUThHdVRUReEc1T10VGd8FVMlydxKNDfRVlpYmlWizV0MgYo2xMmuXSL33CMybZrze36+yOjRIrfd5iTbAACAfiTaQmB6os34JEZAc6hFYY42HfF1V+R54xK/6fi653FM/Z34DbdBxVx6KrmWOh/bvla0kWhDHFiWyEsviVx7rcjKlc59J53krCZ61FFh7x0AAPHC0NGIJdpIYuTOHGqmJjG8+00Sg/jNiW/qqqO643vjaqs4yzO3oi11nrZERVuLLCvaWAwBMbFsmciQISJDhzpJtv33F3n6aZE33yTJBgBAGEi0hbnqqKaKp6ASScQPN34QK9eSaAtnxcioDJ82dWhn0v5rqtgy/Tqku6LNO08bFW1Aelu3ivz85yJ9+4r85S8iRUUiN90ksny5yMUXq/eSsPcQAIB4YuhoxFYddePWWXXGfoAjfmbxdW3D9ESeNy7xw41vaiIsCudoouJMUyLMTWIFWtGW5RxtLIaAqKqvF5k7V+QXvxBZu9a575xzRO6/X6RHj7D3DgAAkGiL2NBRNy6JtvDia18swvChnUEmMXQlAaJScWZqoo1VR7OIr3voqMY52ppb0cZiCIhqkm34cJHf/9653auXyAMPiJx1Vth7BgAAXAwdDTPRJoZPpG74iojM0RZOfN2JQm9c0+PrPodMTbSZnsiLwtBO3XPA+VnRRqINUTJ9upNkU8NE775b5MMPSbIBAJBrqGiLaEVbJBJhYnYSg1VH01PPi/qfGkJtahLD9PhBVeTp2obp8b0JKlOHjoZa0VbIYgiIp3nzRCZNcn5/8EGRkSPD3iMAAJAOFW0RXAzB9CRAVJIMJDH2vg3iR3tBE10JbW8VlYmLLXjj6qoIC6xiTtP+FxeyGALg2rNHZPZskWHDRCzLSbCRZAMAIHeRaAuBpXpJBn+IjkoSwPT4urZBoo34fsW3axc1fKHAayD8RFgYFW0XHH6BHL//8XLRERdlFIPFEBAFajXRfv1ErrzSWWV00CCRWbPC3isAANAUho5GeOgo8aM/R5uJSQxvXOITvznxdW3DG9PUirNExVyE5mhTSbaFP12YcQwWQ4DJli8Xuf56kT/9ybndvr3I1Kkio0aJFNJ7BwAgp/FWHQISbdGOH1TFn6mJvCgcY+KHG1/3YgVBziOofQ41TYkwt1osyIq2bDF0FCbavNlJqKk52NSQUZVUGztWZPJkkXbtwt47AACQCRJtISDRlhvxdSeRdMU3fSJ7b1ziNx2fRGfT8XUmetQ26qw6Y58j01cdLS4oTvt7NlgMASbZvVtkzhyRW28V2bjRue/ss0XuvVekV6+w9w4AAGSDRFuYiyForkgy9QNiVCrOdK+aSqItuvGjsmCH6eeoyYk27Ym8gOZoU8myfU3mUdEGE6hpe196SeSGG0Q++8y574gjRO6/X+Q//iPsvQMAAPtCTw8cTaKiLR6LLZhejRTENnRX85i6/5yjTfMmXkw/BtoXK8g3e462fR02mlTRxmIIyFGLF4uceqrI0KFOkq1DB6eqbckSkmwAAJiMirYQqHl/FKphiJ/L8YPYhqnxSeRlFl/3QgKpv/vJ3ve6CBwDzXPAuVVjuZhoo6INuWr1apFf/lLkySed2yUlItddJ3LjjSJlZWHvHQAAaC4SbRGuaNM9NJWhr3uJz9DRvW6D+OHEj0rVqMmrgupO1uquONM9B5w7LxuJNkTJ1q0i06c7w0Krq537fvxjkTvuEOnWLey9AwAAkRo6+tBDD8lBBx0kJSUlMmDAAFm0aFGTj3/uuefk0EMPtR9/5JFHyp///Oekv1uWJTfffLN07txZSktLZfDgwfKZO/FFDjB96CgVc/EY9pf6u45tEJ/4ubjqaJQqzkxd1dSXoaMFLIaA3KBWD509W+SQQ0SmTXOSbCef7AwdnTuXJBsAAFETeqLt2Wefleuuu06mTJki77//vhx11FFy5plnyrp169I+/u2335bhw4fLFVdcIR988IGcf/759s9HH32UeMzdd98tM2fOlDlz5sjChQulVatWdsxq9+vDkJmeaDM9vpug0lWRl4iva9XRgKqRvNvStQ1Tz6GgKs54/puOH8Q2dK5q6v3XtPi6K9oYOoqoLHTw8ssiffuKXHmlyPr1zgqiL74o8sYbIsccE/YeAgCASCba7rvvPhk5cqT85Cc/kcMPP9xOjrVs2VIeffTRtI//1a9+JUOGDJEJEybIYYcdJrfddpv0799fHnzwwUQ12wMPPCCTJk2S8847T/r27Stz586Vr7/+Wl5UPZtcWnXU0InIiZ9ZfNOTJEFsg/jEb058ndswfR4+3UNH3SRWLifaWAwBYfroo0I588w8OftskWXLRNq3F5k1S90vct55qg8Y9h4CAIBIztFWW1sr7733nkycODFxX35+vj3Uc8GCBWn/G3W/qoDzUtVqbhJtxYoVsmbNGjuGq7y83B6Sqv7biy++uEHMmpoa+8e1VU2ioRJi9fX2j99UMtD5xdmG37wf3HTE9yaQMomvHqPanOm+eKt5dO6/rvjfrnVhHweT47u/6zjGus/RRHxNrzHjz9FvqXkEM42f1THWfI56r0F5Vp6R55A3Ia/zOcr0GO/ra1jXMS4qKEok2vY1vpsEfO5fz8k7X72T9LdxA8bJiL4jsmpzJnS+XpF+6pEZM2bY/T41ImLWrFly/PHHh7pPmzeLXHttnsyd214sK0+KikTGjRO56SaRtm1D3TUAABCHRNuGDRukrq5OOnbsmHS/uv3JJ5+k/W9UZyrd49X97t/d+xp7TKpp06bJrbfe2uD+9evX+z7cVHXq3VVHN23cJIXV/h+Cri27ysqqldJyd8tGh+A2R6fiTva/ba22GcVXHzy2bNlit10lUvemQ4sO9r/tC9pr2f+KvApnO0UdtMQvry+3/+1U0smOn0mbs6HOGTVReNdWXbXsv6q47Nyqs9TU1cjOqp2ye9tu34+x2vc129dISU2JljZ0Luls/1tulWuJ756jFfkVWuK3z2+f2I6Wc9RKPkczkc0xLq4ttquJdJ2jaihgZWmlneTZummr7Mjf4fs2urbuKpurN0tRTZGR52inok6Jc0nHdVpdn3Weo5V5lfa/XUv3/RyqLHBiVFVX2T9eK9ettONm0+ZMbNu2zZc4yHzqETUSQn2ZqkYzqC9ely9fLh06ONfoMJSWirz5purv5cmwYZZMn54nBx0U2u4AAIAQ5FmJ8qrgqeGc+++/vz3v2sCBAxP333DDDTJ//nx7frVURUVF8sQTT9jztLkefvhhO1G2du1aO9b3vvc9O7ZaDMF10UUX2UM1Vccsk4q2bt26yebNm6XM53XW1dP9+v+9LpurNss5fc+Rkhb7PiymMTtqd8i6Hevk4HYHiw61dbWyYvMK6V3ZO6PHqw9wKmm53377ZfRhRiV6lm9YLodWHqpleK06Bss3LpdDKg5JDH/y26cbPpXimmLp1rmb74k25cstX0pZcZmUlzgJE7+t37Fe6qw66dTa+bDu9zHeVrNNNu7aKAe11fPpo2ZPjazaskp6te+lJf7uPbtl4ecLZWDPgVJQUKDlHP1kwyf2/usa+vfZxs/kgPIDpLjQWd3R72O8qmqVVJRWSJviNqLD2u1r7etDh1Z6PlBX7aqSL77+Qo4++Ggtr2F1jq7eslp6tu8pOtTV18mnGz/N+Dqa7fFV5+iyDcukd/ve2s5Rtf/qGuFWt2VL7eMHaz6QTbs2Nfibuv4fUHZAVm3OhOo/tGvXzk7g+d1/QDKVXDvuuOMSU4eoc1j13caOHSu/+MUv9nqc1GgHXcfpL3+plz17NsuQIe20XD9yjXruVeJaJTjj0N44tjlu7Y1jm2lv9MWtzfWa2ptpHyLUirbKykr7Q6pKkHmp2506pf+Ar+5v6vHuv+o+b6JN3e7Xr1/amMXFxfZPKnVAdJyEg3sMtg+6SrLpiN+mpI39o0tJfokc1uGwrP4b9UEv0+dTDXU6ouMRotPhHQ7XGr9XZa9ENZuOY3xguwNFp45tkitC/T7G5aXl9o8upUWlcuh+h2qL36KwhfSq6GVfv3S9Uel+DfTeL7NE+b4e44Mr9CT6XZ3Lvru+69C2tK10a+MkynUcY3WO7ssxyJTa52zPoWyOr9KnYx/RyY/X8LH7H9tkByzbNu9NHDquuSDbqUeCniLktNNU4ro2NkOJsx16HgVxa3Pc2hvHNtPe6Itbm+s1tTfTeKEm2lR12jHHHCOvv/66vXKou+Pq9tVXX532v1GVb+rv48ePT9z36quvJiriDj74YDvZph7jJtZUh0pVx40ZMyaQdgEAACA3ph4JcoqQfRmKbbq4tTeObY5be+PYZtobfXFrc72m9mY6TUioiTZFza8xYsQIOfbYY+0JbNUcGzt27LBXIVUuu+wye3ip6iQp48aNk5NPPlnuvfde+cEPfiDPPPOMLF68WH7zm9/Yf1ffTqsk3O233y49e/a0E2+TJ0+WLl26JJJ5AAAAiAdV+eZdSMudIkQNG9YxdNStlvRzWHIui1t749jmuLU3jm2mvdEXtzbXa2pvSUmJGYm2YcOG2d8o3nzzzfZiBaoKbd68eYlvKVevXp30xJxwwgny1FNPyaRJk+Smm26yk2lqxdE+ffokzfGmknWjRo2SqqoqOfHEE+2YmT4pAAAAyE3ZTj0S9BQhit/DknNd3NobxzbHrb1xbDPtjb64tTlPQ3szjRV6ok1Rw0QbGyr6plq6KcWFF15o/zT1hE6dOtX+AQAAQHTsy9QjAAAAQcmJRBsAAADg19QjAAAAYSHRBgAAAKPsbeoRAACAsJBoAwAAgHGamnoEAAAgLPGYBQ8AAAAAAADQjEQbAAAAAAAA4AMSbQAAAAAAAIAPSLQBAAAAAAAAPiDRBgAAAAAAAPiARBsAAAAAAADgAxJtAAAAAAAAgA8K/QgSNZZl2f9u3bpVS/z6+nrZtm2blJSUSH5+9HOdcWtvHNtMe6Mvbm2mvdGno81uv8HtRyA30c/zV9zaG8c2x629cWwz7Y2+uLW5XlN7M+3rkWhLQx0QpVu3bmHvCgAAMLAfUV5eHvZuoBH08wAAgM6+Xp7F165ps59ff/21tGnTRvLy8nyPr7KgqnP35ZdfSllZmURd3NobxzbT3uiLW5tpb/TpaLPqUqmOV5cuXWLxbbGp6Of5K27tjWOb49beOLaZ9kZf3Nq8VVN7M+3rUdGWhnrCunbtqn076oDH4SSPa3vj2GbaG31xazPtjT6/20wlW+6jn6dH3NobxzbHrb1xbDPtjb64tblMQ3sz6evxdSsAAAAAAADgAxJtAAAAAAAAgA9ItIWguLhYpkyZYv8bB3FrbxzbTHujL25tpr3RF8c2IxhxO7fi1t44tjlu7Y1jm2lv9MWtzcUht5fFEAAAAAAAAAAfUNEGAAAAAAAA+IBEGwAAAAAAAOADEm0AAAAAAACAD0i0AQAAAAAAAD4g0eaTadOmyXHHHSdt2rSRDh06yPnnny/Lly9Pekx1dbVcddVV0r59e2ndurVccMEFsnbt2qTHrF69Wn7wgx9Iy5Yt7TgTJkyQPXv2SC7629/+Juecc4506dJF8vLy5MUXX0z6u7ov3c+MGTMSjznooIMa/H369OliYnsvv/zyBm0ZMmRI0mM2bdokl1xyiZSVlUnbtm3liiuukO3bt4tp7d29e7fceOONcuSRR0qrVq3sx1x22WXy9ddfJ8Uw6fhmcozV2jE333yzdO7cWUpLS2Xw4MHy2WefGXuMU6U7XupHXbeUU045pcHfRo8eLaa65ZZbGrTn0EMPzeqaHcX3qqgd53Qeeugh+3wvKSmRAQMGyKJFi8LeJeQ4+nn086LWz4tjX49+Xrz6eXHs69HPy51+Hok2n8yfP99+kb7zzjvy6quv2m9OZ5xxhuzYsSPxmGuvvVb++Mc/ynPPPWc/Xr1R/ed//mfi73V1dXbnq7a2Vt5++2154okn5PHHH7cv+LlIte2oo46yT+R0vvnmm6SfRx991H4hqwuY19SpU5MeN3bsWDGxvYrqcHnb8vTTTyf9Xb0xf/zxx/Y58qc//cl+wx81apSY1t6dO3fK+++/L5MnT7b//cMf/mBfxM8999wGjzXl+GZyjO+++26ZOXOmzJkzRxYuXGh3PM8880z7TdrEY5zq3XffTTpWqg3KhRdemHjMyJEjkx6jnhOTHXHEEUnt+cc//pHxNTuq71VRPM5ezz77rFx33XX2ku/q+qVe8+p1vG7durB3DTmMfl5D9PPM7ufFsa9HPy9+/by49fXo50nu9PMsaLFu3TpLPb3z58+3b1dVVVktWrSwnnvuucRjli1bZj9mwYIF9u0///nPVn5+vrVmzZrEY2bPnm2VlZVZNTU1Vi5T7XjhhReafMx5551nnXbaaUn3HXjggdb9999vmSZde0eMGGG3sTH/+te/7P/u3XffTdz3v//7v1ZeXp7173//2zL9+C5atMh+3KpVq4w/vunaXF9fb3Xq1MmaMWNG4j71ui4uLraefvpp449xOuPGjbN69Ohht105+eST7fuiYsqUKdZRRx2V9m+ZXLOj+F4VxeOc6vjjj7euuuqqxO26ujqrS5cu1rRp00LdL5iFfl5D9PPM7gPEra9HPy/6/Twl7n09+nlWaP08Kto02bJli/1vRUWF/e97771nZ5RVCbJLla0ecMABsmDBAvu2+leVZ3fs2DHxGJV93bp1q/3NiclUCe7LL79sl1enUuXlqlz36KOPtocb5OoQiky8+eabdplu7969ZcyYMbJx48bE39TxVSXmxx57bOI+dT7k5+fb35pF4ZxX32SrNkbx+K5YsULWrFmT9BouLy+3y5G9r+GoHGNVcfHkk0/Kf/3Xf9nH1fW73/1OKisrpU+fPjJx4kT7G2+TqSEhaghJ9+7d7W+p1bCuTK/ZUXyviupx9p7X6th6j6t6farbUTqu0I9+XjL6edHqA8Sxr0c/L7rv/3Hu69HPk9D6eYWBbi0m6uvrZfz48fK9733PPnkVdeEuKipq8MakOlvqb+5jvJ0v9+/u30ymhkeoseKppbjXXHON9O/f337xq2EU6oWuylfvu+8+MY0aTqDad/DBB8sXX3whN910k5x11ln2i7qgoMA+hqpz5lVYWGi33fTjq0rq1Twew4cPt+esiOLxdY9Ruteo9zUclWOs5i2pqqqy56Rx/ehHP5IDDzzQ7qwsXbrUPuZqGIkaTmIi1XlWw7bUByZ1Xt56660yaNAg+eijjzK6ZkfxvSqKx9lrw4YN9vC9dK/jTz75JLT9glno5zVEPy/a/bw49PXo50Xz/T/OfT36eeH280i0aaDGRasXr3f8d9ypeTvUNwhqQkIvNX7a1bdvX/ti97Of/cyeyLG4uFhMcvHFFyd+V99Yq/b06NHD/vbz9NNPl6hS3wRddNFF9gSys2fPjuzxjZtHHnnE/gCh3oRd3jlI1DmuJgtW57b6wKHOddOo9nnPT9UZUx2P3//+9/YkyHF9r4racQb8Rj+vIfp50e3nKfT1oicO/by49/Xo54WLoaM+u/rqq+2JMd944w3p2rVr4v5OnTrZpYzqm4PUUnv1N/cxqaucuLfdx5jo73//u50l/+lPf7rXx6qLnyo3X7lypZhOlSerktzPP/88cQxTJ2FUbVWrF5l6fN2O16pVq+wJN73fcEbt+LrHKN1r1PsajsIxVsfztdde2+trVh1PxT3HTae+0ezVq5fdnkyu2VF8r4r6cVbXZFV50tTrGGgK/byG6OdFt58Xp74e/bxov//Hra9HP29t6MeURJtP1Dc86oR+4YUX5K9//atdVu51zDHHSIsWLeT1119P3Kc6JWqM+MCBA+3b6t8PP/ww6QLuvqEdfvjhYvI3Jqr9asWPvVmyZIk9jjq1LNtEX331lT13h/qWwD2+6kKuxo271LmiynrdC5yJHS8174F6s1Zzc0T5+KrXtLpAe1/Dal4dNSeH9zUchWP82GOP2cdIrY63t+OpuOe46bZv325/m6fak8k1O4rvVVE/zqrSQh1b73FVr0912+TjCv3o5zWOfl40+3lx6+vRz4v2+39c+nr084pyp58X6NILETZmzBirvLzcevPNN61vvvkm8bNz587EY0aPHm0dcMAB1l//+ldr8eLF1sCBA+0f1549e6w+ffpYZ5xxhrVkyRJr3rx51n777WdNnDjRykXbtm2zPvjgA/tHnUr33Xef/bt3JaItW7ZYLVu2tFfVSvX222/bqxSptn7xxRfWk08+abf3sssus0xrr/rb9ddfb69Qs2LFCuu1116z+vfvb/Xs2dOqrq5OxBgyZIh19NFHWwsXLrT+8Y9/2H8fPny4ZVp7a2trrXPPPdfq2rWrffy857y7cpppxzeTc3r69OlW27ZtrZdeeslaunSpvfrYwQcfbO3atcvIY5yOWplHXaduvPHGpPs///xza+rUqfa1S53j6jno3r27ddJJJ1mm+vnPf25fs1V73nrrLWvw4MFWZWWlvUJTJtfsKL5XRfE4p3rmmWfsVeQef/xxewW5UaNG2a9r70qQQCr6efTzotbPi2Nfj35evPp5cezr0c+zcqafR6LNJ+pine7nscceSzxGXaSvvPJKq127dnanZOjQofaJ77Vy5UrrrLPOskpLS+2LgLo47N6928pFb7zxRto2q+XPXb/+9a/ttqjlk1O999571oABA+yLQUlJiXXYYYdZd955Z1KHxZT2qouX6jirzoVaJlotdT5y5MgGL+iNGzfab8atW7e2ysrKrJ/85Cf2m75p7VUX5sbOefXfmXh8Mzmn1fLnkydPtjp27GhfwE8//XRr+fLlxh7jdF555RW7zantWr16tf0mXFFRYbf9kEMOsSZMmGB/yDLVsGHDrM6dO1tFRUXW/vvvb99WHZBsrtlRe6+K4nFOZ9asWXbHWh17tQz8O++8E/YuIcfRz6OfF7V+Xhz7evTz4tXPi2Nfj35e7vTz8tT/BVtDBwAAAAAAAEQPc7QBAAAAAAAAPiDRBgAAAAAAAPiARBsAAAAAAADgAxJtAAAAAAAAgA9ItAEAAAAAAAA+INEGAAAAAAAA+IBEGwAAAAAAAOADEm0AAAAAAACAD0i0AQAAAAAAAD4g0QYAe3H55ZdLXl6eTJ8+Pen+F1980b4fAAAA5qKvB8BPJNoAIAMlJSVy1113yebNm8PeFQAAAPiMvh4Av5BoA4AMDB48WDp16iTTpk0Le1cAAADgM/p6APxCog0AMlBQUCB33nmnzJo1S7766quwdwcAAAA+oq8HwC8k2gAgQ0OHDpV+/frJlClTwt4VAAAA+Iy+HgA/kGgDgCyouTueeOIJWbZsWdi7AgAAAJ/R1wPQXCTaACALJ510kpx55pkyceLEsHcFAAAAPqOvB6C5CsPeAQAwjVr6XQ0r6N27d9i7AgAAAJ/R1wPQHFS0AUCWjjzySLnkkktk5syZYe8KAAAAfEZfD0BzkGgDgH0wdepUqa+vD3s3AAAAoAF9PQD7Ks+yLGuf/2sAAAAAAAAANiraAAAAAAAAAB+QaAMAAAAAAAB8QKINAAAAAAAA8AGJNgAAAAAAAMAHJNoAAAAAAAAAH5BoAwAAAAAAAHxAog0AAAAAAADwAYk2AAAAAAAAwAck2gAAAAAAAAAfkGgDAAAAAAAAfECiDQAAAAAAAJDm+3+mloOqzcGrkgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc9b1aa9ede4bd49bb3d3f573528ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Reverse:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing N=200 ---\n",
      "  [N=200] Launching 5 parallel tasks...\n",
      "  [N=200] Best candidate score: 0.159613\n",
      "  [N=200] Baseline score: 0.370451, Candidate score: 0.159613\n",
      "  [N=200] ✓ IMPROVED over baseline: 0.370451 -> 0.159613 (Δ=0.210839, 56.91%)\n",
      "⚠️  Collision at N=200 between trees 0 and 18 (overlap area=1.182419e-01)\n",
      "  [N=200] Found 875 collision(s)\n",
      "  [N=200] Too many collisions - reverting to baseline\n",
      "  [N=200] Cumulative Score: 0.370451 | Total Improvements: 1\n",
      "  [N=200] Best candidate score: 0.159613\n",
      "  [N=200] Baseline score: 0.370451, Candidate score: 0.159613\n",
      "  [N=200] ✓ IMPROVED over baseline: 0.370451 -> 0.159613 (Δ=0.210839, 56.91%)\n",
      "⚠️  Collision at N=200 between trees 0 and 18 (overlap area=1.182419e-01)\n",
      "  [N=200] Found 875 collision(s)\n",
      "  [N=200] Too many collisions - reverting to baseline\n",
      "  [N=200] Cumulative Score: 0.370451 | Total Improvements: 1\n",
      "\n",
      "--- Processing N=199 ---\n",
      "  [N=199] Launching 5 parallel tasks...\n",
      "\n",
      "--- Processing N=199 ---\n",
      "  [N=199] Launching 5 parallel tasks...\n",
      "  [N=199] Best candidate score: 0.160415\n",
      "  [N=199] Baseline score: 0.369515, Candidate score: 0.160415\n",
      "  [N=199] ✓ IMPROVED over baseline: 0.369515 -> 0.160415 (Δ=0.209100, 56.59%)\n",
      "  [N=199] Cumulative Score: 0.530866 | Total Improvements: 2\n",
      "\n",
      "--- Processing N=198 ---\n",
      "  [N=198] Launching 5 parallel tasks...\n",
      "  [N=199] Best candidate score: 0.160415\n",
      "  [N=199] Baseline score: 0.369515, Candidate score: 0.160415\n",
      "  [N=199] ✓ IMPROVED over baseline: 0.369515 -> 0.160415 (Δ=0.209100, 56.59%)\n",
      "  [N=199] Cumulative Score: 0.530866 | Total Improvements: 2\n",
      "\n",
      "--- Processing N=198 ---\n",
      "  [N=198] Launching 5 parallel tasks...\n",
      "  [N=198] Best candidate score: 0.161225\n",
      "  [N=198] Baseline score: 0.367559, Candidate score: 0.161225\n",
      "  [N=198] ✓ IMPROVED over baseline: 0.367559 -> 0.161225 (Δ=0.206334, 56.14%)\n",
      "  [N=198] Cumulative Score: 0.692090 | Total Improvements: 3\n",
      "\n",
      "--- Processing N=197 ---\n",
      "  [N=197] Launching 5 parallel tasks...\n",
      "  [N=198] Best candidate score: 0.161225\n",
      "  [N=198] Baseline score: 0.367559, Candidate score: 0.161225\n",
      "  [N=198] ✓ IMPROVED over baseline: 0.367559 -> 0.161225 (Δ=0.206334, 56.14%)\n",
      "  [N=198] Cumulative Score: 0.692090 | Total Improvements: 3\n",
      "\n",
      "--- Processing N=197 ---\n",
      "  [N=197] Launching 5 parallel tasks...\n",
      "  [N=197] Best candidate score: 0.162043\n",
      "  [N=197] Baseline score: 0.369417, Candidate score: 0.162043\n",
      "  [N=197] ✓ IMPROVED over baseline: 0.369417 -> 0.162043 (Δ=0.207374, 56.14%)\n",
      "  [N=197] Cumulative Score: 0.854134 | Total Improvements: 4\n",
      "\n",
      "--- Processing N=196 ---\n",
      "  [N=196] Launching 5 parallel tasks...\n",
      "  [N=197] Best candidate score: 0.162043\n",
      "  [N=197] Baseline score: 0.369417, Candidate score: 0.162043\n",
      "  [N=197] ✓ IMPROVED over baseline: 0.369417 -> 0.162043 (Δ=0.207374, 56.14%)\n",
      "  [N=197] Cumulative Score: 0.854134 | Total Improvements: 4\n",
      "\n",
      "--- Processing N=196 ---\n",
      "  [N=196] Launching 5 parallel tasks...\n",
      "  [N=196] Best candidate score: 0.162870\n",
      "  [N=196] Baseline score: 0.366700, Candidate score: 0.162870\n",
      "  [N=196] ✓ IMPROVED over baseline: 0.366700 -> 0.162870 (Δ=0.203830, 55.58%)\n",
      "  [N=196] Cumulative Score: 1.017004 | Total Improvements: 5\n",
      "\n",
      "--- Processing N=195 ---\n",
      "  [N=195] Launching 5 parallel tasks...\n",
      "  [N=196] Best candidate score: 0.162870\n",
      "  [N=196] Baseline score: 0.366700, Candidate score: 0.162870\n",
      "  [N=196] ✓ IMPROVED over baseline: 0.366700 -> 0.162870 (Δ=0.203830, 55.58%)\n",
      "  [N=196] Cumulative Score: 1.017004 | Total Improvements: 5\n",
      "\n",
      "--- Processing N=195 ---\n",
      "  [N=195] Launching 5 parallel tasks...\n",
      "  [N=195] Best candidate score: 0.163705\n",
      "  [N=195] Baseline score: 0.368402, Candidate score: 0.163705\n",
      "  [N=195] ✓ IMPROVED over baseline: 0.368402 -> 0.163705 (Δ=0.204696, 55.56%)\n",
      "  [N=195] Cumulative Score: 1.180709 | Total Improvements: 6\n",
      "\n",
      "--- Processing N=194 ---\n",
      "  [N=195] Best candidate score: 0.163705\n",
      "  [N=195] Baseline score: 0.368402, Candidate score: 0.163705\n",
      "  [N=195] ✓ IMPROVED over baseline: 0.368402 -> 0.163705 (Δ=0.204696, 55.56%)\n",
      "  [N=195] Cumulative Score: 1.180709 | Total Improvements: 6\n",
      "\n",
      "--- Processing N=194 ---\n",
      "  [N=194] Launching 5 parallel tasks...\n",
      "  [N=194] Launching 5 parallel tasks...\n",
      "  [N=194] Best candidate score: 0.164549\n",
      "  [N=194] Baseline score: 0.367835, Candidate score: 0.164549\n",
      "  [N=194] ✓ IMPROVED over baseline: 0.367835 -> 0.164549 (Δ=0.203286, 55.27%)\n",
      "  [N=194] Cumulative Score: 1.345258 | Total Improvements: 7\n",
      "\n",
      "--- Processing N=193 ---\n",
      "  [N=193] Launching 5 parallel tasks...\n",
      "  [N=194] Best candidate score: 0.164549\n",
      "  [N=194] Baseline score: 0.367835, Candidate score: 0.164549\n",
      "  [N=194] ✓ IMPROVED over baseline: 0.367835 -> 0.164549 (Δ=0.203286, 55.27%)\n",
      "  [N=194] Cumulative Score: 1.345258 | Total Improvements: 7\n",
      "\n",
      "--- Processing N=193 ---\n",
      "  [N=193] Launching 5 parallel tasks...\n",
      "  [N=193] Best candidate score: 0.165402\n",
      "  [N=193] Baseline score: 0.367906, Candidate score: 0.165402\n",
      "  [N=193] ✓ IMPROVED over baseline: 0.367906 -> 0.165402 (Δ=0.202505, 55.04%)\n",
      "  [N=193] Cumulative Score: 1.510659 | Total Improvements: 8\n",
      "\n",
      "--- Processing N=192 ---\n",
      "  [N=192] Launching 5 parallel tasks...\n",
      "  [N=193] Best candidate score: 0.165402\n",
      "  [N=193] Baseline score: 0.367906, Candidate score: 0.165402\n",
      "  [N=193] ✓ IMPROVED over baseline: 0.367906 -> 0.165402 (Δ=0.202505, 55.04%)\n",
      "  [N=193] Cumulative Score: 1.510659 | Total Improvements: 8\n",
      "\n",
      "--- Processing N=192 ---\n",
      "  [N=192] Launching 5 parallel tasks...\n",
      "  [N=192] Best candidate score: 0.166263\n",
      "  [N=192] Baseline score: 0.368722, Candidate score: 0.166263\n",
      "  [N=192] ✓ IMPROVED over baseline: 0.368722 -> 0.166263 (Δ=0.202459, 54.91%)\n",
      "  [N=192] Cumulative Score: 1.676922 | Total Improvements: 9\n",
      "\n",
      "--- Processing N=191 ---\n",
      "  [N=191] Launching 5 parallel tasks...\n",
      "  [N=192] Best candidate score: 0.166263\n",
      "  [N=192] Baseline score: 0.368722, Candidate score: 0.166263\n",
      "  [N=192] ✓ IMPROVED over baseline: 0.368722 -> 0.166263 (Δ=0.202459, 54.91%)\n",
      "  [N=192] Cumulative Score: 1.676922 | Total Improvements: 9\n",
      "\n",
      "--- Processing N=191 ---\n",
      "  [N=191] Launching 5 parallel tasks...\n",
      "  [N=191] Best candidate score: 0.167134\n",
      "  [N=191] Baseline score: 0.365197, Candidate score: 0.167134\n",
      "  [N=191] ✓ IMPROVED over baseline: 0.365197 -> 0.167134 (Δ=0.198064, 54.23%)\n",
      "  [N=191] Cumulative Score: 1.844056 | Total Improvements: 10\n",
      "\n",
      "--- Processing N=190 ---\n",
      "  [N=190] Launching 5 parallel tasks...\n",
      "  [N=191] Best candidate score: 0.167134\n",
      "  [N=191] Baseline score: 0.365197, Candidate score: 0.167134\n",
      "  [N=191] ✓ IMPROVED over baseline: 0.365197 -> 0.167134 (Δ=0.198064, 54.23%)\n",
      "  [N=191] Cumulative Score: 1.844056 | Total Improvements: 10\n",
      "\n",
      "--- Processing N=190 ---\n",
      "  [N=190] Launching 5 parallel tasks...\n",
      "  [N=190] Best candidate score: 0.168013\n",
      "  [N=190] Baseline score: 0.367084, Candidate score: 0.168013\n",
      "  [N=190] ✓ IMPROVED over baseline: 0.367084 -> 0.168013 (Δ=0.199071, 54.23%)\n",
      "⚠️  Collision at N=190 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=190] Found 810 collision(s)\n",
      "  [N=190] Too many collisions - reverting to baseline\n",
      "  [N=190] Cumulative Score: 2.211140 | Total Improvements: 11\n",
      "  [N=190] Best candidate score: 0.168013\n",
      "  [N=190] Baseline score: 0.367084, Candidate score: 0.168013\n",
      "  [N=190] ✓ IMPROVED over baseline: 0.367084 -> 0.168013 (Δ=0.199071, 54.23%)\n",
      "⚠️  Collision at N=190 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=190] Found 810 collision(s)\n",
      "  [N=190] Too many collisions - reverting to baseline\n",
      "  [N=190] Cumulative Score: 2.211140 | Total Improvements: 11\n",
      "\n",
      "--- Processing N=189 ---\n",
      "  [N=189] Launching 5 parallel tasks...\n",
      "\n",
      "--- Processing N=189 ---\n",
      "  [N=189] Launching 5 parallel tasks...\n",
      "  [N=189] Best candidate score: 0.168902\n",
      "  [N=189] Baseline score: 0.369002, Candidate score: 0.168902\n",
      "  [N=189] ✓ IMPROVED over baseline: 0.369002 -> 0.168902 (Δ=0.200100, 54.23%)\n",
      "  [N=189] Cumulative Score: 2.380042 | Total Improvements: 12\n",
      "\n",
      "--- Processing N=188 ---\n",
      "  [N=188] Launching 5 parallel tasks...\n",
      "  [N=189] Best candidate score: 0.168902\n",
      "  [N=189] Baseline score: 0.369002, Candidate score: 0.168902\n",
      "  [N=189] ✓ IMPROVED over baseline: 0.369002 -> 0.168902 (Δ=0.200100, 54.23%)\n",
      "  [N=189] Cumulative Score: 2.380042 | Total Improvements: 12\n",
      "\n",
      "--- Processing N=188 ---\n",
      "  [N=188] Launching 5 parallel tasks...\n",
      "  [N=188] Best candidate score: 0.169801\n",
      "  [N=188] Baseline score: 0.370964, Candidate score: 0.169801\n",
      "  [N=188] ✓ IMPROVED over baseline: 0.370964 -> 0.169801 (Δ=0.201164, 54.23%)\n",
      "  [N=188] Cumulative Score: 2.549842 | Total Improvements: 13\n",
      "\n",
      "--- Processing N=187 ---\n",
      "  [N=187] Launching 5 parallel tasks...\n",
      "  [N=188] Best candidate score: 0.169801\n",
      "  [N=188] Baseline score: 0.370964, Candidate score: 0.169801\n",
      "  [N=188] ✓ IMPROVED over baseline: 0.370964 -> 0.169801 (Δ=0.201164, 54.23%)\n",
      "  [N=188] Cumulative Score: 2.549842 | Total Improvements: 13\n",
      "\n",
      "--- Processing N=187 ---\n",
      "  [N=187] Launching 5 parallel tasks...\n",
      "  [N=187] Best candidate score: 0.170709\n",
      "  [N=187] Baseline score: 0.372943, Candidate score: 0.170709\n",
      "  [N=187] ✓ IMPROVED over baseline: 0.372943 -> 0.170709 (Δ=0.202234, 54.23%)\n",
      "  [N=187] Cumulative Score: 2.720551 | Total Improvements: 14\n",
      "\n",
      "--- Processing N=186 ---\n",
      "  [N=186] Launching 5 parallel tasks...\n",
      "  [N=187] Best candidate score: 0.170709\n",
      "  [N=187] Baseline score: 0.372943, Candidate score: 0.170709\n",
      "  [N=187] ✓ IMPROVED over baseline: 0.372943 -> 0.170709 (Δ=0.202234, 54.23%)\n",
      "  [N=187] Cumulative Score: 2.720551 | Total Improvements: 14\n",
      "\n",
      "--- Processing N=186 ---\n",
      "  [N=186] Launching 5 parallel tasks...\n",
      "  [N=186] Best candidate score: 0.171626\n",
      "  [N=186] Baseline score: 0.373820, Candidate score: 0.171626\n",
      "  [N=186] ✓ IMPROVED over baseline: 0.373820 -> 0.171626 (Δ=0.202194, 54.09%)\n",
      "  [N=186] Cumulative Score: 2.892177 | Total Improvements: 15\n",
      "\n",
      "--- Processing N=185 ---\n",
      "  [N=185] Launching 5 parallel tasks...\n",
      "  [N=186] Best candidate score: 0.171626\n",
      "  [N=186] Baseline score: 0.373820, Candidate score: 0.171626\n",
      "  [N=186] ✓ IMPROVED over baseline: 0.373820 -> 0.171626 (Δ=0.202194, 54.09%)\n",
      "  [N=186] Cumulative Score: 2.892177 | Total Improvements: 15\n",
      "\n",
      "--- Processing N=185 ---\n",
      "  [N=185] Launching 5 parallel tasks...\n",
      "  [N=185] Best candidate score: 0.172554\n",
      "  [N=185] Baseline score: 0.370740, Candidate score: 0.172554\n",
      "  [N=185] ✓ IMPROVED over baseline: 0.370740 -> 0.172554 (Δ=0.198186, 53.46%)\n",
      "  [N=185] Cumulative Score: 3.064731 | Total Improvements: 16\n",
      "  [N=185] Best candidate score: 0.172554\n",
      "  [N=185] Baseline score: 0.370740, Candidate score: 0.172554\n",
      "  [N=185] ✓ IMPROVED over baseline: 0.370740 -> 0.172554 (Δ=0.198186, 53.46%)\n",
      "  [N=185] Cumulative Score: 3.064731 | Total Improvements: 16\n",
      "\n",
      "--- Processing N=184 ---\n",
      "  [N=184] Launching 5 parallel tasks...\n",
      "\n",
      "--- Processing N=184 ---\n",
      "  [N=184] Launching 5 parallel tasks...\n",
      "  [N=184] Best candidate score: 0.173492\n",
      "  [N=184] Baseline score: 0.368257, Candidate score: 0.173492\n",
      "  [N=184] ✓ IMPROVED over baseline: 0.368257 -> 0.173492 (Δ=0.194765, 52.89%)\n",
      "  [N=184] Cumulative Score: 3.238223 | Total Improvements: 17\n",
      "\n",
      "--- Processing N=183 ---\n",
      "  [N=183] Launching 5 parallel tasks...\n",
      "  [N=184] Best candidate score: 0.173492\n",
      "  [N=184] Baseline score: 0.368257, Candidate score: 0.173492\n",
      "  [N=184] ✓ IMPROVED over baseline: 0.368257 -> 0.173492 (Δ=0.194765, 52.89%)\n",
      "  [N=184] Cumulative Score: 3.238223 | Total Improvements: 17\n",
      "\n",
      "--- Processing N=183 ---\n",
      "  [N=183] Launching 5 parallel tasks...\n",
      "  [N=183] Best candidate score: 0.174440\n",
      "  [N=183] Baseline score: 0.370269, Candidate score: 0.174440\n",
      "  [N=183] ✓ IMPROVED over baseline: 0.370269 -> 0.174440 (Δ=0.195829, 52.89%)\n",
      "  [N=183] Cumulative Score: 3.412663 | Total Improvements: 18\n",
      "\n",
     Backups.optimization_utils=182 ---\n",
      "  [N=182] Launching 5 parallel tasks...\n",
      "  [N=183] Best candidate score: 0.174440\n",
      "  [N=183] Baseline score: 0.370269, Candidate score: 0.174440\n",
      "  [N=183] ✓ IMPROVED over baseline: 0.370269 -> 0.174440 (Δ=0.195829, 52.89%)\n",
      "  [N=183] Cumulative Score: 3.412663 | Total Improvements: 18\n",
      "\n",
      "--- Processing N=182 ---\n",
      "  [N=182] Launching 5 parallel tasks...\n",
      "  [N=182] Best candidate score: 0.175398\n",
      "  [N=182] Baseline score: 0.367524, Candidate score: 0.175398\n",
      "  [N=182] ✓ IMPROVED over baseline: 0.367524 -> 0.175398 (Δ=0.192125, 52.28%)\n",
      "  [N=182] Cumulative Score: 3.588061 | Total Improvements: 19\n",
      "\n",
      "--- Processing N=181 ---\n",
      "  [N=181] Launching 5 parallel tasks...\n",
      "  [N=182] Best candidate score: 0.175398\n",
      "  [N=182] Baseline score: 0.367524, Candidate score: 0.175398\n",
      "  [N=182] ✓ IMPROVED over baseline: 0.367524 -> 0.175398 (Δ=0.192125, 52.28%)\n",
      "  [N=182] Cumulative Score: 3.588061 | Total Improvements: 19\n",
      "\n",
      "--- Processing N=181 ---\n",
      "  [N=181] Launching 5 parallel tasks...\n",
      "  [N=181] Best candidate score: 0.167127\n",
      "  [N=181] Baseline score: 0.369483, Candidate score: 0.167127\n",
      "  [N=181] ✓ IMPROVED over baseline: 0.369483 -> 0.167127 (Δ=0.202356, 54.77%)\n",
      "  [N=181] Cumulative Score: 3.755188 | Total Improvements: 20\n",
      "\n",
      "--- Processing N=180 ---\n",
      "  [N=180] Launching 10 parallel tasks...\n",
      "  [N=181] Best candidate score: 0.167127\n",
      "  [N=181] Baseline score: 0.369483, Candidate score: 0.167127\n",
      "  [N=181] ✓ IMPROVED over baseline: 0.369483 -> 0.167127 (Δ=0.202356, 54.77%)\n",
      "  [N=181] Cumulative Score: 3.755188 | Total Improvements: 20\n",
      "\n",
      "--- Processing N=180 ---\n",
      "  [N=180] Launching 10 parallel tasks...\n",
      "  [N=180] Best candidate score: 0.168056\n",
      "  [N=180] Baseline score: 0.366734, Candidate score: 0.168056\n",
      "  [N=180] ✓ IMPROVED over baseline: 0.366734 -> 0.168056 (Δ=0.198679, 54.18%)\n",
      "⚠️  Collision at N=180 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=180] Found 750 collision(s)\n",
      "  [N=180] Too many collisions - reverting to baseline\n",
      "  [N=180] Cumulative Score: 4.121923 | Total Improvements: 21\n",
      "  [N=180] Best candidate score: 0.168056\n",
      "  [N=180] Baseline score: 0.366734, Candidate score: 0.168056\n",
      "  [N=180] ✓ IMPROVED over baseline: 0.366734 -> 0.168056 (Δ=0.198679, 54.18%)\n",
      "⚠️  Collision at N=180 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=180] Found 750 collision(s)\n",
      "  [N=180] Too many collisions - reverting to baseline\n",
      "  [N=180] Cumulative Score: 4.121923 | Total Improvements: 21\n",
      "\n",
      "--- Processing N=179 ---\n",
      "\n",
      "--- Processing N=179 ---\n",
      "  [N=179] Launching 10 parallel tasks...\n",
      "  [N=179] Launching 10 parallel tasks...\n",
      "  [N=179] Best candidate score: 0.168994\n",
      "  [N=179] Baseline score: 0.368782, Candidate score: 0.168994\n",
      "  [N=179] ✓ IMPROVED over baseline: 0.368782 -> 0.168994 (Δ=0.199788, 54.18%)\n",
      "  [N=179] Cumulative Score: 4.290917 | Total Improvements: 22\n",
      "\n",
      "--- Processing N=178 ---\n",
      "  [N=178] Launching 10 parallel tasks...\n",
      "  [N=179] Best candidate score: 0.168994\n",
      "  [N=179] Baseline score: 0.368782, Candidate score: 0.168994\n",
      "  [N=179] ✓ IMPROVED over baseline: 0.368782 -> 0.168994 (Δ=0.199788, 54.18%)\n",
      "  [N=179] Cumulative Score: 4.290917 | Total Improvements: 22\n",
      "\n",
      "--- Processing N=178 ---\n",
      "  [N=178] Launching 10 parallel tasks...\n",
      "  [N=178] Best candidate score: 0.169944\n",
      "  [N=178] Baseline score: 0.370854, Candidate score: 0.169944\n",
      "  [N=178] ✓ IMPROVED over baseline: 0.370854 -> 0.169944 (Δ=0.200910, 54.17%)\n",
      "  [N=178] Cumulative Score: 4.460861 | Total Improvements: 23\n",
      "\n",
      "--- Processing N=177 ---\n",
      "  [N=177] Launching 10 parallel tasks...\n",
      "  [N=178] Best candidate score: 0.169944\n",
      "  [N=178] Baseline score: 0.370854, Candidate score: 0.169944\n",
      "  [N=178] ✓ IMPROVED over baseline: 0.370854 -> 0.169944 (Δ=0.200910, 54.17%)\n",
      "  [N=178] Cumulative Score: 4.460861 | Total Improvements: 23\n",
      "\n",
      "--- Processing N=177 ---\n",
      "  [N=177] Launching 10 parallel tasks...\n",
      "  [N=177] Best candidate score: 0.170904\n",
      "  [N=177] Baseline score: 0.368052, Candidate score: 0.170904\n",
      "  [N=177] ✓ IMPROVED over baseline: 0.368052 -> 0.170904 (Δ=0.197148, 53.57%)\n",
      "  [N=177] Cumulative Score: 4.631765 | Total Improvements: 24\n",
      "\n",
      "--- Processing N=176 ---\n",
      "  [N=176] Launching 10 parallel tasks...\n",
      "  [N=177] Best candidate score: 0.170904\n",
      "  [N=177] Baseline score: 0.368052, Candidate score: 0.170904\n",
      "  [N=177] ✓ IMPROVED over baseline: 0.368052 -> 0.170904 (Δ=0.197148, 53.57%)\n",
      "  [N=177] Cumulative Score: 4.631765 | Total Improvements: 24\n",
      "\n",
      "--- Processing N=176 ---\n",
      "  [N=176] Launching 10 parallel tasks...\n",
      "  [N=176] Best candidate score: 0.171875\n",
      "  [N=176] Baseline score: 0.368762, Candidate score: 0.171875\n",
      "  [N=176] ✓ IMPROVED over baseline: 0.368762 -> 0.171875 (Δ=0.196887, 53.39%)\n",
      "  [N=176] Cumulative Score: 4.803640 | Total Improvements: 25\n",
      "\n",
      "--- Processing N=175 ---\n",
      "  [N=175] Launching 10 parallel tasks...\n",
      "  [N=176] Best candidate score: 0.171875\n",
      "  [N=176] Baseline score: 0.368762, Candidate score: 0.171875\n",
      "  [N=176] ✓ IMPROVED over baseline: 0.368762 -> 0.171875 (Δ=0.196887, 53.39%)\n",
      "  [N=176] Cumulative Score: 4.803640 | Total Improvements: 25\n",
      "\n",
      "--- Processing N=175 ---\n",
      "  [N=175] Launching 10 parallel tasks...\n",
      "  [N=175] Best candidate score: 0.172857\n",
      "  [N=175] Baseline score: 0.370737, Candidate score: 0.172857\n",
      "  [N=175] ✓ IMPROVED over baseline: 0.370737 -> 0.172857 (Δ=0.197880, 53.37%)\n",
      "  [N=175] Cumulative Score: 4.976497 | Total Improvements: 26\n",
      "  [N=175] Best candidate score: 0.172857\n",
      "  [N=175] Baseline score: 0.370737, Candidate score: 0.172857\n",
      "  [N=175] ✓ IMPROVED over baseline: 0.370737 -> 0.172857 (Δ=0.197880, 53.37%)\n",
      "  [N=175] Cumulative Score: 4.976497 | Total Improvements: 26\n",
      "\n",
      "--- Processing N=174 ---\n",
      "  [N=174] Launching 10 parallel tasks...\n",
      "\n",
      "--- Processing N=174 ---\n",
      "  [N=174] Launching 10 parallel tasks...\n",
      "  [N=174] Best candidate score: 0.173851\n",
      "  [N=174] Baseline score: 0.368172, Candidate score: 0.173851\n",
      "  [N=174] ✓ IMPROVED over baseline: 0.368172 -> 0.173851 (Δ=0.194321, 52.78%)\n",
      "  [N=174] Cumulative Score: 5.150348 | Total Improvements: 27\n",
      "\n",
      "--- Processing N=173 ---\n",
      "  [N=173] Launching 10 parallel tasks...\n",
      "  [N=174] Best candidate score: 0.173851\n",
      "  [N=174] Baseline score: 0.368172, Candidate score: 0.173851\n",
      "  [N=174] ✓ IMPROVED over baseline: 0.368172 -> 0.173851 (Δ=0.194321, 52.78%)\n",
      "  [N=174] Cumulative Score: 5.150348 | Total Improvements: 27\n",
      "\n",
      "--- Processing N=173 ---\n",
      "  [N=173] Launching 10 parallel tasks...\n",
      "  [N=173] Best candidate score: 0.174855\n",
      "  [N=173] Baseline score: 0.369973, Candidate score: 0.174855\n",
      "  [N=173] ✓ IMPROVED over baseline: 0.369973 -> 0.174855 (Δ=0.195117, 52.74%)\n",
      "  [N=173] Cumulative Score: 5.325203 | Total Improvements: 28\n",
      "\n",
      "--- Processing N=172 ---\n",
      "  [N=172] Launching 10 parallel tasks...\n",
      "  [N=173] Best candidate score: 0.174855\n",
      "  [N=173] Baseline score: 0.369973, Candidate score: 0.174855\n",
      "  [N=173] ✓ IMPROVED over baseline: 0.369973 -> 0.174855 (Δ=0.195117, 52.74%)\n",
      "  [N=173] Cumulative Score: 5.325203 | Total Improvements: 28\n",
      "\n",
      "--- Processing N=172 ---\n",
      "  [N=172] Launching 10 parallel tasks...\n",
      "  [N=172] Best candidate score: 0.175872\n",
      "  [N=172] Baseline score: 0.371911, Candidate score: 0.175872\n",
      "  [N=172] ✓ IMPROVED over baseline: 0.371911 -> 0.175872 (Δ=0.196039, 52.71%)\n",
      "  [N=172] Cumulative Score: 5.501075 | Total Improvements: 29\n",
      "\n",
      "--- Processing N=171 ---\n",
      "  [N=171] Launching 10 parallel tasks...\n",
      "  [N=172] Best candidate score: 0.175872\n",
      "  [N=172] Baseline score: 0.371911, Candidate score: 0.175872\n",
      "  [N=172] ✓ IMPROVED over baseline: 0.371911 -> 0.175872 (Δ=0.196039, 52.71%)\n",
      "  [N=172] Cumulative Score: 5.501075 | Total Improvements: 29\n",
      "\n",
      "--- Processing N=171 ---\n",
      "  [N=171] Launching 10 parallel tasks...\n",
      "  [N=171] Best candidate score: 0.176901\n",
      "  [N=171] Baseline score: 0.373548, Candidate score: 0.176901\n",
      "  [N=171] ✓ IMPROVED over baseline: 0.373548 -> 0.176901 (Δ=0.196648, 52.64%)\n",
      "  [N=171] Cumulative Score: 5.677976 | Total Improvements: 30\n",
      "\n",
      "--- Processing N=170 ---\n",
      "  [N=170] Launching 10 parallel tasks...\n",
      "  [N=171] Best candidate score: 0.176901\n",
      "  [N=171] Baseline score: 0.373548, Candidate score: 0.176901\n",
      "  [N=171] ✓ IMPROVED over baseline: 0.373548 -> 0.176901 (Δ=0.196648, 52.64%)\n",
      "  [N=171] Cumulative Score: 5.677976 | Total Improvements: 30\n",
      "\n",
      "--- Processing N=170 ---\n",
      "  [N=170] Launching 10 parallel tasks...\n",
      "  [N=170] Best candidate score: 0.177941\n",
      "  [N=170] Baseline score: 0.366362, Candidate score: 0.177941\n",
      "  [N=170] ✓ IMPROVED over baseline: 0.366362 -> 0.177941 (Δ=0.188421, 51.43%)\n",
      "⚠️  Collision at N=170 between trees 0 and 1 (overlap area=6.085227e-02)\n",
      "  [N=170] Found 720 collision(s)\n",
      "  [N=170] Too many collisions - reverting to baseline\n",
      "  [N=170] Cumulative Score: 6.044338 | Total Improvements: 31\n",
      "  [N=170] Best candidate score: 0.177941\n",
      "  [N=170] Baseline score: 0.366362, Candidate score: 0.177941\n",
      "  [N=170] ✓ IMPROVED over baseline: 0.366362 -> 0.177941 (Δ=0.188421, 51.43%)\n",
      "⚠️  Collision at N=170 between trees 0 and 1 (overlap area=6.085227e-02)\n",
      "  [N=170] Found 720 collision(s)\n",
      "  [N=170] Too many collisions - reverting to baseline\n",
      "  [N=170] Cumulative Score: 6.044338 | Total Improvements: 31\n",
      "\n",
      "--- Processing N=169 ---\n",
      "\n",
      "--- Processing N=169 ---\n",
      "  [N=169] Launching 10 parallel tasks...\n",
      "  [N=169] Launching 10 parallel tasks...\n",
      "  [N=169] Best candidate score: 0.178994\n",
      "  [N=169] Baseline score: 0.366912, Candidate score: 0.178994\n",
      "  [N=169] ✓ IMPROVED over baseline: 0.366912 -> 0.178994 (Δ=0.187917, 51.22%)\n",
      "  [N=169] Cumulative Score: 6.223332 | Total Improvements: 32\n",
      "\n",
      "--- Processing N=168 ---\n",
      "  [N=168] Launching 10 parallel tasks...\n",
      "  [N=169] Best candidate score: 0.178994\n",
      "  [N=169] Baseline score: 0.366912, Candidate score: 0.178994\n",
      "  [N=169] ✓ IMPROVED over baseline: 0.366912 -> 0.178994 (Δ=0.187917, 51.22%)\n",
      "  [N=169] Cumulative Score: 6.223332 | Total Improvements: 32\n",
      "\n",
      "--- Processing N=168 ---\n",
      "  [N=168] Launching 10 parallel tasks...\n",
      "  [N=168] Best candidate score: 0.180060\n",
      "  [N=168] Baseline score: 0.369093, Candidate score: 0.180060\n",
      "  [N=168] ✓ IMPROVED over baseline: 0.369093 -> 0.180060 (Δ=0.189034, 51.22%)\n",
      "  [N=168] Cumulative Score: 6.403392 | Total Improvements: 33\n",
      "\n",
      "--- Processing N=167 ---\n",
      "  [N=167] Launching 10 parallel tasks...\n",
      "  [N=168] Best candidate score: 0.180060\n",
      "  [N=168] Baseline score: 0.369093, Candidate score: 0.180060\n",
      "  [N=168] ✓ IMPROVED over baseline: 0.369093 -> 0.180060 (Δ=0.189034, 51.22%)\n",
      "  [N=168] Cumulative Score: 6.403392 | Total Improvements: 33\n",
      "\n",
      "--- Processing N=167 ---\n",
      "  [N=167] Launching 10 parallel tasks...\n",
      "  [N=167] Best candidate score: 0.181138\n",
      "  [N=167] Baseline score: 0.368284, Candidate score: 0.181138\n",
      "  [N=167] ✓ IMPROVED over baseline: 0.368284 -> 0.181138 (Δ=0.187146, 50.82%)\n",
      "  [N=167] Cumulative Score: 6.584530 | Total Improvements: 34\n",
      "\n",
      "--- Processing N=166 ---\n",
      "  [N=166] Launching 10 parallel tasks...\n",
      "  [N=167] Best candidate score: 0.181138\n",
      "  [N=167] Baseline score: 0.368284, Candidate score: 0.181138\n",
      "  [N=167] ✓ IMPROVED over baseline: 0.368284 -> 0.181138 (Δ=0.187146, 50.82%)\n",
      "  [N=167] Cumulative Score: 6.584530 | Total Improvements: 34\n",
      "\n",
      "--- Processing N=166 ---\n",
      "  [N=166] Launching 10 parallel tasks...\n",
      "  [N=166] Best candidate score: 0.182229\n",
      "  [N=166] Baseline score: 0.366886, Candidate score: 0.182229\n",
      "  [N=166] ✓ IMPROVED over baseline: 0.366886 -> 0.182229 (Δ=0.184657, 50.33%)\n",
      "  [N=166] Cumulative Score: 6.766759 | Total Improvements: 35\n",
      "\n",
      "--- Processing N=165 ---\n",
      "  [N=165] Launching 10 parallel tasks...\n",
      "  [N=166] Best candidate score: 0.182229\n",
      "  [N=166] Baseline score: 0.366886, Candidate score: 0.182229\n",
      "  [N=166] ✓ IMPROVED over baseline: 0.366886 -> 0.182229 (Δ=0.184657, 50.33%)\n",
      "  [N=166] Cumulative Score: 6.766759 | Total Improvements: 35\n",
      "\n",
      "--- Processing N=165 ---\n",
      "  [N=165] Launching 10 parallel tasks...\n",
      "  [N=165] Best candidate score: 0.176276\n",
      "  [N=165] Baseline score: 0.369093, Candidate score: 0.176276\n",
      "  [N=165] ✓ IMPROVED over baseline: 0.369093 -> 0.176276 (Δ=0.192817, 52.24%)\n",
      "  [N=165] Cumulative Score: 6.943035 | Total Improvements: 36\n",
      "  [N=165] Best candidate score: 0.176276\n",
      "  [N=165] Baseline score: 0.369093, Candidate score: 0.176276\n",
      "  [N=165] ✓ IMPROVED over baseline: 0.369093 -> 0.176276 (Δ=0.192817, 52.24%)\n",
      "  [N=165] Cumulative Score: 6.943035 | Total Improvements: 36\n",
      "\n",
      "--- Processing N=164 ---\n",
      "  [N=164] Launching 15 parallel tasks...\n",
      "\n",
      "--- Processing N=164 ---\n",
      "  [N=164] Launching 15 parallel tasks...\n",
      "  [N=164] Best candidate score: 0.157046\n",
      "  [N=164] Baseline score: 0.366211, Candidate score: 0.157046\n",
      "  [N=164] ✓ IMPROVED over baseline: 0.366211 -> 0.157046 (Δ=0.209165, 57.12%)\n",
      "  [N=164] Cumulative Score: 7.100081 | Total Improvements: 37\n",
      "\n",
      "--- Processing N=163 ---\n",
      "  [N=163] Launching 30 parallel tasks...\n",
      "  [N=164] Best candidate score: 0.157046\n",
      "  [N=164] Baseline score: 0.366211, Candidate score: 0.157046\n",
      "  [N=164] ✓ IMPROVED over baseline: 0.366211 -> 0.157046 (Δ=0.209165, 57.12%)\n",
      "  [N=164] Cumulative Score: 7.100081 | Total Improvements: 37\n",
      "\n",
      "--- Processing N=163 ---\n",
      "  [N=163] Launching 30 parallel tasks...\n",
      "  [N=163] Best candidate score: 0.158010\n",
      "  [N=163] Baseline score: 0.368311, Candidate score: 0.158010\n",
      "  [N=163] ✓ IMPROVED over baseline: 0.368311 -> 0.158010 (Δ=0.210301, 57.10%)\n",
      "  [N=163] Cumulative Score: 7.258091 | Total Improvements: 38\n",
      "\n",
      "--- Processing N=162 ---\n",
      "  [N=163] Best candidate score: 0.158010\n",
      "  [N=163] Baseline score: 0.368311, Candidate score: 0.158010\n",
      "  [N=163] ✓ IMPROVED over baseline: 0.368311 -> 0.158010 (Δ=0.210301, 57.10%)\n",
      "  [N=163] Cumulative Score: 7.258091 | Total Improvements: 38\n",
      "\n",
      "--- Processing N=162 ---\n",
      "  [N=162] Launching 45 parallel tasks...\n",
      "  [N=162] Launching 45 parallel tasks...\n",
      "  [N=162] Best candidate score: 0.158985\n",
      "  [N=162] Baseline score: 0.367127, Candidate score: 0.158985\n",
      "  [N=162] ✓ IMPROVED over baseline: 0.367127 -> 0.158985 (Δ=0.208142, 56.69%)\n",
      "  [N=162] Cumulative Score: 7.417076 | Total Improvements: 39\n",
      "\n",
      "--- Processing N=161 ---\n",
      "  [N=162] Best candidate score: 0.158985\n",
      "  [N=162] Baseline score: 0.367127, Candidate score: 0.158985\n",
      "  [N=162] ✓ IMPROVED over baseline: 0.367127 -> 0.158985 (Δ=0.208142, 56.69%)\n",
      "  [N=162] Cumulative Score: 7.417076 | Total Improvements: 39\n",
      "\n",
      "--- Processing N=161 ---\n",
      "  [N=161] Launching 60 parallel tasks...\n",
      "  [N=161] Launching 60 parallel tasks...\n",
      "  [N=161] Best candidate score: 0.159973\n",
      "  [N=161] Baseline score: 0.369395, Candidate score: 0.159973\n",
      "  [N=161] ✓ IMPROVED over baseline: 0.369395 -> 0.159973 (Δ=0.209422, 56.69%)\n",
      "  [N=161] Cumulative Score: 7.577049 | Total Improvements: 40\n",
      "\n",
      "--- Processing N=160 ---\n",
      "  [N=161] Best candidate score: 0.159973\n",
      "  [N=161] Baseline score: 0.369395, Candidate score: 0.159973\n",
      "  [N=161] ✓ IMPROVED over baseline: 0.369395 -> 0.159973 (Δ=0.209422, 56.69%)\n",
      "  [N=161] Cumulative Score: 7.577049 | Total Improvements: 40\n",
      "\n",
      "--- Processing N=160 ---\n",
      "  [N=160] Launching 60 parallel tasks...\n",
      "  [N=160] Launching 60 parallel tasks...\n",
      "  [N=160] Best candidate score: 0.160973\n",
      "  [N=160] Baseline score: 0.367973, Candidate score: 0.160973\n",
      "  [N=160] ✓ IMPROVED over baseline: 0.367973 -> 0.160973 (Δ=0.207000, 56.25%)\n",
      "⚠️  Collision at N=160 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=160] Found 682 collision(s)\n",
      "  [N=160] Too many collisions - reverting to baseline\n",
      "  [N=160] Cumulative Score: 7.945022 | Total Improvements: 41\n",
      "  [N=160] Best candidate score: 0.160973\n",
      "  [N=160] Baseline score: 0.367973, Candidate score: 0.160973\n",
      "  [N=160] ✓ IMPROVED over baseline: 0.367973 -> 0.160973 (Δ=0.207000, 56.25%)\n",
      "⚠️  Collision at N=160 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=160] Found 682 collision(s)\n",
      "  [N=160] Too many collisions - reverting to baseline\n",
      "  [N=160] Cumulative Score: 7.945022 | Total Improvements: 41\n",
      "\n",
      "--- Processing N=159 ---\n",
      "  [N=159] Launching 60 parallel tasks...\n",
      "\n",
      "--- Processing N=159 ---\n",
      "  [N=159] Launching 60 parallel tasks...\n",
      "  [N=159] Best candidate score: 0.161985\n",
      "  [N=159] Baseline score: 0.370130, Candidate score: 0.161985\n",
      "  [N=159] ✓ IMPROVED over baseline: 0.370130 -> 0.161985 (Δ=0.208145, 56.24%)\n",
      "  [N=159] Cumulative Score: 8.107008 | Total Improvements: 42\n",
      "\n",
      "--- Processing N=158 ---\n",
      "  [N=159] Best candidate score: 0.161985\n",
      "  [N=159] Baseline score: 0.370130, Candidate score: 0.161985\n",
      "  [N=159] ✓ IMPROVED over baseline: 0.370130 -> 0.161985 (Δ=0.208145, 56.24%)\n",
      "  [N=159] Cumulative Score: 8.107008 | Total Improvements: 42\n",
      "\n",
      "--- Processing N=158 ---\n",
      "  [N=158] Launching 60 parallel tasks...\n",
      "  [N=158] Launching 60 parallel tasks...\n",
      "  [N=158] Best candidate score: 0.163010\n",
      "  [N=158] Baseline score: 0.368406, Candidate score: 0.163010\n",
      "  [N=158] ✓ IMPROVED over baseline: 0.368406 -> 0.163010 (Δ=0.205396, 55.75%)\n",
      "  [N=158] Cumulative Score: 8.270018 | Total Improvements: 43\n",
      "\n",
      "--- Processing N=157 ---\n",
      "  [N=158] Best candidate score: 0.163010\n",
      "  [N=158] Baseline score: 0.368406, Candidate score: 0.163010\n",
      "  [N=158] ✓ IMPROVED over baseline: 0.368406 -> 0.163010 (Δ=0.205396, 55.75%)\n",
      "  [N=158] Cumulative Score: 8.270018 | Total Improvements: 43\n",
      "\n",
      "--- Processing N=157 ---\n",
      "  [N=157] Launching 60 parallel tasks...\n",
      "  [N=157] Launching 60 parallel tasks...\n",
      "  [N=157] Best candidate score: 0.156067\n",
      "  [N=157] Baseline score: 0.370599, Candidate score: 0.156067\n",
      "  [N=157] ✓ IMPROVED over baseline: 0.370599 -> 0.156067 (Δ=0.214532, 57.89%)\n",
      "  [N=157] Cumulative Score: 8.426085 | Total Improvements: 44\n",
      "\n",
      "--- Processing N=156 ---\n",
      "  [N=157] Best candidate score: 0.156067\n",
      "  [N=157] Baseline score: 0.370599, Candidate score: 0.156067\n",
      "  [N=157] ✓ IMPROVED over baseline: 0.370599 -> 0.156067 (Δ=0.214532, 57.89%)\n",
      "  [N=157] Cumulative Score: 8.426085 | Total Improvements: 44\n",
      "\n",
      "--- Processing N=156 ---\n",
      "  [N=156] Launching 60 parallel tasks...\n",
      "  [N=156] Launching 60 parallel tasks...\n",
      "  [N=156] Best candidate score: 0.157067\n",
      "  [N=156] Baseline score: 0.363654, Candidate score: 0.157067\n",
      "  [N=156] ✓ IMPROVED over baseline: 0.363654 -> 0.157067 (Δ=0.206587, 56.81%)\n",
      "  [N=156] Cumulative Score: 8.583152 | Total Improvements: 45\n",
      "\n",
      "--- Processing N=155 ---\n",
      "  [N=156] Best candidate score: 0.157067\n",
      "  [N=156] Baseline score: 0.363654, Candidate score: 0.157067\n",
      "  [N=156] ✓ IMPROVED over baseline: 0.363654 -> 0.157067 (Δ=0.206587, 56.81%)\n",
      "  [N=156] Cumulative Score: 8.583152 | Total Improvements: 45\n",
      "\n",
      "--- Processing N=155 ---\n",
      "  [N=155] Launching 60 parallel tasks...\n",
      "  [N=155] Launching 60 parallel tasks...\n",
      "  [N=155] Best candidate score: 0.158081\n",
      "  [N=155] Baseline score: 0.365998, Candidate score: 0.158081\n",
      "  [N=155] ✓ IMPROVED over baseline: 0.365998 -> 0.158081 (Δ=0.207918, 56.81%)\n",
      "  [N=155] Cumulative Score: 8.741233 | Total Improvements: 46\n",
      "  [N=155] Best candidate score: 0.158081\n",
      "  [N=155] Baseline score: 0.365998, Candidate score: 0.158081\n",
      "  [N=155] ✓ IMPROVED over baseline: 0.365998 -> 0.158081 (Δ=0.207918, 56.81%)\n",
      "  [N=155] Cumulative Score: 8.741233 | Total Improvements: 46\n",
      "\n",
      "--- Processing N=154 ---\n",
      "\n",
      "--- Processing N=154 ---\n",
      "  [N=154] Launching 60 parallel tasks...\n",
      "  [N=154] Launching 60 parallel tasks...\n",
      "  [N=154] Best candidate score: 0.159107\n",
      "  [N=154] Baseline score: 0.368375, Candidate score: 0.159107\n",
      "  [N=154] ✓ IMPROVED over baseline: 0.368375 -> 0.159107 (Δ=0.209267, 56.81%)\n",
      "  [N=154] Cumulative Score: 8.900340 | Total Improvements: 47\n",
      "\n",
      "--- Processing N=153 ---\n",
      "  [N=154] Best candidate score: 0.159107\n",
      "  [N=154] Baseline score: 0.368375, Candidate score: 0.159107\n",
      "  [N=154] ✓ IMPROVED over baseline: 0.368375 -> 0.159107 (Δ=0.209267, 56.81%)\n",
      "  [N=154] Cumulative Score: 8.900340 | Total Improvements: 47\n",
      "\n",
      "--- Processing N=153 ---\n",
      "  [N=153] Launching 60 parallel tasks...\n",
      "  [N=153] Launching 60 parallel tasks...\n",
      "  [N=153] Best candidate score: 0.160147\n",
      "  [N=153] Baseline score: 0.367545, Candidate score: 0.160147\n",
      "  [N=153] ✓ IMPROVED over baseline: 0.367545 -> 0.160147 (Δ=0.207398, 56.43%)\n",
      "  [N=153] Cumulative Score: 9.060487 | Total Improvements: 48\n",
      "\n",
      "--- Processing N=152 ---\n",
      "  [N=153] Best candidate score: 0.160147\n",
      "  [N=153] Baseline score: 0.367545, Candidate score: 0.160147\n",
      "  [N=153] ✓ IMPROVED over baseline: 0.367545 -> 0.160147 (Δ=0.207398, 56.43%)\n",
      "  [N=153] Cumulative Score: 9.060487 | Total Improvements: 48\n",
      "\n",
      "--- Processing N=152 ---\n",
      "  [N=152] Launching 60 parallel tasks...\n",
      "  [N=152] Launching 60 parallel tasks...\n",
      "  [N=152] Best candidate score: 0.161201\n",
      "  [N=152] Baseline score: 0.369944, Candidate score: 0.161201\n",
      "  [N=152] ✓ IMPROVED over baseline: 0.369944 -> 0.161201 (Δ=0.208743, 56.43%)\n",
      "  [N=152] Cumulative Score: 9.221687 | Total Improvements: 49\n",
      "\n",
      "--- Processing N=151 ---\n",
      "  [N=152] Best candidate score: 0.161201\n",
      "  [N=152] Baseline score: 0.369944, Candidate score: 0.161201\n",
      "  [N=152] ✓ IMPROVED over baseline: 0.369944 -> 0.161201 (Δ=0.208743, 56.43%)\n",
      "  [N=152] Cumulative Score: 9.221687 | Total Improvements: 49\n",
      "\n",
      "--- Processing N=151 ---\n",
      "  [N=151] Launching 60 parallel tasks...\n",
      "  [N=151] Launching 60 parallel tasks...\n",
      "  [N=151] Best candidate score: 0.162268\n",
      "  [N=151] Baseline score: 0.368177, Candidate score: 0.162268\n",
      "  [N=151] ✓ IMPROVED over baseline: 0.368177 -> 0.162268 (Δ=0.205909, 55.93%)\n",
      "  [N=151] Cumulative Score: 9.383956 | Total Improvements: 50\n",
      "\n",
      "--- Processing N=150 ---\n",
      "  [N=151] Best candidate score: 0.162268\n",
      "  [N=151] Baseline score: 0.368177, Candidate score: 0.162268\n",
      "  [N=151] ✓ IMPROVED over baseline: 0.368177 -> 0.162268 (Δ=0.205909, 55.93%)\n",
      "  [N=151] Cumulative Score: 9.383956 | Total Improvements: 50\n",
      "\n",
      "--- Processing N=150 ---\n",
      "  [N=150] Launching 60 parallel tasks...\n",
      "  [N=150] Launching 60 parallel tasks...\n",
      "  [N=150] Best candidate score: 0.155204\n",
      "  [N=150] Baseline score: 0.368737, Candidate score: 0.155204\n",
      "  [N=150] ✓ IMPROVED over baseline: 0.368737 -> 0.155204 (Δ=0.213533, 57.91%)\n",
      "⚠️  Collision at N=150 between trees 0 and 17 (overlap area=9.388889e-03)\n",
      "  [N=150] Found 636 collision(s)\n",
      "  [N=150] Too many collisions - reverting to baseline\n",
      "  [N=150] Cumulative Score: 9.752692 | Total Improvements: 51\n",
      "  [N=150] Best candidate score: 0.155204\n",
      "  [N=150] Baseline score: 0.368737, Candidate score: 0.155204\n",
      "  [N=150] ✓ IMPROVED over baseline: 0.368737 -> 0.155204 (Δ=0.213533, 57.91%)\n",
      "⚠️  Collision at N=150 between trees 0 and 17 (overlap area=9.388889e-03)\n",
      "  [N=150] Found 636 collision(s)\n",
      "  [N=150] Too many collisions - reverting to baseline\n",
      "  [N=150] Cumulative Score: 9.752692 | Total Improvements: 51\n",
      "\n",
      "--- Processing N=149 ---\n",
      "\n",
      "--- Processing N=149 ---\n",
      "  [N=149] Launching 60 parallel tasks...\n",
      "  [N=149] Launching 60 parallel tasks...\n",
      "  [N=149] Best candidate score: 0.156246\n",
      "  [N=149] Baseline score: 0.371101, Candidate score: 0.156246\n",
      "  [N=149] ✓ IMPROVED over baseline: 0.371101 -> 0.156246 (Δ=0.214856, 57.90%)\n",
      "  [N=149] Cumulative Score: 9.908938 | Total Improvements: 52\n",
      "\n",
      "--- Processing N=148 ---\n",
      "  [N=149] Best candidate score: 0.156246\n",
      "  [N=149] Baseline score: 0.371101, Candidate score: 0.156246\n",
      "  [N=149] ✓ IMPROVED over baseline: 0.371101 -> 0.156246 (Δ=0.214856, 57.90%)\n",
      "  [N=149] Cumulative Score: 9.908938 | Total Improvements: 52\n",
      "\n",
      "--- Processing N=148 ---\n",
      "  [N=148] Launching 60 parallel tasks...\n",
      "  [N=148] Launching 60 parallel tasks...\n",
      "  [N=148] Best candidate score: 0.157302\n",
      "  [N=148] Baseline score: 0.367239, Candidate score: 0.157302\n",
      "  [N=148] ✓ IMPROVED over baseline: 0.367239 -> 0.157302 (Δ=0.209938, 57.17%)\n",
      "  [N=148] Cumulative Score: 10.066240 | Total Improvements: 53\n",
      "\n",
      "--- Processing N=147 ---\n",
      "  [N=148] Best candidate score: 0.157302\n",
      "  [N=148] Baseline score: 0.367239, Candidate score: 0.157302\n",
      "  [N=148] ✓ IMPROVED over baseline: 0.367239 -> 0.157302 (Δ=0.209938, 57.17%)\n",
      "  [N=148] Cumulative Score: 10.066240 | Total Improvements: 53\n",
      "\n",
      "--- Processing N=147 ---\n",
      "  [N=147] Launching 60 parallel tasks...\n",
      "  [N=147] Launching 60 parallel tasks...\n",
      "  [N=147] Best candidate score: 0.158372\n",
      "  [N=147] Baseline score: 0.368206, Candidate score: 0.158372\n",
      "  [N=147] ✓ IMPROVED over baseline: 0.368206 -> 0.158372 (Δ=0.209835, 56.99%)\n",
      "  [N=147] Cumulative Score: 10.224611 | Total Improvements: 54\n",
      "\n",
      "--- Processing N=146 ---\n",
      "  [N=147] Best candidate score: 0.158372\n",
      "  [N=147] Baseline score: 0.368206, Candidate score: 0.158372\n",
      "  [N=147] ✓ IMPROVED over baseline: 0.368206 -> 0.158372 (Δ=0.209835, 56.99%)\n",
      "  [N=147] Cumulative Score: 10.224611 | Total Improvements: 54\n",
      "\n",
      "--- Processing N=146 ---\n",
      "  [N=146] Launching 60 parallel tasks...\n",
      "  [N=146] Launching 60 parallel tasks...\n",
      "  [N=146] Best candidate score: 0.159456\n",
      "  [N=146] Baseline score: 0.369421, Candidate score: 0.159456\n",
      "  [N=146] ✓ IMPROVED over baseline: 0.369421 -> 0.159456 (Δ=0.209965, 56.84%)\n",
      "  [N=146] Cumulative Score: 10.384068 | Total Improvements: 55\n",
      "\n",
      "--- Processing N=145 ---\n",
      "  [N=145] Launching 60 parallel tasks...\n",
      "  [N=145] Best candidate score: 0.160556\n",
      "  [N=145] Baseline score: 0.367644, Candidate score: 0.160556\n",
      "  [N=145] ✓ IMPROVED over baseline: 0.367644 -> 0.160556 (Δ=0.207088, 56.33%)\n",
      "  [N=145] Cumulative Score: 10.544624 | Total Improvements: 56\n",
      "\n",
      "--- Processing N=144 ---\n",
      "  [N=144] Launching 60 parallel tasks...\n",
      "  [N=144] Best candidate score: 0.161671\n",
      "  [N=144] Baseline score: 0.365271, Candidate score: 0.161671\n",
      "  [N=144] ✓ IMPROVED over baseline: 0.365271 -> 0.161671 (Δ=0.203600, 55.74%)\n",
      "  [N=144] Cumulative Score: 10.706295 | Total Improvements: 57\n",
      "\n",
      "--- Processing N=143 ---\n",
      "  [N=143] Launching 60 parallel tasks...\n",
      "  [N=143] Best candidate score: 0.162802\n",
      "  [N=143] Baseline score: 0.367826, Candidate score: 0.162802\n",
      "  [N=143] ✓ IMPROVED over baseline: 0.367826 -> 0.162802 (Δ=0.205024, 55.74%)\n",
      "  [N=143] Cumulative Score: 10.869096 | Total Improvements: 58\n",
      "\n",
      "--- Processing N=142 ---\n",
      "  [N=142] Launching 60 parallel tasks...\n",
      "  [N=142] Best candidate score: 0.163948\n",
      "  [N=142] Baseline score: 0.368778, Candidate score: 0.163948\n",
      "  [N=142] ✓ IMPROVED over baseline: 0.368778 -> 0.163948 (Δ=0.204830, 55.54%)\n",
      "  [N=142] Cumulative Score: 11.033044 | Total Improvements: 59\n",
      "\n",
      "--- Processing N=141 ---\n",
      "  [N=141] Launching 60 parallel tasks...\n",
      "  [N=141] Best candidate score: 0.165111\n",
      "  [N=141] Baseline score: 0.371393, Candidate score: 0.165111\n",
      "  [N=141] ✓ IMPROVED over baseline: 0.371393 -> 0.165111 (Δ=0.206282, 55.54%)\n",
      "  [N=141] Cumulative Score: 11.198155 | Total Improvements: 60\n",
      "\n",
      "--- Processing N=140 ---\n",
      "  [N=140] Launching 60 parallel tasks...\n",
      "  [N=140] Best candidate score: 0.166290\n",
      "  [N=140] Baseline score: 0.369065, Candidate score: 0.166290\n",
      "  [N=140] ✓ IMPROVED over baseline: 0.369065 -> 0.166290 (Δ=0.202775, 54.94%)\n",
      "⚠️  Collision at N=140 between trees 0 and 1 (overlap area=6.085227e-02)\n",
      "  [N=140] Found 568 collision(s)\n",
      "  [N=140] Too many collisions - reverting to baseline\n",
      "  [N=140] Cumulative Score: 11.567221 | Total Improvements: 61\n",
      "\n",
      "--- Processing N=139 ---\n",
      "  [N=139] Launching 60 parallel tasks...\n",
      "  [N=139] Best candidate score: 0.167487\n",
      "  [N=139] Baseline score: 0.371678, Candidate score: 0.167487\n",
      "  [N=139] ✓ IMPROVED over baseline: 0.371678 -> 0.167487 (Δ=0.204191, 54.94%)\n",
      "  [N=139] Cumulative Score: 11.734707 | Total Improvements: 62\n",
      "\n",
      "--- Processing N=138 ---\n",
      "  [N=138] Launching 60 parallel tasks...\n",
      "  [N=138] Best candidate score: 0.168700\n",
      "  [N=138] Baseline score: 0.369761, Candidate score: 0.168700\n",
      "  [N=138] ✓ IMPROVED over baseline: 0.369761 -> 0.168700 (Δ=0.201061, 54.38%)\n",
      "  [N=138] Cumulative Score: 11.903407 | Total Improvements: 63\n",
      "\n",
      "--- Processing N=137 ---\n",
      "  [N=137] Launching 60 parallel tasks...\n",
      "  [N=137] Best candidate score: 0.169932\n",
      "  [N=137] Baseline score: 0.370279, Candidate score: 0.169932\n",
      "  [N=137] ✓ IMPROVED over baseline: 0.370279 -> 0.169932 (Δ=0.200347, 54.11%)\n",
      "  [N=137] Cumulative Score: 12.073339 | Total Improvements: 64\n",
      "\n",
      "--- Processing N=136 ---\n",
      "  [N=136] Launching 60 parallel tasks...\n",
      "  [N=136] Best candidate score: 0.171181\n",
      "  [N=136] Baseline score: 0.368445, Candidate score: 0.171181\n",
      "  [N=136] ✓ IMPROVED over baseline: 0.368445 -> 0.171181 (Δ=0.197264, 53.54%)\n",
      "  [N=136] Cumulative Score: 12.244520 | Total Improvements: 65\n",
      "\n",
      "--- Processing N=135 ---\n",
      "  [N=135] Launching 60 parallel tasks...\n",
      "  [N=135] Best candidate score: 0.172449\n",
      "  [N=135] Baseline score: 0.371113, Candidate score: 0.172449\n",
      "  [N=135] ✓ IMPROVED over baseline: 0.371113 -> 0.172449 (Δ=0.198663, 53.53%)\n",
      "  [N=135] Cumulative Score: 12.416969 | Total Improvements: 66\n",
      "\n",
      "--- Processing N=134 ---\n",
      "  [N=134] Launching 60 parallel tasks...\n",
      "  [N=134] Best candidate score: 0.173736\n",
      "  [N=134] Baseline score: 0.368122, Candidate score: 0.173736\n",
      "  [N=134] ✓ IMPROVED over baseline: 0.368122 -> 0.173736 (Δ=0.194386, 52.80%)\n",
      "  [N=134] Cumulative Score: 12.590705 | Total Improvements: 67\n",
      "\n",
      "--- Processing N=133 ---\n",
      "  [N=133] Launching 60 parallel tasks...\n",
      "  [N=133] Best candidate score: 0.175042\n",
      "  [N=133] Baseline score: 0.368775, Candidate score: 0.175042\n",
      "  [N=133] ✓ IMPROVED over baseline: 0.368775 -> 0.175042 (Δ=0.193732, 52.53%)\n",
      "  [N=133] Cumulative Score: 12.765747 | Total Improvements: 68\n",
      "\n",
      "--- Processing N=132 ---\n",
      "  [N=132] Launching 60 parallel tasks...\n",
      "  [N=132] Best candidate score: 0.176368\n",
      "  [N=132] Baseline score: 0.370278, Candidate score: 0.176368\n",
      "  [N=132] ✓ IMPROVED over baseline: 0.370278 -> 0.176368 (Δ=0.193910, 52.37%)\n",
      "  [N=132] Cumulative Score: 12.942116 | Total Improvements: 69\n",
      "\n",
      "--- Processing N=131 ---\n",
      "  [N=131] Launching 60 parallel tasks...\n",
      "  [N=131] Best candidate score: 0.177715\n",
      "  [N=131] Baseline score: 0.369822, Candidate score: 0.177715\n",
      "  [N=131] ✓ IMPROVED over baseline: 0.369822 -> 0.177715 (Δ=0.192107, 51.95%)\n",
      "  [N=131] Cumulative Score: 13.119830 | Total Improvements: 70\n",
      "\n",
      "--- Processing N=130 ---\n",
      "  [N=130] Launching 60 parallel tasks...\n",
      "  [N=130] Best candidate score: 0.173558\n",
      "  [N=130] Baseline score: 0.366434, Candidate score: 0.173558\n",
      "  [N=130] ✓ IMPROVED over baseline: 0.366434 -> 0.173558 (Δ=0.192877, 52.64%)\n",
      "⚠️  Collision at N=130 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=130] Found 513 collision(s)\n",
      "  [N=130] Too many collisions - reverting to baseline\n",
      "  [N=130] Cumulative Score: 13.486265 | Total Improvements: 71\n",
      "\n",
      "--- Processing N=129 ---\n",
      "  [N=129] Launching 60 parallel tasks...\n",
      "  [N=129] Best candidate score: 0.164031\n",
      "  [N=129] Baseline score: 0.369074, Candidate score: 0.164031\n",
      "  [N=129] ✓ IMPROVED over baseline: 0.369074 -> 0.164031 (Δ=0.205043, 55.56%)\n",
      "  [N=129] Cumulative Score: 13.650296 | Total Improvements: 72\n",
      "\n",
      "--- Processing N=128 ---\n",
      "  [N=128] Launching 60 parallel tasks...\n",
      "  [N=128] Best candidate score: 0.165312\n",
      "  [N=128] Baseline score: 0.367748, Candidate score: 0.165312\n",
      "  [N=128] ✓ IMPROVED over baseline: 0.367748 -> 0.165312 (Δ=0.202436, 55.05%)\n",
      "  [N=128] Cumulative Score: 13.815608 | Total Improvements: 73\n",
      "\n",
      "--- Processing N=127 ---\n",
      "  [N=127] Launching 60 parallel tasks...\n",
      "  [N=127] Best candidate score: 0.166614\n",
      "  [N=127] Baseline score: 0.369306, Candidate score: 0.166614\n",
      "  [N=127] ✓ IMPROVED over baseline: 0.369306 -> 0.166614 (Δ=0.202692, 54.88%)\n",
      "  [N=127] Cumulative Score: 13.982222 | Total Improvements: 74\n",
      "\n",
      "--- Processing N=126 ---\n",
      "  [N=126] Launching 60 parallel tasks...\n",
      "  [N=126] Best candidate score: 0.167937\n",
      "  [N=126] Baseline score: 0.370716, Candidate score: 0.167937\n",
      "  [N=126] ✓ IMPROVED over baseline: 0.370716 -> 0.167937 (Δ=0.202779, 54.70%)\n",
      "  [N=126] Cumulative Score: 14.150159 | Total Improvements: 75\n",
      "\n",
      "--- Processing N=125 ---\n",
      "  [N=125] Launching 60 parallel tasks...\n",
      "  [N=125] Best candidate score: 0.169280\n",
      "  [N=125] Baseline score: 0.370532, Candidate score: 0.169280\n",
      "  [N=125] ✓ IMPROVED over baseline: 0.370532 -> 0.169280 (Δ=0.201252, 54.31%)\n",
      "  [N=125] Cumulative Score: 14.319439 | Total Improvements: 76\n",
      "\n",
      "--- Processing N=124 ---\n",
      "  [N=124] Launching 60 parallel tasks...\n",
      "  [N=124] Best candidate score: 0.170645\n",
      "  [N=124] Baseline score: 0.370763, Candidate score: 0.170645\n",
      "  [N=124] ✓ IMPROVED over baseline: 0.370763 -> 0.170645 (Δ=0.200117, 53.97%)\n",
      "  [N=124] Cumulative Score: 14.490084 | Total Improvements: 77\n",
      "\n",
      "--- Processing N=123 ---\n",
      "  [N=123] Launching 60 parallel tasks...\n",
      "  [N=123] Best candidate score: 0.172033\n",
      "  [N=123] Baseline score: 0.369954, Candidate score: 0.172033\n",
      "  [N=123] ✓ IMPROVED over baseline: 0.369954 -> 0.172033 (Δ=0.197921, 53.50%)\n",
      "  [N=123] Cumulative Score: 14.662117 | Total Improvements: 78\n",
      "\n",
      "--- Processing N=122 ---\n",
      "  [N=122] Launching 60 parallel tasks...\n",
      "  [N=122] Best candidate score: 0.173443\n",
      "  [N=122] Baseline score: 0.369879, Candidate score: 0.173443\n",
      "  [N=122] ✓ IMPROVED over baseline: 0.369879 -> 0.173443 (Δ=0.196436, 53.11%)\n",
      "  [N=122] Cumulative Score: 14.835559 | Total Improvements: 79\n",
      "\n",
      "--- Processing N=121 ---\n",
      "  [N=121] Launching 60 parallel tasks...\n",
      "  [N=121] Best candidate score: 0.174876\n",
      "  [N=121] Baseline score: 0.366734, Candidate score: 0.174876\n",
      "  [N=121] ✓ IMPROVED over baseline: 0.366734 -> 0.174876 (Δ=0.191858, 52.32%)\n",
      "  [N=121] Cumulative Score: 15.010435 | Total Improvements: 80\n",
      "\n",
      "--- Processing N=120 ---\n",
      "  [N=120] Launching 60 parallel tasks...\n",
      "  [N=120] Best candidate score: 0.176333\n",
      "  [N=120] Baseline score: 0.369618, Candidate score: 0.176333\n",
      "  [N=120] ✓ IMPROVED over baseline: 0.369618 -> 0.176333 (Δ=0.193285, 52.29%)\n",
      "⚠️  Collision at N=120 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=120] Found 484 collision(s)\n",
      "  [N=120] Too many collisions - reverting to baseline\n",
      "  [N=120] Cumulative Score: 15.380053 | Total Improvements: 81\n",
      "\n",
      "--- Processing N=119 ---\n",
      "  [N=119] Launching 60 parallel tasks...\n",
      "  [N=119] Best candidate score: 0.177815\n",
      "  [N=119] Baseline score: 0.368490, Candidate score: 0.177815\n",
      "  [N=119] ✓ IMPROVED over baseline: 0.368490 -> 0.177815 (Δ=0.190675, 51.74%)\n",
      "  [N=119] Cumulative Score: 15.557868 | Total Improvements: 82\n",
      "\n",
      "--- Processing N=118 ---\n",
      "  [N=118] Launching 60 parallel tasks...\n",
      "  [N=118] Best candidate score: 0.179322\n",
      "  [N=118] Baseline score: 0.371545, Candidate score: 0.179322\n",
      "  [N=118] ✓ IMPROVED over baseline: 0.371545 -> 0.179322 (Δ=0.192223, 51.74%)\n",
      "  [N=118] Cumulative Score: 15.737190 | Total Improvements: 83\n",
      "\n",
      "--- Processing N=117 ---\n",
      "  [N=117] Launching 60 parallel tasks...\n",
      "  [N=117] Best candidate score: 0.180855\n",
      "  [N=117] Baseline score: 0.368081, Candidate score: 0.180855\n",
      "  [N=117] ✓ IMPROVED over baseline: 0.368081 -> 0.180855 (Δ=0.187226, 50.87%)\n",
      "  [N=117] Cumulative Score: 15.918045 | Total Improvements: 84\n",
      "\n",
      "--- Processing N=116 ---\n",
      "  [N=116] Launching 60 parallel tasks...\n",
      "  [N=116] Best candidate score: 0.182414\n",
      "  [N=116] Baseline score: 0.371058, Candidate score: 0.182414\n",
      "  [N=116] ✓ IMPROVED over baseline: 0.371058 -> 0.182414 (Δ=0.188645, 50.84%)\n",
      "  [N=116] Cumulative Score: 16.100459 | Total Improvements: 85\n",
      "\n",
      "--- Processing N=115 ---\n",
      "  [N=115] Launching 60 parallel tasks...\n",
      "  [N=115] Best candidate score: 0.184000\n",
      "  [N=115] Baseline score: 0.369207, Candidate score: 0.184000\n",
      "  [N=115] ✓ IMPROVED over baseline: 0.369207 -> 0.184000 (Δ=0.185207, 50.16%)\n",
      "  [N=115] Cumulative Score: 16.284459 | Total Improvements: 86\n",
      "\n",
      "--- Processing N=114 ---\n",
      "  [N=114] Launching 60 parallel tasks...\n",
      "  [N=114] Best candidate score: 0.183602\n",
      "  [N=114] Baseline score: 0.369700, Candidate score: 0.183602\n",
      "  [N=114] ✓ IMPROVED over baseline: 0.369700 -> 0.183602 (Δ=0.186098, 50.34%)\n",
      "  [N=114] Cumulative Score: 16.468061 | Total Improvements: 87\n",
      "\n",
      "--- Processing N=113 ---\n",
      "  [N=113] Launching 60 parallel tasks...\n",
      "  [N=113] Best candidate score: 0.185227\n",
      "  [N=113] Baseline score: 0.370132, Candidate score: 0.185227\n",
      "  [N=113] ✓ IMPROVED over baseline: 0.370132 -> 0.185227 (Δ=0.184905, 49.96%)\n",
      "  [N=113] Cumulative Score: 16.653287 | Total Improvements: 88\n",
      "\n",
      "--- Processing N=112 ---\n",
      "  [N=112] Launching 60 parallel tasks...\n",
      "  [N=112] Best candidate score: 0.186881\n",
      "  [N=112] Baseline score: 0.369095, Candidate score: 0.186881\n",
      "  [N=112] ✓ IMPROVED over baseline: 0.369095 -> 0.186881 (Δ=0.182215, 49.37%)\n",
      "  [N=112] Cumulative Score: 16.840168 | Total Improvements: 89\n",
      "\n",
      "--- Processing N=111 ---\n",
      "  [N=111] Launching 60 parallel tasks...\n",
      "  [N=111] Best candidate score: 0.188564\n",
      "  [N=111] Baseline score: 0.372323, Candidate score: 0.188564\n",
      "  [N=111] ✓ IMPROVED over baseline: 0.372323 -> 0.188564 (Δ=0.183759, 49.35%)\n",
      "  [N=111] Cumulative Score: 17.028732 | Total Improvements: 90\n",
      "\n",
      "--- Processing N=110 ---\n",
      "  [N=110] Launching 60 parallel tasks...\n",
      "  [N=110] Best candidate score: 0.180023\n",
      "  [N=110] Baseline score: 0.370013, Candidate score: 0.180023\n",
      "  [N=110] ✓ IMPROVED over baseline: 0.370013 -> 0.180023 (Δ=0.189991, 51.35%)\n",
      "⚠️  Collision at N=110 between trees 0 and 96 (overlap area=4.122242e-03)\n",
      "  [N=110] Found 451 collision(s)\n",
      "  [N=110] Too many collisions - reverting to baseline\n",
      "  [N=110] Cumulative Score: 17.398746 | Total Improvements: 91\n",
      "\n",
      "--- Processing N=109 ---\n",
      "  [N=109] Launching 60 parallel tasks...\n",
      "  [N=109] Best candidate score: 0.181674\n",
      "  [N=109] Baseline score: 0.368798, Candidate score: 0.181674\n",
      "  [N=109] ✓ IMPROVED over baseline: 0.368798 -> 0.181674 (Δ=0.187123, 50.74%)\n",
      "  [N=109] Cumulative Score: 17.580420 | Total Improvements: 92\n",
      "\n",
      "--- Processing N=108 ---\n",
      "  [N=108] Launching 60 parallel tasks...\n",
      "  [N=108] Best candidate score: 0.183356\n",
      "  [N=108] Baseline score: 0.372042, Candidate score: 0.183356\n",
      "  [N=108] ✓ IMPROVED over baseline: 0.372042 -> 0.183356 (Δ=0.188685, 50.72%)\n",
      "  [N=108] Cumulative Score: 17.763777 | Total Improvements: 93\n",
      "\n",
      "--- Processing N=107 ---\n",
      "  [N=107] Launching 60 parallel tasks...\n",
      "  [N=107] Best candidate score: 0.185070\n",
      "  [N=107] Baseline score: 0.367593, Candidate score: 0.185070\n",
      "  [N=107] ✓ IMPROVED over baseline: 0.367593 -> 0.185070 (Δ=0.182523, 49.65%)\n",
      "  [N=107] Cumulative Score: 17.948847 | Total Improvements: 94\n",
      "\n",
      "--- Processing N=106 ---\n",
      "  [N=106] Launching 60 parallel tasks...\n",
      "  [N=106] Best candidate score: 0.177529\n",
      "  [N=106] Baseline score: 0.370949, Candidate score: 0.177529\n",
      "  [N=106] ✓ IMPROVED over baseline: 0.370949 -> 0.177529 (Δ=0.193420, 52.14%)\n",
      "  [N=106] Cumulative Score: 18.126375 | Total Improvements: 95\n",
      "\n",
      "--- Processing N=105 ---\n",
      "  [N=105] Launching 60 parallel tasks...\n",
      "  [N=105] Best candidate score: 0.178149\n",
      "  [N=105] Baseline score: 0.366468, Candidate score: 0.178149\n",
      "  [N=105] ✓ IMPROVED over baseline: 0.366468 -> 0.178149 (Δ=0.188320, 51.39%)\n",
      "  [N=105] Cumulative Score: 18.304524 | Total Improvements: 96\n",
      "\n",
      "--- Processing N=104 ---\n",
      "  [N=104] Launching 60 parallel tasks...\n",
      "  [N=104] Best candidate score: 0.179862\n",
      "  [N=104] Baseline score: 0.369023, Candidate score: 0.179862\n",
      "  [N=104] ✓ IMPROVED over baseline: 0.369023 -> 0.179862 (Δ=0.189161, 51.26%)\n",
      "  [N=104] Cumulative Score: 18.484386 | Total Improvements: 97\n",
      "\n",
      "--- Processing N=103 ---\n",
      "  [N=103] Launching 60 parallel tasks...\n",
      "  [N=103] Best candidate score: 0.181608\n",
      "  [N=103] Baseline score: 0.370550, Candidate score: 0.181608\n",
      "  [N=103] ✓ IMPROVED over baseline: 0.370550 -> 0.181608 (Δ=0.188942, 50.99%)\n",
      "  [N=103] Cumulative Score: 18.665994 | Total Improvements: 98\n",
      "\n",
      "--- Processing N=102 ---\n",
      "  [N=102] Launching 60 parallel tasks...\n",
      "  [N=102] Best candidate score: 0.183388\n",
      "  [N=102] Baseline score: 0.370065, Candidate score: 0.183388\n",
      "  [N=102] ✓ IMPROVED over baseline: 0.370065 -> 0.183388 (Δ=0.186676, 50.44%)\n",
      "  [N=102] Cumulative Score: 18.849383 | Total Improvements: 99\n",
      "\n",
      "--- Processing N=101 ---\n",
      "  [N=101] Launching 60 parallel tasks...\n",
      "  [N=101] Best candidate score: 0.185204\n",
      "  [N=101] Baseline score: 0.367454, Candidate score: 0.185204\n",
      "  [N=101] ✓ IMPROVED over baseline: 0.367454 -> 0.185204 (Δ=0.182250, 49.60%)\n",
      "  [N=101] Cumulative Score: 19.034587 | Total Improvements: 100\n",
      "\n",
      "--- Processing N=100 ---\n",
      "  [N=100] Critical N! Widening beam to 24\n",
      "  [N=100] Launching 60 parallel tasks...\n",
      "  [N=100] Best candidate score: 0.187056\n",
      "  [N=100] Baseline score: 0.369147, Candidate score: 0.187056\n",
      "  [N=100] ✓ IMPROVED over baseline: 0.369147 -> 0.187056 (Δ=0.182090, 49.33%)\n",
      "⚠️  Collision at N=100 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=100] Found 402 collision(s)\n",
      "  [N=100] Too many collisions - reverting to baseline\n",
      "  [N=100] Cumulative Score: 19.403733 | Total Improvements: 101\n",
      "\n",
      "--- Processing N=99 ---\n",
      "  [N=99] Launching 60 parallel tasks...\n",
      "  [N=99] Best candidate score: 0.178182\n",
      "  [N=99] Baseline score: 0.367315, Candidate score: 0.178182\n",
      "  [N=99] ✓ IMPROVED over baseline: 0.367315 -> 0.178182 (Δ=0.189133, 51.49%)\n",
      "  [N=99] Cumulative Score: 19.581915 | Total Improvements: 102\n",
      "\n",
      "--- Processing N=98 ---\n",
      "  [N=98] Launching 60 parallel tasks...\n",
      "  [N=98] Best candidate score: 0.169445\n",
      "  [N=98] Baseline score: 0.371051, Candidate score: 0.169445\n",
      "  [N=98] ✓ IMPROVED over baseline: 0.371051 -> 0.169445 (Δ=0.201606, 54.33%)\n",
      "  [N=98] Cumulative Score: 19.751360 | Total Improvements: 103\n",
      "\n",
      "--- Processing N=97 ---\n",
      "  [N=97] Launching 60 parallel tasks...\n",
      "  [N=97] Best candidate score: 0.171192\n",
      "  [N=97] Baseline score: 0.371371, Candidate score: 0.171192\n",
      "  [N=97] ✓ IMPROVED over baseline: 0.371371 -> 0.171192 (Δ=0.200179, 53.90%)\n",
      "  [N=97] Cumulative Score: 19.922552 | Total Improvements: 104\n",
      "\n",
      "--- Processing N=96 ---\n",
      "  [N=96] Launching 60 parallel tasks...\n",
      "  [N=96] Best candidate score: 0.172975\n",
      "  [N=96] Baseline score: 0.367015, Candidate score: 0.172975\n",
      "  [N=96] ✓ IMPROVED over baseline: 0.367015 -> 0.172975 (Δ=0.194040, 52.87%)\n",
      "  [N=96] Cumulative Score: 20.095528 | Total Improvements: 105\n",
      "\n",
      "--- Processing N=95 ---\n",
      "  [N=95] Launching 60 parallel tasks...\n",
      "  [N=95] Best candidate score: 0.174796\n",
      "  [N=95] Baseline score: 0.370878, Candidate score: 0.174796\n",
      "  [N=95] ✓ IMPROVED over baseline: 0.370878 -> 0.174796 (Δ=0.196082, 52.87%)\n",
      "  [N=95] Cumulative Score: 20.270324 | Total Improvements: 106\n",
      "\n",
      "--- Processing N=94 ---\n",
      "  [N=94] Launching 60 parallel tasks...\n",
      "  [N=94] Best candidate score: 0.176656\n",
      "  [N=94] Baseline score: 0.366223, Candidate score: 0.176656\n",
      "  [N=94] ✓ IMPROVED over baseline: 0.366223 -> 0.176656 (Δ=0.189567, 51.76%)\n",
      "  [N=94] Cumulative Score: 20.446979 | Total Improvements: 107\n",
      "\n",
      "--- Processing N=93 ---\n",
      "  [N=93] Launching 60 parallel tasks...\n",
      "  [N=93] Best candidate score: 0.178555\n",
      "  [N=93] Baseline score: 0.366823, Candidate score: 0.178555\n",
      "  [N=93] ✓ IMPROVED over baseline: 0.366823 -> 0.178555 (Δ=0.188268, 51.32%)\n",
      "  [N=93] Cumulative Score: 20.625534 | Total Improvements: 108\n",
      "\n",
      "--- Processing N=92 ---\n",
      "  [N=92] Launching 60 parallel tasks...\n",
      "  [N=92] Best candidate score: 0.169592\n",
      "  [N=92] Baseline score: 0.370360, Candidate score: 0.169592\n",
      "  [N=92] ✓ IMPROVED over baseline: 0.370360 -> 0.169592 (Δ=0.200767, 54.21%)\n",
      "  [N=92] Cumulative Score: 20.795127 | Total Improvements: 109\n",
      "\n",
      "--- Processing N=91 ---\n",
      "  [N=91] Launching 60 parallel tasks...\n",
      "  [N=91] Best candidate score: 0.160776\n",
      "  [N=91] Baseline score: 0.370852, Candidate score: 0.160776\n",
      "  [N=91] ✓ IMPROVED over baseline: 0.370852 -> 0.160776 (Δ=0.210076, 56.65%)\n",
      "  [N=91] Cumulative Score: 20.955903 | Total Improvements: 110\n",
      "\n",
      "--- Processing N=90 ---\n",
      "  [N=90] Launching 60 parallel tasks...\n",
      "  [N=90] Best candidate score: 0.162563\n",
      "  [N=90] Baseline score: 0.371182, Candidate score: 0.162563\n",
      "  [N=90] ✓ IMPROVED over baseline: 0.371182 -> 0.162563 (Δ=0.208619, 56.20%)\n",
      "⚠️  Collision at N=90 between trees 0 and 2 (overlap area=1.000000e-02)\n",
      "  [N=90] Found 367 collision(s)\n",
      "  [N=90] Too many collisions - reverting to baseline\n",
      "  [N=90] Cumulative Score: 21.327085 | Total Improvements: 111\n",
      "\n",
      "--- Processing N=89 ---\n",
      "  [N=89] Launching 60 parallel tasks...\n",
      "  [N=89] Best candidate score: 0.164389\n",
      "  [N=89] Baseline score: 0.371919, Candidate score: 0.164389\n",
      "  [N=89] ✓ IMPROVED over baseline: 0.371919 -> 0.164389 (Δ=0.207530, 55.80%)\n",
      "  [N=89] Cumulative Score: 21.491474 | Total Improvements: 112\n",
      "\n",
      "--- Processing N=88 ---\n",
      "  [N=88] Launching 60 parallel tasks...\n",
      "  [N=88] Best candidate score: 0.166257\n",
      "  [N=88] Baseline score: 0.369513, Candidate score: 0.166257\n",
      "  [N=88] ✓ IMPROVED over baseline: 0.369513 -> 0.166257 (Δ=0.203256, 55.01%)\n",
      "  [N=88] Cumulative Score: 21.657731 | Total Improvements: 113\n",
      "\n",
      "--- Processing N=87 ---\n",
      "  [N=87] Launching 60 parallel tasks...\n",
      "  [N=87] Best candidate score: 0.157356\n",
      "  [N=87] Baseline score: 0.372366, Candidate score: 0.157356\n",
      "  [N=87] ✓ IMPROVED over baseline: 0.372366 -> 0.157356 (Δ=0.215009, 57.74%)\n",
      "  [N=87] Cumulative Score: 21.815087 | Total Improvements: 114\n",
      "\n",
      "--- Processing N=86 ---\n",
      "  [N=86] Launching 60 parallel tasks...\n",
      "  [N=86] Best candidate score: 0.159186\n",
      "  [N=86] Baseline score: 0.371389, Candidate score: 0.159186\n",
      "  [N=86] ✓ IMPROVED over baseline: 0.371389 -> 0.159186 (Δ=0.212203, 57.14%)\n",
      "  [N=86] Cumulative Score: 21.974273 | Total Improvements: 115\n",
      "\n",
      "--- Processing N=85 ---\n",
      "  [N=85] Launching 60 parallel tasks...\n",
      "  [N=85] Best candidate score: 0.161059\n",
      "  [N=85] Baseline score: 0.370625, Candidate score: 0.161059\n",
      "  [N=85] ✓ IMPROVED over baseline: 0.370625 -> 0.161059 (Δ=0.209566, 56.54%)\n",
      "  [N=85] Cumulative Score: 22.135332 | Total Improvements: 116\n",
      "\n",
      "--- Processing N=84 ---\n",
      "  [N=84] Launching 60 parallel tasks...\n",
      "  [N=84] Best candidate score: 0.162976\n",
      "  [N=84] Baseline score: 0.368300, Candidate score: 0.162976\n",
      "  [N=84] ✓ IMPROVED over baseline: 0.368300 -> 0.162976 (Δ=0.205324, 55.75%)\n",
      "  [N=84] Cumulative Score: 22.298308 | Total Improvements: 117\n",
      "\n",
      "--- Processing N=83 ---\n",
      "  [N=83] Launching 60 parallel tasks...\n",
      "  [N=83] Best candidate score: 0.164940\n",
      "  [N=83] Baseline score: 0.371648, Candidate score: 0.164940\n",
      "  [N=83] ✓ IMPROVED over baseline: 0.371648 -> 0.164940 (Δ=0.206708, 55.62%)\n",
      "  [N=83] Cumulative Score: 22.463248 | Total Improvements: 118\n",
      "\n",
      "--- Processing N=82 ---\n",
      "  [N=82] Launching 55 parallel tasks...\n",
      "  [N=82] Best candidate score: 0.166951\n",
      "  [N=82] Baseline score: 0.364938, Candidate score: 0.166951\n",
      "  [N=82] ✓ IMPROVED over baseline: 0.364938 -> 0.166951 (Δ=0.197987, 54.25%)\n",
      "  [N=82] Cumulative Score: 22.630199 | Total Improvements: 119\n",
      "\n",
      "--- Processing N=81 ---\n",
      "  [N=81] Launching 55 parallel tasks...\n",
      "  [N=81] Best candidate score: 0.169012\n",
      "  [N=81] Baseline score: 0.369401, Candidate score: 0.169012\n",
      "  [N=81] ✓ IMPROVED over baseline: 0.369401 -> 0.169012 (Δ=0.200388, 54.25%)\n",
      "  [N=81] Cumulative Score: 22.799211 | Total Improvements: 120\n",
      "\n",
      "--- Processing N=80 ---\n",
      "  [N=80] Launching 55 parallel tasks...\n",
      "  [N=80] Best candidate score: 0.171125\n",
      "  [N=80] Baseline score: 0.369934, Candidate score: 0.171125\n",
      "  [N=80] ✓ IMPROVED over baseline: 0.369934 -> 0.171125 (Δ=0.198809, 53.74%)\n",
      "⚠️  Collision at N=80 between trees 0 and 1 (overlap area=6.085227e-02)\n",
      "  [N=80] Found 327 collision(s)\n",
      "  [N=80] Too many collisions - reverting to baseline\n",
      "  [N=80] Cumulative Score: 23.169145 | Total Improvements: 121\n",
      "\n",
      "--- Processing N=79 ---\n",
      "  [N=79] Launching 55 parallel tasks...\n",
      "  [N=79] Best candidate score: 0.173291\n",
      "  [N=79] Baseline score: 0.370342, Candidate score: 0.173291\n",
      "  [N=79] ✓ IMPROVED over baseline: 0.370342 -> 0.173291 (Δ=0.197051, 53.21%)\n",
      "  [N=79] Cumulative Score: 23.342436 | Total Improvements: 122\n",
      "\n",
      "--- Processing N=78 ---\n",
      "  [N=78] Launching 55 parallel tasks...\n",
      "  [N=78] Best candidate score: 0.175513\n",
      "  [N=78] Baseline score: 0.367700, Candidate score: 0.175513\n",
      "  [N=78] ✓ IMPROVED over baseline: 0.367700 -> 0.175513 (Δ=0.192187, 52.27%)\n",
      "  [N=78] Cumulative Score: 23.517949 | Total Improvements: 123\n",
      "\n",
      "--- Processing N=77 ---\n",
      "  [N=77] Launching 55 parallel tasks...\n",
      "  [N=77] Best candidate score: 0.177792\n",
      "  [N=77] Baseline score: 0.368571, Candidate score: 0.177792\n",
      "  [N=77] ✓ IMPROVED over baseline: 0.368571 -> 0.177792 (Δ=0.190779, 51.76%)\n",
      "  [N=77] Cumulative Score: 23.695741 | Total Improvements: 124\n",
      "\n",
      "--- Processing N=76 ---\n",
      "  [N=76] Launching 55 parallel tasks...\n",
      "  [N=76] Best candidate score: 0.180132\n",
      "  [N=76] Baseline score: 0.372634, Candidate score: 0.180132\n",
      "  [N=76] ✓ IMPROVED over baseline: 0.372634 -> 0.180132 (Δ=0.192502, 51.66%)\n",
      "  [N=76] Cumulative Score: 23.875873 | Total Improvements: 125\n",
      "\n",
      "--- Processing N=75 ---\n",
      "  [N=75] Launching 55 parallel tasks...\n",
      "  [N=75] Best candidate score: 0.182533\n",
      "  [N=75] Baseline score: 0.367851, Candidate score: 0.182533\n",
      "  [N=75] ✓ IMPROVED over baseline: 0.367851 -> 0.182533 (Δ=0.185317, 50.38%)\n",
      "  [N=75] Cumulative Score: 24.058406 | Total Improvements: 126\n",
      "\n",
      "--- Processing N=74 ---\n",
      "  [N=74] Launching 55 parallel tasks...\n",
      "  [N=74] Best candidate score: 0.185000\n",
      "  [N=74] Baseline score: 0.370586, Candidate score: 0.185000\n",
      "  [N=74] ✓ IMPROVED over baseline: 0.370586 -> 0.185000 (Δ=0.185586, 50.08%)\n",
      "  [N=74] Cumulative Score: 24.243406 | Total Improvements: 127\n",
      "\n",
      "--- Processing N=73 ---\n",
      "  [N=73] Launching 55 parallel tasks...\n",
      "  [N=73] Best candidate score: 0.176010\n",
      "  [N=73] Baseline score: 0.372555, Candidate score: 0.176010\n",
      "  [N=73] ✓ IMPROVED over baseline: 0.372555 -> 0.176010 (Δ=0.196544, 52.76%)\n",
      "  [N=73] Cumulative Score: 24.419417 | Total Improvements: 128\n",
      "\n",
      "--- Processing N=72 ---\n",
      "  [N=72] Launching 60 parallel tasks...\n",
      "  [N=72] Best candidate score: 0.177509\n",
      "  [N=72] Baseline score: 0.369245, Candidate score: 0.177509\n",
      "  [N=72] ✓ IMPROVED over baseline: 0.369245 -> 0.177509 (Δ=0.191736, 51.93%)\n",
      "  [N=72] Cumulative Score: 24.596925 | Total Improvements: 129\n",
      "\n",
      "--- Processing N=71 ---\n",
      "  [N=71] Launching 60 parallel tasks...\n",
      "  [N=71] Best candidate score: 0.180009\n",
      "  [N=71] Baseline score: 0.370585, Candidate score: 0.180009\n",
      "  [N=71] ✓ IMPROVED over baseline: 0.370585 -> 0.180009 (Δ=0.190576, 51.43%)\n",
      "  [N=71] Cumulative Score: 24.776934 | Total Improvements: 130\n",
      "\n",
      "--- Processing N=70 ---\n",
      "  [N=70] Launching 60 parallel tasks...\n",
      "  [N=70] Best candidate score: 0.182580\n",
      "  [N=70] Baseline score: 0.371931, Candidate score: 0.182580\n",
      "  [N=70] ✓ IMPROVED over baseline: 0.371931 -> 0.182580 (Δ=0.189351, 50.91%)\n",
      "⚠️  Collision at N=70 between trees 0 and 12 (overlap area=9.388889e-03)\n",
      "  [N=70] Found 296 collision(s)\n",
      "  [N=70] Too many collisions - reverting to baseline\n",
      "  [N=70] Cumulative Score: 25.148865 | Total Improvements: 131\n",
      "\n",
      "--- Processing N=69 ---\n",
      "  [N=69] Launching 60 parallel tasks...\n",
      "  [N=69] Best candidate score: 0.185226\n",
      "  [N=69] Baseline score: 0.367506, Candidate score: 0.185226\n",
      "  [N=69] ✓ IMPROVED over baseline: 0.367506 -> 0.185226 (Δ=0.182280, 49.60%)\n",
      "  [N=69] Cumulative Score: 25.334092 | Total Improvements: 132\n",
      "\n",
      "--- Processing N=68 ---\n",
      "  [N=68] Launching 55 parallel tasks...\n",
      "  [N=68] Best candidate score: 0.187950\n",
      "  [N=68] Baseline score: 0.370141, Candidate score: 0.187950\n",
      "  [N=68] ✓ IMPROVED over baseline: 0.370141 -> 0.187950 (Δ=0.182191, 49.22%)\n",
      "  [N=68] Cumulative Score: 25.522042 | Total Improvements: 133\n",
      "\n",
      "--- Processing N=67 ---\n",
      "  [N=67] Launching 55 parallel tasks...\n",
      "  [N=67] Best candidate score: 0.190756\n",
      "  [N=67] Baseline score: 0.367032, Candidate score: 0.190756\n",
      "  [N=67] ✓ IMPROVED over baseline: 0.367032 -> 0.190756 (Δ=0.176276, 48.03%)\n",
      "  [N=67] Cumulative Score: 25.712798 | Total Improvements: 134\n",
      "\n",
      "--- Processing N=66 ---\n",
      "  [N=66] Launching 60 parallel tasks...\n",
      "  [N=66] Best candidate score: 0.193646\n",
      "  [N=66] Baseline score: 0.365908, Candidate score: 0.193646\n",
      "  [N=66] ✓ IMPROVED over baseline: 0.365908 -> 0.193646 (Δ=0.172262, 47.08%)\n",
      "  [N=66] Cumulative Score: 25.906444 | Total Improvements: 135\n",
      "\n",
      "--- Processing N=65 ---\n",
      "  [N=65] Launching 60 parallel tasks...\n",
      "  [N=65] Best candidate score: 0.196625\n",
      "  [N=65] Baseline score: 0.371352, Candidate score: 0.196625\n",
      "  [N=65] ✓ IMPROVED over baseline: 0.371352 -> 0.196625 (Δ=0.174727, 47.05%)\n",
      "  [N=65] Cumulative Score: 26.103069 | Total Improvements: 136\n",
      "\n",
      "--- Processing N=64 ---\n",
      "  [N=64] Launching 60 parallel tasks...\n",
      "  [N=64] Best candidate score: 0.199697\n",
      "  [N=64] Baseline score: 0.356070, Candidate score: 0.199697\n",
      "  [N=64] ✓ IMPROVED over baseline: 0.356070 -> 0.199697 (Δ=0.156373, 43.92%)\n",
      "  [N=64] Cumulative Score: 26.302766 | Total Improvements: 137\n",
      "\n",
      "--- Processing N=63 ---\n",
      "  [N=63] Launching 60 parallel tasks...\n",
      "  [N=63] Best candidate score: 0.202867\n",
      "  [N=63] Baseline score: 0.361624, Candidate score: 0.202867\n",
      "  [N=63] ✓ IMPROVED over baseline: 0.361624 -> 0.202867 (Δ=0.158756, 43.90%)\n",
      "  [N=63] Cumulative Score: 26.505633 | Total Improvements: 138\n",
      "\n",
      "--- Processing N=62 ---\n",
      "  [N=62] Launching 60 parallel tasks...\n",
      "  [N=62] Best candidate score: 0.206139\n",
      "  [N=62] Baseline score: 0.366064, Candidate score: 0.206139\n",
      "  [N=62] ✓ IMPROVED over baseline: 0.366064 -> 0.206139 (Δ=0.159925, 43.69%)\n",
      "  [N=62] Cumulative Score: 26.711772 | Total Improvements: 139\n",
      "\n",
      "--- Processing N=61 ---\n",
      "  [N=61] Launching 60 parallel tasks...\n",
      "  [N=61] Best candidate score: 0.209518\n",
      "  [N=61] Baseline score: 0.371777, Candidate score: 0.209518\n",
      "  [N=61] ✓ IMPROVED over baseline: 0.371777 -> 0.209518 (Δ=0.162258, 43.64%)\n",
      "  [N=61] Cumulative Score: 26.921290 | Total Improvements: 140\n",
      "\n",
      "--- Processing N=60 ---\n",
      "  [N=60] Launching 60 parallel tasks...\n",
      "  [N=60] Best candidate score: 0.213010\n",
      "  [N=60] Baseline score: 0.370453, Candidate score: 0.213010\n",
      "  [N=60] ✓ IMPROVED over baseline: 0.370453 -> 0.213010 (Δ=0.157442, 42.50%)\n",
      "⚠️  Collision at N=60 between trees 0 and 1 (overlap area=6.085227e-02)\n",
      "  [N=60] Found 229 collision(s)\n",
      "  [N=60] Too many collisions - reverting to baseline\n",
      "  [N=60] Cumulative Score: 27.291743 | Total Improvements: 141\n",
      "\n",
      "--- Processing N=59 ---\n",
      "  [N=59] Launching 60 parallel tasks...\n",
      "  [N=59] Best candidate score: 0.201737\n",
      "  [N=59] Baseline score: 0.369829, Candidate score: 0.201737\n",
      "  [N=59] ✓ IMPROVED over baseline: 0.369829 -> 0.201737 (Δ=0.168091, 45.45%)\n",
      "  [N=59] Cumulative Score: 27.493481 | Total Improvements: 142\n",
      "\n",
      "--- Processing N=58 ---\n",
      "  [N=58] Launching 60 parallel tasks...\n",
      "  [N=58] Best candidate score: 0.205216\n",
      "  [N=58] Baseline score: 0.371246, Candidate score: 0.205216\n",
      "  [N=58] ✓ IMPROVED over baseline: 0.371246 -> 0.205216 (Δ=0.166031, 44.72%)\n",
      "  [N=58] Cumulative Score: 27.698696 | Total Improvements: 143\n",
      "\n",
      "--- Processing N=57 ---\n",
      "  [N=57] Launching 60 parallel tasks...\n",
      "  [N=57] Best candidate score: 0.208816\n",
      "  [N=57] Baseline score: 0.369997, Candidate score: 0.208816\n",
      "  [N=57] ✓ IMPROVED over baseline: 0.369997 -> 0.208816 (Δ=0.161182, 43.56%)\n",
      "  [N=57] Cumulative Score: 27.907512 | Total Improvements: 144\n",
      "\n",
      "--- Processing N=56 ---\n",
      "  [N=56] Launching 60 parallel tasks...\n",
      "  [N=56] Best candidate score: 0.210641\n",
      "  [N=56] Baseline score: 0.374822, Candidate score: 0.210641\n",
      "  [N=56] ✓ IMPROVED over baseline: 0.374822 -> 0.210641 (Δ=0.164181, 43.80%)\n",
      "  [N=56] Cumulative Score: 28.118153 | Total Improvements: 145\n",
      "\n",
      "--- Processing N=55 ---\n",
      "  [N=55] Launching 60 parallel tasks...\n",
      "  [N=55] Best candidate score: 0.201011\n",
      "  [N=55] Baseline score: 0.367422, Candidate score: 0.201011\n",
      "  [N=55] ✓ IMPROVED over baseline: 0.367422 -> 0.201011 (Δ=0.166411, 45.29%)\n",
      "  [N=55] Cumulative Score: 28.319164 | Total Improvements: 146\n",
      "\n",
      "--- Processing N=54 ---\n",
      "  [N=54] Launching 60 parallel tasks...\n",
      "  [N=54] Best candidate score: 0.204734\n",
      "  [N=54] Baseline score: 0.374137, Candidate score: 0.204734\n",
      "  [N=54] ✓ IMPROVED over baseline: 0.374137 -> 0.204734 (Δ=0.169403, 45.28%)\n",
      "  [N=54] Cumulative Score: 28.523898 | Total Improvements: 147\n",
      "\n",
      "--- Processing N=53 ---\n",
      "  [N=53] Launching 60 parallel tasks...\n",
      "  [N=53] Best candidate score: 0.193208\n",
      "  [N=53] Baseline score: 0.364385, Candidate score: 0.193208\n",
      "  [N=53] ✓ IMPROVED over baseline: 0.364385 -> 0.193208 (Δ=0.171178, 46.98%)\n",
      "  [N=53] Cumulative Score: 28.717106 | Total Improvements: 148\n",
      "\n",
      "--- Processing N=52 ---\n",
      "  [N=52] Launching 60 parallel tasks...\n",
      "  [N=52] Best candidate score: 0.196923\n",
      "  [N=52] Baseline score: 0.368939, Candidate score: 0.196923\n",
      "  [N=52] ✓ IMPROVED over baseline: 0.368939 -> 0.196923 (Δ=0.172016, 46.62%)\n",
      "  [N=52] Cumulative Score: 28.914029 | Total Improvements: 149\n",
      "\n",
      "--- Processing N=51 ---\n",
      "  [N=51] Launching 60 parallel tasks...\n",
      "  [N=51] Best candidate score: 0.200784\n",
      "  [N=51] Baseline score: 0.370994, Candidate score: 0.200784\n",
      "  [N=51] ✓ IMPROVED over baseline: 0.370994 -> 0.200784 (Δ=0.170210, 45.88%)\n",
      "  [N=51] Cumulative Score: 29.114813 | Total Improvements: 150\n",
      "\n",
      "--- Processing N=50 ---\n",
      "  [N=50] Critical N! Widening beam to 24\n",
      "  [N=50] Launching 60 parallel tasks...\n",
      "  [N=50] Best candidate score: 0.189113\n",
      "  [N=50] Baseline score: 0.371935, Candidate score: 0.189113\n",
      "  [N=50] ✓ IMPROVED over baseline: 0.371935 -> 0.189113 (Δ=0.182822, 49.15%)\n",
      "⚠️  Collision at N=50 between trees 0 and 10 (overlap area=1.182419e-01)\n",
      "  [N=50] Found 187 collision(s)\n",
      "  [N=50] Too many collisions - reverting to baseline\n",
      "  [N=50] Cumulative Score: 29.486748 | Total Improvements: 151\n",
      "\n",
      "--- Processing N=49 ---\n",
      "  [N=49] Launching 65 parallel tasks...\n",
      "  [N=49] Best candidate score: 0.192972\n",
      "  [N=49] Baseline score: 0.370888, Candidate score: 0.192972\n",
      "  [N=49] ✓ IMPROVED over baseline: 0.370888 -> 0.192972 (Δ=0.177916, 47.97%)\n",
      "  [N=49] Cumulative Score: 29.679720 | Total Improvements: 152\n",
      "\n",
      "--- Processing N=48 ---\n",
      "  [N=48] Launching 60 parallel tasks...\n",
      "  [N=48] Best candidate score: 0.181302\n",
      "  [N=48] Baseline score: 0.374823, Candidate score: 0.181302\n",
      "  [N=48] ✓ IMPROVED over baseline: 0.374823 -> 0.181302 (Δ=0.193521, 51.63%)\n",
      "  [N=48] Cumulative Score: 29.861022 | Total Improvements: 153\n",
      "\n",
      "--- Processing N=47 ---\n",
      "  [N=47] Launching 60 parallel tasks...\n",
      "  [N=47] Best candidate score: 0.185160\n",
      "  [N=47] Baseline score: 0.374941, Candidate score: 0.185160\n",
      "  [N=47] ✓ IMPROVED over baseline: 0.374941 -> 0.185160 (Δ=0.189781, 50.62%)\n",
      "  [N=47] Cumulative Score: 30.046182 | Total Improvements: 154\n",
      "\n",
      "--- Processing N=46 ---\n",
      "  [N=46] Launching 60 parallel tasks...\n",
      "  [N=46] Best candidate score: 0.189185\n",
      "  [N=46] Baseline score: 0.368678, Candidate score: 0.189185\n",
      "  [N=46] ✓ IMPROVED over baseline: 0.368678 -> 0.189185 (Δ=0.179493, 48.69%)\n",
      "  [N=46] Cumulative Score: 30.235367 | Total Improvements: 155\n",
      "\n",
      "--- Processing N=45 ---\n",
      "  [N=45] Launching 60 parallel tasks...\n",
      "  [N=45] Best candidate score: 0.177347\n",
      "  [N=45] Baseline score: 0.366988, Candidate score: 0.177347\n",
      "  [N=45] ✓ IMPROVED over baseline: 0.366988 -> 0.177347 (Δ=0.189641, 51.67%)\n",
      "  [N=45] Cumulative Score: 30.412714 | Total Improvements: 156\n",
      "\n",
      "--- Processing N=44 ---\n",
      "  [N=44] Launching 60 parallel tasks...\n",
      "  [N=44] Best candidate score: 0.181378\n",
      "  [N=44] Baseline score: 0.371452, Candidate score: 0.181378\n",
      "  [N=44] ✓ IMPROVED over baseline: 0.371452 -> 0.181378 (Δ=0.190074, 51.17%)\n",
      "  [N=44] Cumulative Score: 30.594092 | Total Improvements: 157\n",
      "\n",
      "--- Processing N=43 ---\n",
      "  [N=43] Launching 60 parallel tasks...\n",
      "  [N=43] Best candidate score: 0.182326\n",
      "  [N=43] Baseline score: 0.375307, Candidate score: 0.182326\n",
      "  [N=43] ✓ IMPROVED over baseline: 0.375307 -> 0.182326 (Δ=0.192982, 51.42%)\n",
      "  [N=43] Cumulative Score: 30.776417 | Total Improvements: 158\n",
      "\n",
      "--- Processing N=42 ---\n",
      "  [N=42] Launching 60 parallel tasks...\n",
      "  [N=42] Best candidate score: 0.186667\n",
      "  [N=42] Baseline score: 0.370434, Candidate score: 0.186667\n",
      "  [N=42] ✓ IMPROVED over baseline: 0.370434 -> 0.186667 (Δ=0.183768, 49.61%)\n",
      "  [N=42] Cumulative Score: 30.963084 | Total Improvements: 159\n",
      "\n",
      "--- Processing N=41 ---\n",
      "  [N=41] Launching 60 parallel tasks...\n",
      "  [N=41] Best candidate score: 0.191220\n",
      "  [N=41] Baseline score: 0.371981, Candidate score: 0.191220\n",
      "  [N=41] ✓ IMPROVED over baseline: 0.371981 -> 0.191220 (Δ=0.180761, 48.59%)\n",
      "  [N=41] Cumulative Score: 31.154303 | Total Improvements: 160\n",
      "\n",
      "--- Processing N=40 ---\n",
      "  [N=40] Launching 60 parallel tasks...\n",
      "  [N=40] Best candidate score: 0.196000\n",
      "  [N=40] Baseline score: 0.373961, Candidate score: 0.196000\n",
      "  [N=40] ✓ IMPROVED over baseline: 0.373961 -> 0.196000 (Δ=0.177961, 47.59%)\n",
      "⚠️  Collision at N=40 between trees 0 and 1 (overlap area=6.085227e-02)\n",
      "  [N=40] Found 139 collision(s)\n",
      "  [N=40] Too many collisions - reverting to baseline\n",
      "  [N=40] Cumulative Score: 31.528264 | Total Improvements: 161\n",
      "\n",
      "--- Processing N=39 ---\n",
      "  [N=39] Launching 60 parallel tasks...\n",
      "  [N=39] Best candidate score: 0.201026\n",
      "  [N=39] Baseline score: 0.363708, Candidate score: 0.201026\n",
      "  [N=39] ✓ IMPROVED over baseline: 0.363708 -> 0.201026 (Δ=0.162682, 44.73%)\n",
      "  [N=39] Cumulative Score: 31.729290 | Total Improvements: 162\n",
      "\n",
      "--- Processing N=38 ---\n",
      "  [N=38] Launching 60 parallel tasks...\n",
      "  [N=38] Best candidate score: 0.206316\n",
      "  [N=38] Baseline score: 0.372957, Candidate score: 0.206316\n",
      "  [N=38] ✓ IMPROVED over baseline: 0.372957 -> 0.206316 (Δ=0.166641, 44.68%)\n",
      "  [N=38] Cumulative Score: 31.935606 | Total Improvements: 163\n",
      "\n",
      "--- Processing N=37 ---\n",
      "  [N=37] Launching 60 parallel tasks...\n",
      "  [N=37] Best candidate score: 0.211892\n",
      "  [N=37] Baseline score: 0.371585, Candidate score: 0.211892\n",
      "  [N=37] ✓ IMPROVED over baseline: 0.371585 -> 0.211892 (Δ=0.159693, 42.98%)\n",
      "  [N=37] Cumulative Score: 32.147497 | Total Improvements: 164\n",
      "\n",
      "--- Processing N=36 ---\n",
      "  [N=36] Launching 60 parallel tasks...\n",
      "  [N=36] Best candidate score: 0.217778\n",
      "  [N=36] Baseline score: 0.366036, Candidate score: 0.217778\n",
      "  [N=36] ✓ IMPROVED over baseline: 0.366036 -> 0.217778 (Δ=0.148259, 40.50%)\n",
      "  [N=36] Cumulative Score: 32.365275 | Total Improvements: 165\n",
      "\n",
      "--- Processing N=35 ---\n",
      "  [N=35] Launching 60 parallel tasks...\n",
      "  [N=35] Best candidate score: 0.224000\n",
      "  [N=35] Baseline score: 0.375993, Candidate score: 0.224000\n",
      "  [N=35] ✓ IMPROVED over baseline: 0.375993 -> 0.224000 (Δ=0.151993, 40.42%)\n",
      "  [N=35] Cumulative Score: 32.589275 | Total Improvements: 166\n",
      "\n",
      "--- Processing N=34 ---\n",
      "  [N=34] Launching 60 parallel tasks...\n",
      "  [N=34] Best candidate score: 0.214412\n",
      "  [N=34] Baseline score: 0.372156, Candidate score: 0.214412\n",
      "  [N=34] ✓ IMPROVED over baseline: 0.372156 -> 0.214412 (Δ=0.157744, 42.39%)\n",
      "  [N=34] Cumulative Score: 32.803687 | Total Improvements: 167\n",
      "\n",
      "--- Processing N=33 ---\n",
      "  [N=33] Launching 60 parallel tasks...\n",
      "  [N=33] Best candidate score: 0.220909\n",
      "  [N=33] Baseline score: 0.372840, Candidate score: 0.220909\n",
      "  [N=33] ✓ IMPROVED over baseline: 0.372840 -> 0.220909 (Δ=0.151931, 40.75%)\n",
      "  [N=33] Cumulative Score: 33.024596 | Total Improvements: 168\n",
      "\n",
      "--- Processing N=32 ---\n",
      "  [N=32] Launching 60 parallel tasks...\n",
      "  [N=32] Best candidate score: 0.227813\n",
      "  [N=32] Baseline score: 0.367728, Candidate score: 0.227813\n",
      "  [N=32] ✓ IMPROVED over baseline: 0.367728 -> 0.227813 (Δ=0.139915, 38.05%)\n",
      "  [N=32] Cumulative Score: 33.252409 | Total Improvements: 169\n",
      "\n",
      "--- Processing N=31 ---\n",
      "  [N=31] Launching 60 parallel tasks...\n",
      "  [N=31] Best candidate score: 0.226532\n",
      "  [N=31] Baseline score: 0.373138, Candidate score: 0.226532\n",
      "  [N=31] ✓ IMPROVED over baseline: 0.373138 -> 0.226532 (Δ=0.146606, 39.29%)\n",
      "  [N=31] Cumulative Score: 33.478941 | Total Improvements: 170\n",
      "\n",
      "--- Processing N=30 ---\n",
      "  [N=30] Launching 60 parallel tasks...\n",
      "  [N=30] Best candidate score: 0.234083\n",
      "  [N=30] Baseline score: 0.375478, Candidate score: 0.234083\n",
      "  [N=30] ✓ IMPROVED over baseline: 0.375478 -> 0.234083 (Δ=0.141395, 37.66%)\n",
      "⚠️  Collision at N=30 between trees 0 and 1 (overlap area=6.085227e-02)\n",
      "  [N=30] Found 88 collision(s)\n",
      "  [N=30] Too many collisions - reverting to baseline\n",
      "  [N=30] Cumulative Score: 33.854419 | Total Improvements: 171\n",
      "\n",
      "--- Processing N=29 ---\n",
      "  [N=29] Launching 60 parallel tasks...\n",
      "  [N=29] Best candidate score: 0.242155\n",
      "  [N=29] Baseline score: 0.377787, Candidate score: 0.242155\n",
      "  [N=29] ✓ IMPROVED over baseline: 0.377787 -> 0.242155 (Δ=0.135632, 35.90%)\n",
      "  [N=29] Cumulative Score: 34.096574 | Total Improvements: 172\n",
      "\n",
      "--- Processing N=28 ---\n",
      "  [N=28] Launching 60 parallel tasks...\n",
      "  [N=28] Best candidate score: 0.193058\n",
      "  [N=28] Baseline score: 0.374449, Candidate score: 0.193058\n",
      "  [N=28] ✓ IMPROVED over baseline: 0.374449 -> 0.193058 (Δ=0.181391, 48.44%)\n",
      "  [N=28] Cumulative Score: 34.289632 | Total Improvements: 173\n",
      "\n",
      "--- Processing N=27 ---\n",
      "  [N=27] Launching 60 parallel tasks...\n",
      "  [N=27] Best candidate score: 0.200208\n",
      "  [N=27] Baseline score: 0.378276, Candidate score: 0.200208\n",
      "  [N=27] ✓ IMPROVED over baseline: 0.378276 -> 0.200208 (Δ=0.178068, 47.07%)\n",
      "  [N=27] Cumulative Score: 34.489840 | Total Improvements: 174\n",
      "\n",
      "--- Processing N=26 ---\n",
      "  [N=26] Launching 60 parallel tasks...\n",
      "  [N=26] Best candidate score: 0.207909\n",
      "  [N=26] Baseline score: 0.376667, Candidate score: 0.207909\n",
      "  [N=26] ✓ IMPROVED over baseline: 0.376667 -> 0.207909 (Δ=0.168759, 44.80%)\n",
      "  [N=26] Cumulative Score: 34.697749 | Total Improvements: 175\n",
      "\n",
      "--- Processing N=25 ---\n",
      "  [N=25] Critical N! Widening beam to 24\n",
      "  [N=25] Launching 60 parallel tasks...\n",
      "  [N=25] Hybrid Search: Generating forward candidates...\n",
      "  [N=25] Added 3 forward candidates.\n",
      "  [N=25] Best candidate score: 0.216225\n",
      "  [N=25] Baseline score: 0.373225, Candidate score: 0.216225\n",
      "  [N=25] ✓ IMPROVED over baseline: 0.373225 -> 0.216225 (Δ=0.157000, 42.07%)\n",
      "  [N=25] Cumulative Score: 34.913974 | Total Improvements: 176\n",
      "\n",
      "--- Processing N=24 ---\n",
      "  [N=24] Launching 75 parallel tasks...\n",
      "  [N=24] Hybrid Search: Generating forward candidates...\n",
      "  [N=24] Added 3 forward candidates.\n",
      "  [N=24] Best candidate score: 0.225234\n",
      "  [N=24] Baseline score: 0.367224, Candidate score: 0.225234\n",
      "  [N=24] ✓ IMPROVED over baseline: 0.367224 -> 0.225234 (Δ=0.141990, 38.67%)\n",
      "  [N=24] Cumulative Score: 35.139208 | Total Improvements: 177\n",
      "\n",
      "--- Processing N=23 ---\n",
      "  [N=23] Launching 60 parallel tasks...\n",
      "  [N=23] Hybrid Search: Generating forward candidates...\n",
      "  [N=23] Added 3 forward candidates.\n",
      "  [N=23] Best candidate score: 0.235027\n",
      "  [N=23] Baseline score: 0.370417, Candidate score: 0.235027\n",
      "  [N=23] ✓ IMPROVED over baseline: 0.370417 -> 0.235027 (Δ=0.135390, 36.55%)\n",
      "  [N=23] Cumulative Score: 35.374235 | Total Improvements: 178\n",
      "\n",
      "--- Processing N=22 ---\n",
      "  [N=22] Launching 60 parallel tasks...\n",
      "  [N=22] Hybrid Search: Generating forward candidates...\n",
      "  [N=22] Added 3 forward candidates.\n",
      "  [N=22] Best candidate score: 0.245710\n",
      "  [N=22] Baseline score: 0.380400, Candidate score: 0.245710\n",
      "  [N=22] ✓ IMPROVED over baseline: 0.380400 -> 0.245710 (Δ=0.134689, 35.41%)\n",
      "  [N=22] Cumulative Score: 35.619946 | Total Improvements: 179\n",
      "\n",
      "--- Processing N=21 ---\n",
      "  [N=21] Launching 60 parallel tasks...\n",
      "  [N=21] Hybrid Search: Generating forward candidates...\n",
      "  [N=21] Added 3 forward candidates.\n",
      "  [N=21] Best candidate score: 0.257411\n",
      "  [N=21] Baseline score: 0.378364, Candidate score: 0.257411\n",
      "  [N=21] ✓ IMPROVED over baseline: 0.378364 -> 0.257411 (Δ=0.120953, 31.97%)\n",
      "  [N=21] Cumulative Score: 35.877356 | Total Improvements: 180\n",
      "\n",
      "--- Processing N=20 ---\n",
      "  [N=20] Launching 60 parallel tasks...\n",
      "  [N=20] Hybrid Search: Generating forward candidates...\n",
      "  [N=20] Added 3 forward candidates.\n",
      "  [N=20] Best candidate score: 0.242000\n",
      "  [N=20] Baseline score: 0.378766, Candidate score: 0.242000\n",
      "  [N=20] ✓ IMPROVED over baseline: 0.378766 -> 0.242000 (Δ=0.136766, 36.11%)\n",
      "⚠️  Collision at N=20 between trees 0 and 6 (overlap area=1.182419e-01)\n",
      "  [N=20] Found 60 collision(s)\n",
      "  [N=20] Too many collisions - reverting to baseline\n",
      "  [N=20] Cumulative Score: 36.256122 | Total Improvements: 181\n",
      "\n",
      "--- Processing N=19 ---\n",
      "  [N=19] Launching 60 parallel tasks...\n",
      "  [N=19] Hybrid Search: Generating forward candidates...\n",
      "  [N=19] Added 3 forward candidates.\n",
      "  [N=19] Best candidate score: 0.254737\n",
      "  [N=19] Baseline score: 0.379252, Candidate score: 0.254737\n",
      "  [N=19] ✓ IMPROVED over baseline: 0.379252 -> 0.254737 (Δ=0.124515, 32.83%)\n",
      "  [N=19] Cumulative Score: 36.510859 | Total Improvements: 182\n",
      "\n",
      "--- Processing N=18 ---\n",
      "  [N=18] Launching 60 parallel tasks...\n",
      "  [N=18] Hybrid Search: Generating forward candidates...\n",
      "  [N=18] Added 3 forward candidates.\n",
      "  [N=18] Best candidate score: 0.268889\n",
      "  [N=18] Baseline score: 0.371206, Candidate score: 0.268889\n",
      "  [N=18] ✓ IMPROVED over baseline: 0.371206 -> 0.268889 (Δ=0.102317, 27.56%)\n",
      "  [N=18] Cumulative Score: 36.779748 | Total Improvements: 183\n",
      "\n",
      "--- Processing N=17 ---\n",
      "  [N=17] Launching 60 parallel tasks...\n",
      "  [N=17] Hybrid Search: Generating forward candidates...\n",
      "  [N=17] Added 3 forward candidates.\n",
      "  [N=17] Best candidate score: 0.253272\n",
      "  [N=17] Baseline score: 0.370876, Candidate score: 0.253272\n",
      "  [N=17] ✓ IMPROVED over baseline: 0.370876 -> 0.253272 (Δ=0.117604, 31.71%)\n",
      "  [N=17] Cumulative Score: 37.033020 | Total Improvements: 184\n",
      "\n",
      "--- Processing N=16 ---\n",
      "  [N=16] Launching 60 parallel tasks...\n",
      "  [N=16] Hybrid Search: Generating forward candidates...\n",
      "  [N=16] Added 3 forward candidates.\n",
      "  [N=16] Best candidate score: 0.237656\n",
      "  [N=16] Baseline score: 0.380281, Candidate score: 0.237656\n",
      "  [N=16] ✓ IMPROVED over baseline: 0.380281 -> 0.237656 (Δ=0.142624, 37.51%)\n",
      "  [N=16] Cumulative Score: 37.270676 | Total Improvements: 185\n",
      "\n",
      "--- Processing N=15 ---\n",
      "  [N=15] Launching 60 parallel tasks...\n",
      "  [N=15] Hybrid Search: Generating forward candidates...\n",
      "  [N=15] Added 3 forward candidates.\n",
      "  [N=15] Best candidate score: 0.253500\n",
      "  [N=15] Baseline score: 0.379959, Candidate score: 0.253500\n",
      "  [N=15] ✓ IMPROVED over baseline: 0.379959 -> 0.253500 (Δ=0.126459, 33.28%)\n",
      "  [N=15] Cumulative Score: 37.524176 | Total Improvements: 186\n",
      "\n",
      "--- Processing N=14 ---\n",
      "  [N=14] Launching 60 parallel tasks...\n",
      "  [N=14] Hybrid Search: Generating forward candidates...\n",
      "  [N=14] Added 3 forward candidates.\n",
      "  [N=14] Best candidate score: 0.257857\n",
      "  [N=14] Baseline score: 0.382313, Candidate score: 0.257857\n",
      "  [N=14] ✓ IMPROVED over baseline: 0.382313 -> 0.257857 (Δ=0.124456, 32.55%)\n",
      "  [N=14] Cumulative Score: 37.782034 | Total Improvements: 187\n",
      "\n",
      "--- Processing N=13 ---\n",
      "  [N=13] Launching 60 parallel tasks...\n",
      "  [N=13] Hybrid Search: Generating forward candidates...\n",
      "  [N=13] Added 3 forward candidates.\n",
      "  [N=13] Best candidate score: 0.277692\n",
      "  [N=13] Baseline score: 0.373334, Candidate score: 0.277692\n",
      "  [N=13] ✓ IMPROVED over baseline: 0.373334 -> 0.277692 (Δ=0.095641, 25.62%)\n",
      "  [N=13] Cumulative Score: 38.059726 | Total Improvements: 188\n",
      "\n",
      "--- Processing N=12 ---\n",
      "  [N=12] Launching 60 parallel tasks...\n",
      "  [N=12] Hybrid Search: Generating forward candidates...\n",
      "  [N=12] Added 3 forward candidates.\n",
      "  [N=12] Best candidate score: 0.300833\n",
      "  [N=12] Baseline score: 0.375278, Candidate score: 0.300833\n",
      "  [N=12] ✓ IMPROVED over baseline: 0.375278 -> 0.300833 (Δ=0.074444, 19.84%)\n",
      "  [N=12] Cumulative Score: 38.360559 | Total Improvements: 189\n",
      "\n",
      "--- Processing N=11 ---\n",
      "  [N=11] Launching 60 parallel tasks...\n",
      "  [N=11] Hybrid Search: Generating forward candidates...\n",
      "  [N=11] Added 3 forward candidates.\n",
      "  [N=11] Best candidate score: 0.328182\n",
      "  [N=11] Baseline score: 0.377046, Candidate score: 0.328182\n",
      "  [N=11] ✓ IMPROVED over baseline: 0.377046 -> 0.328182 (Δ=0.048865, 12.96%)\n",
      "  [N=11] Cumulative Score: 38.688741 | Total Improvements: 190\n",
      "\n",
      "--- Processing N=10 ---\n",
      "  [N=10] Critical N! Widening beam to 24\n",
      "  [N=10] Launching 60 parallel tasks...\n",
      "  [N=10] Hybrid Search: Generating forward candidates...\n",
      "  [N=10] Added 3 forward candidates.\n",
      "  [N=10] Best candidate score: 0.361000\n",
      "  [N=10] Baseline score: 0.381910, Candidate score: 0.361000\n",
      "  [N=10] ✓ IMPROVED over baseline: 0.381910 -> 0.361000 (Δ=0.020910, 5.48%)\n",
      "⚠️  Collision at N=10 between trees 0 and 1 (overlap area=6.085227e-02)\n",
      "  [N=10] Found 16 collision(s)\n",
      "  [N=10] Too many collisions - reverting to baseline\n",
      "  [N=10] Cumulative Score: 39.070651 | Total Improvements: 191\n",
      "\n",
      "--- Processing N=9 ---\n",
      "  [N=9] Launching 105 parallel tasks...\n",
      "  [N=9] Hybrid Search: Generating forward candidates...\n",
      "  [N=9] Added 3 forward candidates.\n",
      "  [N=9] Best candidate score: 0.340278\n",
      "  [N=9] Baseline score: 0.387562, Candidate score: 0.340278\n",
      "  [N=9] ✓ IMPROVED over baseline: 0.387562 -> 0.340278 (Δ=0.047284, 12.20%)\n",
      "  [N=9] Cumulative Score: 39.410929 | Total Improvements: 192\n",
      "\n",
      "--- Processing N=8 ---\n",
      "  [N=8] Launching 60 parallel tasks...\n",
      "  [N=8] Hybrid Search: Generating forward candidates...\n",
      "  [N=8] Added 3 forward candidates.\n",
      "  [N=8] Best candidate score: 0.382812\n",
      "  [N=8] Baseline score: 0.390416, Candidate score: 0.382812\n",
      "  [N=8] ✓ IMPROVED over baseline: 0.390416 -> 0.382812 (Δ=0.007604, 1.95%)\n",
      "  [N=8] Cumulative Score: 39.793741 | Total Improvements: 193\n",
      "\n",
      "--- Processing N=7 ---\n",
      "  [N=7] Launching 60 parallel tasks...\n",
      "  [N=7] Hybrid Search: Generating forward candidates...\n",
      "  [N=7] Added 3 forward candidates.\n",
      "  [N=7] Best candidate score: 0.354375\n",
      "  [N=7] Baseline score: 0.400226, Candidate score: 0.354375\n",
      "  [N=7] ✓ IMPROVED over baseline: 0.400226 -> 0.354375 (Δ=0.045851, 11.46%)\n",
      "  [N=7] Cumulative Score: 40.148116 | Total Improvements: 194\n",
      "\n",
      "--- Processing N=6 ---\n",
      "  [N=6] Launching 60 parallel tasks...\n",
      "  [N=6] Hybrid Search: Generating forward candidates...\n",
      "  [N=6] Added 3 forward candidates.\n",
      "  [N=6] Best candidate score: 0.292604\n",
      "  [N=6] Baseline score: 0.400036, Candidate score: 0.292604\n",
      "  [N=6] ✓ IMPROVED over baseline: 0.400036 -> 0.292604 (Δ=0.107432, 26.86%)\n",
      "  [N=6] Cumulative Score: 40.440721 | Total Improvements: 195\n",
      "\n",
      "--- Processing N=5 ---\n",
      "  [N=5] Critical N! Widening beam to 24\n",
      "  [N=5] Launching 60 parallel tasks...\n",
      "  [N=5] Hybrid Search: Generating forward candidates...\n",
      "  [N=5] Added 3 forward candidates.\n",
      "  [N=5] Best candidate score: 0.288000\n",
      "  [N=5] Baseline score: 0.417065, Candidate score: 0.288000\n",
      "  [N=5] ✓ IMPROVED over baseline: 0.417065 -> 0.288000 (Δ=0.129065, 30.95%)\n",
      "  [N=5] Cumulative Score: 40.728721 | Total Improvements: 196\n",
      "\n",
      "--- Processing N=4 ---\n",
      "  [N=4] Critical N! Widening beam to 24\n",
      "  [N=4] Launching 110 parallel tasks...\n",
      "  [N=4] Hybrid Search: Generating forward candidates...\n",
      "  [N=4] Added 3 forward candidates.\n",
      "  [N=4] Best candidate score: 0.330625\n",
      "  [N=4] Baseline score: 0.416635, Candidate score: 0.330625\n",
      "  [N=4] ✓ IMPROVED over baseline: 0.416635 -> 0.330625 (Δ=0.086010, 20.64%)\n",
      "  [N=4] Cumulative Score: 41.059346 | Total Improvements: 197\n",
      "\n",
      "--- Processing N=3 ---\n",
      "  [N=3] Critical N! Widening beam to 24\n",
      "  [N=3] Launching 120 parallel tasks...\n",
      "  [N=3] Hybrid Search: Generating forward candidates...\n",
      "  [N=3] Added 3 forward candidates.\n",
      "  [N=3] Best candidate score: 0.440833\n",
      "  [N=3] Baseline score: 0.434745, Candidate score: 0.440833\n",
      "  [N=3] Using baseline (better than candidates)\n",
      "  [N=3] Cumulative Score: 41.494091 | Total Improvements: 197\n",
      "\n",
      "--- Processing N=2 ---\n",
      "  [N=2] Critical N! Widening beam to 24\n",
      "  [N=2] Launching 120 parallel tasks...\n",
      "  [N=2] Hybrid Search: Generating forward candidates...\n",
      "  [N=2] Added 3 forward candidates.\n",
      "  [N=2] Best candidate score: 0.500000\n",
      "  [N=2] Baseline score: 0.450779, Candidate score: 0.500000\n",
      "  [N=2] Using baseline (better than candidates)\n",
      "  [N=2] Cumulative Score: 41.944870 | Total Improvements: 197\n",
      "\n",
      "--- Processing N=1 ---\n",
      "  [N=1] Critical N! Widening beam to 24\n",
      "  [N=1] Launching 120 parallel tasks...\n",
      "  [N=1] Hybrid Search: Generating forward candidates...\n",
      "  [N=1] Added 3 forward candidates.\n",
      "  [N=1] Best candidate score: 0.661251\n",
      "  [N=1] Baseline score: 0.661250, Candidate score: 0.661251\n",
      "  [N=1] Using baseline (better than candidates)\n",
      "  [N=1] Cumulative Score: 42.606120 | Total Improvements: 197\n",
      "\n",
      "============================================================\n",
      "Processing complete!\n",
      "Total improvements over baseline: 197\n",
      "Final cumulative score: 42.606120\n",
      "============================================================\n",
      "\n",
      "✓ Submission generated: submission.csv\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Load baseline from test.csv\n",
    "known_solutions = {}\n",
    "try:\n",
    "    print(\"Loading baseline from test.csv...\")\n",
    "    baseline_df = pd.read_csv('test.csv')\n",
    "    \n",
    "    def parse_s(val):\n",
    "        return float(str(val).replace('s', ''))\n",
    "    \n",
    "    baseline_df['x'] = baseline_df['x'].apply(parse_s)\n",
    "    baseline_df['y'] = baseline_df['y'].apply(parse_s)\n",
    "    baseline_df['deg'] = baseline_df['deg'].apply(parse_s)\n",
    "    \n",
    "    for n in range(1, 201):\n",
    "        prefix = f\"{n:03d}_\"\n",
    "        rows = baseline_df[baseline_df['id'].str.startswith(prefix)]\n",
    "        if len(rows) == n:\n",
    "            trees = []\n",
    "            for _, row in rows.iterrows():\n",
    "                trees.append(ChristmasTree(row['x'], row['y'], row['deg']))\n",
    "            known_solutions[n] = trees\n",
    "            \n",
    "    print(f\"Loaded {len(known_solutions)} configurations from test.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load baseline: {e}\")\n",
    "\n",
    "# Helper function for parallel execution (REVERSE MODE) - OPTIMIZED\n",
    "def process_beam_candidate_reverse(base_trees, n, packer_params, target_side=None, remove_idx=None, parent_id=None, task_id=0):\n",
    "    \"\"\"\n",
    "    Takes a solution of size >= n.\n",
    "    If size > n, removes trees to reach size n.\n",
    "    Then optimizes.\n",
    "    OPTIMIZED: Reduced tree copying and faster gravity passes.\n",
    "    \"\"\"\n",
    "    # Re-seed random for this process\n",
    "    seed_val = (int(time.time() * 1000000) + os.getpid() + task_id) % (2**32)\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    \n",
    "    # Shallow copy first - only deep copy if we need to modify\n",
    "    current_trees = list(base_trees)\n",
    "    \n",
    "    # Remove trees if needed (Shrink strategy)\n",
    "    # Only create new objects when we actually remove trees\n",
    "    needs_copy = len(current_trees) > n\n",
    "    if needs_copy:\n",
    "        current_trees = [ChristmasTree(t.center_x, t.center_y, t.angle) for t in current_trees]\n",
    "        \n",
    "    removal_count = 0\n",
    "    while len(current_trees) > n:\n",
    "        if remove_idx is not None and remove_idx < len(current_trees):\n",
    "            # Remove specific tree requested\n",
    "            current_trees.pop(remove_idx)\n",
    "            remove_idx = None # Only use once\n",
    "        else:\n",
    "            # Fallback: Remove a tree that contributes to the bounds (Outliers)\n",
    "            # Calculate global bounds\n",
    "            union = unary_union([t.polygon for t in current_trees])\n",
    "            minx, miny, maxx, maxy = union.bounds\n",
    "            \n",
    "            candidates_to_remove = []\n",
    "            for i, t in enumerate(current_trees):\n",
    "                tb = t.polygon.bounds\n",
    "                # Check if touching global bounds (Contribution to Envelope)\n",
    "                if (abs(tb[0] - minx) < 1e-3 or abs(tb[1] - miny) < 1e-3 or \n",
    "                    abs(tb[2] - maxx) < 1e-3 or abs(tb[3] - maxy) < 1e-3):\n",
    "                    candidates_to_remove.append(i)\n",
    "            \n",
    "            if candidates_to_remove:\n",
    "                idx = random.choice(candidates_to_remove)\n",
    "            else:\n",
    "                idx = random.randint(0, len(current_trees) - 1)\n",
    "            current_trees.pop(idx)\n",
    "            removal_count += 1\n",
    "            \n",
    "            # Gravity Pass after removal - less frequent and shorter\n",
    "            # Only every 10 removals instead of every 5\n",
    "            if removal_count % 10 == 0:\n",
    "                current_trees = optimize_packing(current_trees, {\n",
    "                    'iterations': 2000,  # Reduced from 3000\n",
    "                    'step_size': 0.5,\n",
    "                    'angle_step': 5.0,\n",
    "                    'initial_temp': 0.1,\n",
    "                    'compression': 0.5\n",
    "                })\n",
    "            \n",
    "    # ADAPTIVE OPTIMIZATION based on N (Bucketed)\n",
    "    params = get_sa_params(n)\n",
    "    \n",
    "    candidate_trees = optimize_packing(current_trees, params, target_side=target_side)\n",
    "        \n",
    "    # Score Candidate\n",
    "    side_candidate = get_bounds(candidate_trees)\n",
    "    score_candidate = (side_candidate ** 2) / n\n",
    "    \n",
    "    return (score_candidate, candidate_trees, parent_id)\n",
    "\n",
    "# Helper for Forward Search (Hybrid)\n",
    "def process_forward_candidate(n, packer_params, task_id=0):\n",
    "    # Fix: Ensure seed is within 32-bit range for numpy\n",
    "    seed_val = (int(time.time() * 1000000) + os.getpid() + task_id) % (2**32)\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    \n",
    "    packer = GreedyPacker(**packer_params)\n",
    "    \n",
    "    # Start with one tree\n",
    "    trees = [ChristmasTree(0, 0, 0)]\n",
    "    \n",
    "    for _ in range(n - 1):\n",
    "        next_tree = packer.place_next_tree(trees, ChristmasTree)\n",
    "        trees.append(next_tree)\n",
    "        \n",
    "    # Optimize\n",
    "    params = get_sa_params(n)\n",
    "    trees = optimize_packing(trees, params)\n",
    "    \n",
    "    side = get_bounds(trees)\n",
    "    score = (side ** 2) / n\n",
    "    return (score, trees, \"forward\")\n",
    "\n",
    "# BEAM SEARCH PARAMETERS - OPTIMIZED\n",
    "BASE_BEAM_WIDTH = CONFIG['dynamic_beam_width']['base_width']\n",
    "BRANCH_FACTOR = 5     # Reduced from 8 to 5 for speed\n",
    "n_jobs = multiprocessing.cpu_count()\n",
    "print(f\"Running Optimized Advanced Search (200 -> 1) on {n_jobs} cores.\")\n",
    "print(f\"Beam width: {BASE_BEAM_WIDTH}, Branch factor: {BRANCH_FACTOR}\")\n",
    "\n",
    "packer_params = {'n_trials': 200, 'step_size': 0.2, 'fine_step': 0.01}\n",
    "submission_rows = []\n",
    "improvements = 0\n",
    "all_solutions = {}\n",
    "\n",
    "# Initialize candidates with Lattice for N=200\n",
    "print(\"\\nInitializing with Lattice Generator for N=200.\")\n",
    "lattice_200 = generate_lattice(200)\n",
    "# Optimize the lattice initially\n",
    "print(\"Optimizing initial lattice...\")\n",
    "lattice_200 = optimize_packing(lattice_200, get_sa_params(200))\n",
    "current_candidates = [(lattice_200, \"lattice\")]\n",
    "\n",
    "# Metrics Tracking\n",
    "history_n = []\n",
    "history_improvement = []\n",
    "history_total_score = []\n",
    "current_total_score = 0\n",
    "\n",
    "# Setup Metrics File\n",
    "os.makedirs('Data', exist_ok=True)\n",
    "timestamp_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "metrics_file = f'Data/run_metrics_advanced_{timestamp_str}.csv'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    f.write('n,score,baseline,improvement,source\\n')\n",
    "\n",
    "# Setup Realtime Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "plt.close(fig) # Don't show yet\n",
    "plot_display_id = \"metrics_plot_advanced\"\n",
    "display(fig, display_id=plot_display_id) # Show initial empty plot\n",
    "\n",
    "# REVERSE LOOP: 200 down to 1\n",
    "for n in tqdm(range(200, 0, -1), desc=\"Processing Reverse\"):\n",
    "    \n",
    "    print(f\"\\n--- Processing N={n} ---\")\n",
    "\n",
    "    # Dynamic Beam Width\n",
    "    current_beam_width = BASE_BEAM_WIDTH\n",
    "    if CONFIG['dynamic_beam_width']['enabled']:\n",
    "        # Critical N values: 100, 50, 25, 10\n",
    "        if n in [100, 50, 25, 10] or n <= 5:\n",
    "            current_beam_width = int(BASE_BEAM_WIDTH * CONFIG['dynamic_beam_width']['critical_n_multiplier'])\n",
    "            print(f\"  [N={n}] Critical N! Widening beam to {current_beam_width}\")\n",
    "\n",
    "    # Get baseline side for pruning/comparison\n",
    "    baseline_side = None\n",
    "    if n in known_solutions:\n",
    "        baseline_side = get_bounds(known_solutions[n])\n",
    "    \n",
    "    # 1. Expand candidates in parallel\n",
    "    tasks = []\n",
    "    task_counter = 0\n",
    "    # Enumerate candidates to assign parent IDs\n",
    "    for p_idx, (base_trees, _) in enumerate(current_candidates):\n",
    "        # Identify boundary trees for intelligent removal\n",
    "        minx, miny, maxx, maxy = unary_union([t.polygon for t in base_trees]).bounds\n",
    "        boundary_indices = []\n",
    "        for i, t in enumerate(base_trees):\n",
    "            tb = t.polygon.bounds\n",
    "            # Check if touching global bounds\n",
    "            if (abs(tb[0] - minx) < 1e-2 or abs(tb[1] - miny) < 1e-2 or \n",
    "                abs(tb[2] - maxx) < 1e-2 or abs(tb[3] - maxy) < 1e-2):\n",
    "                boundary_indices.append(i)\n",
    "        \n",
    "        # If we have enough boundary trees, pick distinct ones. If not, fill with randoms.\n",
    "        indices_to_try = boundary_indices[:BRANCH_FACTOR]\n",
    "        while len(indices_to_try) < BRANCH_FACTOR:\n",
    "            indices_to_try.append(None) # None triggers random selection inside function\n",
    "            \n",
    "        for idx in indices_to_try:\n",
    "            # Pass p_idx as parent_id\n",
    "            tasks.append((base_trees, n, packer_params, baseline_side, idx, p_idx, task_counter))\n",
    "            task_counter += 1\n",
    "            \n",
    "    print(f\"  [N={n}] Launching {len(tasks)} parallel tasks...\")\n",
    "    \n",
    "    # Run in parallel\n",
    "    results = []\n",
    "    try:\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_beam_candidate_reverse)(t[0], t[1], t[2], t[3], t[4], t[5], t[6]) for t in tasks\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"  [N={n}] ERROR in parallel execution: {e}\")\n",
    "        if n in known_solutions:\n",
    "            results = [((get_bounds(known_solutions[n])**2)/n, known_solutions[n], \"fallback\")]\n",
    "    \n",
    "    # Hybrid Search: Inject Forward Candidates\n",
    "    if CONFIG['hybrid_search']['enabled'] and n <= CONFIG['hybrid_search']['switch_n']:\n",
    "        print(f\"  [N={n}] Hybrid Search: Generating forward candidates...\")\n",
    "        # Generate a few forward candidates\n",
    "        forward_tasks = [(n, packer_params, i) for i in range(3)] # Reduced from 4 to 3\n",
    "        try:\n",
    "            forward_results = Parallel(n_jobs=n_jobs)(\n",
    "                delayed(process_forward_candidate)(t[0], t[1], t[2]) for t in forward_tasks\n",
    "            )\n",
    "            results.extend(forward_results)\n",
    "            print(f\"  [N={n}] Added {len(forward_results)} forward candidates.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [N={n}] Forward search failed: {e}\")\n",
    "\n",
    "    # 2. Sort and select top K (Beam Selection with Diversity)\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    if results:\n",
    "        print(f\"  [N={n}] Best candidate score: {results[0][0]:.6f}\")\n",
    "    else:\n",
    "        print(f\"  [N={n}] No results found!\")\n",
    "        continue\n",
    "    \n",
    "    # Update current candidates with diversity check\n",
    "    unique_candidates = []\n",
    "    seen_scores = set()\n",
    "    parent_counts = {} # Track how many children from each parent\n",
    "    \n",
    "    max_children = CONFIG['beam_diversity']['max_children_per_parent'] if CONFIG['beam_diversity']['enabled'] else 999\n",
    "    \n",
    "    for res in results:\n",
    "        score = round(res[0], 6)\n",
    "        trees = res[1]\n",
    "        pid = res[2]\n",
    "        \n",
    "        # Diversity Check 1: Score Uniqueness\n",
    "        if score in seen_scores:\n",
    "            continue\n",
    "            \n",
    "        # Diversity Check 2: Parent Limit\n",
    "        p_count = parent_counts.get(pid, 0)\n",
    "        if p_count >= max_children:\n",
    "            continue\n",
    "            \n",
    "        unique_candidates.append((trees, pid))\n",
    "        seen_scores.add(score)\n",
    "        parent_counts[pid] = p_count + 1\n",
    "        \n",
    "        if len(unique_candidates) >= current_beam_width:\n",
    "            break\n",
    "            \n",
    "    current_candidates = unique_candidates\n",
    "    \n",
    "    # The best candidate for this N\n",
    "    best_score_greedy = results[0][0]\n",
    "    best_trees_greedy = results[0][1]\n",
    "    \n",
    "    # 3. Compare with Baseline - FIXED LOGIC\n",
    "    best_trees_n = best_trees_greedy\n",
    "    best_score_n = best_score_greedy\n",
    "    source = \"ReverseGreedy\"\n",
    "    baseline_val = 0\n",
    "    \n",
    "    if n in known_solutions:\n",
    "        known_trees = known_solutions[n]\n",
    "        # Don't modify the original baseline\n",
    "        known_trees_copy = [ChristmasTree(t.center_x, t.center_y, t.angle) for t in known_trees]\n",
    "        known_trees_copy = center_packing(known_trees_copy)\n",
    "        side_known = get_bounds(known_trees_copy)\n",
    "        score_known = (side_known ** 2) / n\n",
    "        baseline_val = score_known\n",
    "        \n",
    "        print(f\"  [N={n}] Baseline score: {score_known:.6f}, Candidate score: {best_score_greedy:.6f}\")\n",
    "\n",
    "        # Compare (Lower is better)\n",
    "        if score_known < best_score_greedy - 1e-7:\n",
    "            # Baseline is better - use it and try to optimize\n",
    "            best_trees_n = known_trees_copy\n",
    "            best_score_n = score_known\n",
    "            source = \"Baseline\"\n",
    "            print(f\"  [N={n}] Using baseline (better than candidates)\")\n",
    "            \n",
    "            # Try to optimize baseline\n",
    "            optimized_known = [ChristmasTree(t.center_x, t.center_y, t.angle) for t in known_trees]\n",
    "            params = get_sa_params(n)\n",
    "            params['iterations'] = int(params['iterations'] * 1.5)\n",
    "            \n",
    "            optimized_known = optimize_packing(optimized_known, params)\n",
    "            \n",
    "            side_opt = get_bounds(optimized_known)\n",
    "            score_opt = (side_opt ** 2) / n\n",
    "            \n",
    "            if score_opt < score_known - 1e-7:\n",
    "                best_trees_n = optimized_known\n",
    "                best_score_n = score_opt\n",
    "                source = \"Baseline+Opt\"\n",
    "                improvements += 1\n",
    "                print(f\"  [N={n}] ✓ Optimized baseline: {score_known:.6f} -> {score_opt:.6f} (Δ={score_known-score_opt:.6f})\")\n",
    "            \n",
    "            # Inject baseline into beam\n",
    "            current_candidates.append((best_trees_n, \"baseline\"))\n",
    "            temp_candidates = []\n",
    "            for c, pid in current_candidates:\n",
    "                s = get_bounds(c)\n",
    "                temp_candidates.append(((s**2)/n, c, pid))\n",
    "            temp_candidates.sort(key=lambda x: x[0])\n",
    "            current_candidates = [(x[1], x[2]) for x in temp_candidates[:current_beam_width]]\n",
    "            \n",
    "        elif best_score_greedy < score_known - 1e-7:\n",
    "            # Our candidate beat the baseline\n",
    "            improvements += 1\n",
    "            delta = score_known - best_score_greedy\n",
    "            pct_improvement = (delta / score_known) * 100\n",
    "            print(f\"  [N={n}] ✓ IMPROVED over baseline: {score_known:.6f} -> {best_score_greedy:.6f} (Δ={delta:.6f}, {pct_improvement:.2f}%)\")\n",
    "        else:\n",
    "            # Scores are essentially equal\n",
    "            print(f\"  [N={n}] Matched baseline (no improvement)\")\n",
    "    else:\n",
    "        print(f\"  [N={n}] No baseline available, using candidate\")\n",
    "            \n",
    "    # 4. Update State & Check Bounds\n",
    "    all_solutions[n] = best_trees_n\n",
    "    \n",
    "    # Validation - OPTIMIZED (only check periodically)\n",
    "    if CONFIG['validation']['strict_boundary']:\n",
    "        for t in best_trees_n:\n",
    "            if abs(t.center_x) > 100 or abs(t.center_y) > 100:\n",
    "                print(f\"⚠️  WARNING: Tree at N={n} outside bounds: ({t.center_x:.2f}, {t.center_y:.2f})\")\n",
    "    \n",
    "    # Collision check - OPTIMIZED (only every 10th N and using STRtree for speed)\n",
    "    if CONFIG['validation']['collision_check'] and n % 10 == 0:\n",
    "        polys = [t.polygon for t in best_trees_n]\n",
    "        tree_index = STRtree(polys)\n",
    "        collision_count = 0\n",
    "        for i, poly in enumerate(polys):\n",
    "            candidates = tree_index.query(poly)\n",
    "            for j in candidates:\n",
    "                if j > i and polys[i].intersects(polys[j]) and not polys[i].touches(polys[j]):\n",
    "                    area = polys[i].intersection(polys[j]).area\n",
    "                    if area > 1e-5:  # Only count significant overlaps\n",
    "                        collision_count += 1\n",
    "                        if collision_count == 1:\n",
    "                            print(f\"⚠️  Collision at N={n} between trees {i} and {j} (overlap area={area:.6e})\")\n",
    "        \n",
    "        if collision_count > 0:\n",
    "            print(f\"  [N={n}] Found {collision_count} collision(s)\")\n",
    "            # If significant collisions found, use baseline instead\n",
    "            if collision_count > 5 and n in known_solutions:\n",
    "                print(f\"  [N={n}] Too many collisions - reverting to baseline\")\n",
    "                best_trees_n = known_solutions[n]\n",
    "                best_score_n = (get_bounds(best_trees_n) ** 2) / n\n",
    "                source = \"Baseline_CollisionFallback\"\n",
    "\n",
    "    # 5. Metrics & Plotting\n",
    "    current_total_score += best_score_n\n",
    "    imp = max(0, baseline_val - best_score_n) if baseline_val > 0 else 0\n",
    "    \n",
    "    print(f\"  [N={n}] Cumulative Score: {current_total_score:.6f} | Total Improvements: {improvements}\")\n",
    "\n",
    "    history_n.append(n)\n",
    "    history_improvement.append(imp)\n",
    "    history_total_score.append(current_total_score)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(metrics_file, 'a') as f:\n",
    "        f.write(f\"{n},{best_score_n:.10f},{baseline_val:.10f},{imp:.10f},{source}\\n\")\n",
    "        \n",
    "    # Realtime Plot (Update every 5 steps)\n",
    "    if n % 5 == 0 or n == 1:\n",
    "        # Update axes\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        \n",
    "        # Note: history_n is decreasing [200, 199, ...]\n",
    "        ax1.plot(history_n, history_improvement, 'g-', label='Improvement')\n",
    "        ax1.set_title(f'Improvement per N (Total: {improvements})')\n",
    "        ax1.set_xlabel('N')\n",
    "        ax1.set_ylabel('Score Reduction')\n",
    "        ax1.invert_xaxis() # Invert X axis to show 200 -> 1 flow\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.plot(history_n, history_total_score, 'b-', label='Total Score')\n",
    "        ax2.set_title(f'Cumulative Score: {current_total_score:.4f}')\n",
    "        ax2.set_xlabel('N')\n",
    "        ax2.set_ylabel('Total Score')\n",
    "        ax2.invert_xaxis()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Update the display\n",
    "        display(fig, display_id=plot_display_id, update=True)\n",
    "    \n",
    "    # 6. Prepare Submission Rows\n",
    "    for i, tree in enumerate(best_trees_n):\n",
    "        submission_rows.append([\n",
    "            f\"{n:03d}_{i}\", \n",
    "            f\"s{tree.center_x:.10f}\", \n",
    "            f\"s{tree.center_y:.10f}\", \n",
    "            f\"s{tree.angle:.10f}\"\n",
    "        ])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Processing complete!\")\n",
    "print(f\"Total improvements over baseline: {improvements}\")\n",
    "print(f\"Final cumulative score: {current_total_score:.6f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "df_sub = pd.DataFrame(submission_rows, columns=['id', 'x', 'y', 'deg'])\n",
    "# Sort by ID to ensure 001_... comes first\n",
    "df_sub.sort_values('id', inplace=True)\n",
    "df_sub.to_csv('submission.csv', index=False)\n",
    "print(\"\\n✓ Submission generated: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a5fc2",
   "metadata": {},
   "source": [
    "## 10. Evaluation Helper\n",
    "\n",
    "Calculate the local score to estimate leaderboard performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87b62dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Polishing Phase...\n",
      "Loaded submission.csv for polishing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fa61f0c458425a8f4b4f8d2faa0bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Polishing:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polishing Complete.\n",
      "Score Before: 42.606120\n",
      "Score After:  42.599180\n",
      "Improvement:  0.006940\n",
      "Saved polished submission to submission.csv\n"
     ]
    }
   ],
   "source": [
    "## 9. Polishing & Refinement\n",
    "# Run this cell to further optimize the existing submission using the new Numba engine.\n",
    "# This is much faster than the full search and can squeeze out extra points.\n",
    "\n",
    "print(\"Starting Polishing Phase...\")\n",
    "try:\n",
    "    # Load current best submission\n",
    "    if os.path.exists('submission.csv'):\n",
    "        polish_df = pd.read_csv('submission.csv')\n",
    "        print(\"Loaded submission.csv for polishing.\")\n",
    "    else:\n",
    "        polish_df = pd.read_csv('test.csv')\n",
    "        print(\"Loaded test.csv for polishing.\")\n",
    "\n",
    "    def parse_s(val):\n",
    "        return float(str(val).replace('s', ''))\n",
    "    \n",
    "    polish_df['x'] = polish_df['x'].apply(parse_s)\n",
    "    polish_df['y'] = polish_df['y'].apply(parse_s)\n",
    "    polish_df['deg'] = polish_df['deg'].apply(parse_s)\n",
    "    \n",
    "    polish_solutions = {}\n",
    "    for n in range(1, 201):\n",
    "        prefix = f\"{n:03d}_\"\n",
    "        rows = polish_df[polish_df['id'].str.startswith(prefix)]\n",
    "        if len(rows) == n:\n",
    "            trees = []\n",
    "            for _, row in rows.iterrows():\n",
    "                trees.append(ChristmasTree(row['x'], row['y'], row['deg']))\n",
    "            polish_solutions[n] = trees\n",
    "\n",
    "    total_score_before = 0\n",
    "    total_score_after = 0\n",
    "    \n",
    "    # Polish each N\n",
    "    for n in tqdm(range(1, 201), desc=\"Polishing\"):\n",
    "        if n not in polish_solutions: continue\n",
    "        \n",
    "        trees = polish_solutions[n]\n",
    "        side_before = get_bounds(trees)\n",
    "        score_before = (side_before ** 2) / n\n",
    "        total_score_before += score_before\n",
    "        \n",
    "        # Run Numba Optimization with Bucketed Params\n",
    "        params = get_sa_params(n).copy()\n",
    "        \n",
    "        # Boost iterations for polishing\n",
    "        # We want a very thorough local search\n",
    "        params['iterations'] = 200000 if n < 50 else 500000\n",
    "        \n",
    "        # Reduce initial temperature for polishing (we are already close to a good solution)\n",
    "        params['initial_temp'] = params['initial_temp'] * 0.5\n",
    "        \n",
    "        optimized_trees = optimize_packing(trees, params)\n",
    "        \n",
    "        side_after = get_bounds(optimized_trees)\n",
    "        score_after = (side_after ** 2) / n\n",
    "        \n",
    "        if score_after < score_before - 1e-9:\n",
    "            polish_solutions[n] = optimized_trees\n",
    "            total_score_after += score_after\n",
    "            # print(f\"  N={n} Improved: {score_before:.6f} -> {score_after:.6f}\")\n",
    "        else:\n",
    "            total_score_after += score_before # Keep original\n",
    "            \n",
    "    print(f\"Polishing Complete.\")\n",
    "    print(f\"Score Before: {total_score_before:.6f}\")\n",
    "    print(f\"Score After:  {total_score_after:.6f}\")\n",
    "    print(f\"Improvement:  {total_score_before - total_score_after:.6f}\")\n",
    "    \n",
    "    # Save if improved\n",
    "    if total_score_after < total_score_before:\n",
    "        new_rows = []\n",
    "        for n in range(1, 201):\n",
    "            if n in polish_solutions:\n",
    "                for i, tree in enumerate(polish_solutions[n]):\n",
    "                    new_rows.append([\n",
    "                        f\"{n:03d}_{i}\", \n",
    "                        f\"s{tree.center_x:.10f}\", \n",
    "                        f\"s{tree.center_y:.10f}\", \n",
    "                        f\"s{tree.angle:.10f}\"\n",
    "                    ])\n",
    "        \n",
    "        df_polished = pd.DataFrame(new_rows, columns=['id', 'x', 'y', 'deg'])\n",
    "        df_polished.sort_values('id', inplace=True)\n",
    "        df_polished.to_csv('submission_polished.csv', index=False)\n",
    "        df_polished.to_csv('submission.csv', index=False) # Overwrite main\n",
    "        print(\"Saved polished submission to submission.csv\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Polishing failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1baa5",
   "metadata": {},
   "source": [
    "## NEW: Worst-N Refinement Strategy\n",
    "The scoring metric is Σ(s²/n). Small N with bad packing hurt disproportionately.\n",
    "This cell identifies the worst performing N values and runs intensive optimization on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ae34447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization_utils import sa_numba_growing\n",
    "import numpy as np\n",
    "\n",
    "def refine_worst_n(all_solutions, top_k=30, seeds=20, iterations=100000):\n",
    "    \"\"\"Refine worst-performing N values based on Σ(s^2/n) contribution.\n",
    "    Uses normalized SA params to avoid KeyError and multi-seed Growing Trees.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\\nWORST-N REFINEMENT (top_k={top_k}, seeds={seeds}, iter={iterations})\\n{'='*50}\")\n",
    "    if not all_solutions:\n",
    "        print(\"No solutions to refine.\")\n",
    "        return all_solutions\n",
    "\n",
    "    # Build contribution table\n",
    "    contrib_rows = []\n",
    "    for n, trees in all_solutions.items():\n",
    "        side = get_bounds(trees)\n",
    "        contrib = (side * side) / n\n",
    "        contrib_rows.append((n, side, contrib))\n",
    "    arr = np.array(contrib_rows, dtype=np.float64)\n",
    "    worst_idxs = np.argsort(arr[:, 2])[::-1][:top_k]\n",
    "    worst_list = arr[worst_idxs, 0].astype(int).tolist()\n",
    "\n",
    "    print(\"Worst N selection:\")\n",
    "    for rank, idx in enumerate(worst_idxs, 1):\n",
    "        n_val = int(arr[idx, 0]); s_val = arr[idx, 1]; c_val = arr[idx, 2]\n",
    "        print(f\"  {rank:2d}. N={n_val:3d} side={s_val:.5f} contrib={c_val:.5f}\")\n",
    "\n",
    "    refined = dict(all_solutions)\n",
    "    improvements = 0\n",
    "\n",
    "    for n in tqdm(worst_list, desc=\"Refining\"):\n",
    "        trees = refined[n]\n",
    "        base_side = get_bounds(trees)\n",
    "        xs = np.fromiter((t.center_x for t in trees), dtype=np.float64, count=len(trees))\n",
    "        ys = np.fromiter((t.center_y for t in trees), dtype=np.float64, count=len(trees))\n",
    "        angs = np.fromiter((t.angle for t in trees), dtype=np.float64, count=len(trees))\n",
    "\n",
    "        raw_params = get_sa_params(n)\n",
    "        p = normalize_sa_params(raw_params)\n",
    "        T0, Tmin = p['T0'], p['Tmin']\n",
    "        move_scale, rot_scale = p['move_scale'], p['rot_scale']\n",
    "        compression = p['compression']\n",
    "        iter_use = iterations or p['iterations']\n",
    "\n",
    "        best_side = base_side\n",
    "        best_bxs, best_bys, best_bangs = xs, ys, angs\n",
    "\n",
    "        for seed in range(seeds):\n",
    "            bxs, bys, bangs, side_candidate = sa_numba_growing(\n",
    "                xs, ys, angs, n,\n",
    "                iterations=iter_use,\n",
    "                T0=T0,\n",
    "                Tmin=Tmin,\n",
    "                move_scale=move_scale,\n",
    "                rot_scale=rot_scale,\n",
    "                seed=seed + 12345,\n",
    "                compression=compression\n",
    "            )\n",
    "            if side_candidate < best_side - 1e-12:\n",
    "                best_side = side_candidate\n",
    "                best_bxs, best_bys, best_bangs = bxs.copy(), bys.copy(), bangs.copy()\n",
    "\n",
    "        if best_side < base_side - 1e-9:\n",
    "            refined[n] = [ChristmasTree(best_bxs[i], best_bys[i], best_bangs[i]) for i in range(n)]\n",
    "            improvements += 1\n",
    "            pct = 100.0 * (base_side - best_side) / base_side\n",
    "            print(f\"  ✓ N={n:3d} improved {base_side:.5f} -> {best_side:.5f} (-{pct:.2f}%)\")\n",
    "\n",
    "    print(f\"Refinement complete. Improved {improvements}/{len(worst_list)} targets.\")\n",
    "    return refined\n",
    "\n",
    "# Example:\n",
    "# all_solutions = refine_worst_n(all_solutions, top_k=30, seeds=20, iterations=80000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b919c9d",
   "metadata": {},
   "source": [
    "## NEW: Forward Chain Search\n",
    "Instead of only using Reverse Beam (200→1), also build solutions forward (1→200).\n",
    "This helps with tricky N values (primes, etc.) that don't fit well as subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6934600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_chain_search(max_n=200, seeds_per_n=3, iterations=30000):\n",
    "    \"\"\"Build solutions forward 1..max_n, inserting one tree then optimizing.\n",
    "    Uses normalized SA params and reduced redundant calculations.\"\"\"\n",
    "    print(f\"\\n{'='*50}\\nFORWARD CHAIN SEARCH (max_n={max_n})\\n{'='*50}\")\n",
    "    forward = {1: [ChristmasTree(0.0, 0.0, 0.0)]}\n",
    "\n",
    "    for n in tqdm(range(2, max_n + 1), desc=\"ForwardChain\"):\n",
    "        prev = forward[n - 1]\n",
    "        m = len(prev)\n",
    "        xs = np.fromiter((t.center_x for t in prev), dtype=np.float64, count=m)\n",
    "        ys = np.fromiter((t.center_y for t in prev), dtype=np.float64, count=m)\n",
    "        angs = np.fromiter((t.angle for t in prev), dtype=np.float64, count=m)\n",
    "\n",
    "        minx, maxx = xs.min(), xs.max()\n",
    "        miny, maxy = ys.min(), ys.max()\n",
    "        cx, cy = (minx + maxx) / 2.0, (miny + maxy) / 2.0\n",
    "\n",
    "        candidates = [\n",
    "            (maxx + 0.6, cy, np.random.rand() * 360.0),\n",
    "            (cx + np.random.randn() * 0.4, cy + np.random.randn() * 0.4, np.random.rand() * 360.0),\n",
    "            (minx - 0.5, maxy + 0.5, np.random.rand() * 360.0),\n",
    "        ]\n",
    "\n",
    "        raw_params = get_sa_params(n)\n",
    "        p = normalize_sa_params(raw_params)\n",
    "        T0, Tmin = p['T0'], p['Tmin']\n",
    "        move_scale, rot_scale = p['move_scale'], p['rot_scale']\n",
    "        compression = p['compression']\n",
    "        iter_use = iterations or p['iterations']\n",
    "\n",
    "        best_side = float('inf')\n",
    "        best_sol = None\n",
    "\n",
    "        for (ix, iy, iang) in candidates:\n",
    "            xs_new = np.concatenate([xs, [ix]])\n",
    "            ys_new = np.concatenate([ys, [iy]])\n",
    "            angs_new = np.concatenate([angs, [iang]])\n",
    "            for seed in range(seeds_per_n):\n",
    "                bxs, bys, bangs, side = sa_numba_growing(\n",
    "                    xs_new, ys_new, angs_new, n,\n",
    "                    iterations=iter_use,\n",
    "                    T0=T0, Tmin=Tmin,\n",
    "                    move_scale=move_scale, rot_scale=rot_scale,\n",
    "                    seed=seed + n * 1000 + 777,\n",
    "                    compression=compression\n",
    "                )\n",
    "                if side < best_side - 1e-12:\n",
    "                    best_side = side\n",
    "                    best_sol = (bxs.copy(), bys.copy(), bangs.copy())\n",
    "\n",
    "        if best_sol is None:\n",
    "            forward[n] = prev + [ChristmasTree(cx, cy, 0.0)]\n",
    "        else:\n",
    "            bxs, bys, bangs = best_sol\n",
    "            forward[n] = [ChristmasTree(bxs[i], bys[i], bangs[i]) for i in range(n)]\n",
    "\n",
    "    print(\"Forward chain complete.\")\n",
    "    return forward\n",
    "\n",
    "# forward_solutions = forward_chain_search(max_n=200, seeds_per_n=3, iterations=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1d2e7",
   "metadata": {},
   "source": [
    "## NEW: Solution Ensemble & Merge\n",
    "Combine results from multiple strategies (Reverse Beam, Forward Chain, Refinement).\n",
    "For each N, pick the solution with the smallest bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29ec5971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_solutions(*solution_dicts, labels=None):\n",
    "    \"\"\"Merge multiple solution dicts; keep smallest bounding box per N.\n",
    "    Adds win statistics per source.\"\"\"\n",
    "    if labels is None:\n",
    "        labels = [f\"S{i+1}\" for i in range(len(solution_dicts))]\n",
    "\n",
    "    merged = {}\n",
    "    stats = {lab: {'wins': 0, 'attempts': 0} for lab in labels}\n",
    "\n",
    "    for n in range(1, 201):\n",
    "        candidate_rows = []\n",
    "        for sol, lab in zip(solution_dicts, labels):\n",
    "            if n in sol:\n",
    "                trees = sol[n]\n",
    "                side = get_bounds(trees)\n",
    "                candidate_rows.append((side, trees, lab))\n",
    "        if not candidate_rows:\n",
    "            continue\n",
    "        # Sort by side ascending\n",
    "        candidate_rows.sort(key=lambda r: r[0])\n",
    "        best_side, best_trees, best_lab = candidate_rows[0]\n",
    "        merged[n] = best_trees\n",
    "        for side, trees, lab in candidate_rows:\n",
    "            stats[lab]['attempts'] += 1\n",
    "        stats[best_lab]['wins'] += 1\n",
    "\n",
    "    # Report\n",
    "    print(\"Merge stats:\")\n",
    "    for lab in labels:\n",
    "        att = stats[lab]['attempts']; wins = stats[lab]['wins']\n",
    "        wr = (wins / att * 100.0) if att else 0.0\n",
    "        print(f\"  {lab:15s} wins={wins:3d}/{att:3d} ({wr:5.1f}%)\")\n",
    "\n",
    "    total_score = sum((get_bounds(merged[n])**2)/n for n in merged)\n",
    "    print(f\"Merged total score: {total_score:.6f} over {len(merged)} Ns\")\n",
    "    return merged\n",
    "\n",
    "def save_solution_to_csv(solutions, filename='submission_ensemble.csv'):\n",
    "    rows = []\n",
    "    for n in range(1, 201):\n",
    "        if n in solutions:\n",
    "            for i, t in enumerate(solutions[n]):\n",
    "                rows.append([f\"{n:03d}_{i}\", f\"s{t.center_x:.10f}\", f\"s{t.center_y:.10f}\", f\"s{t.angle:.10f}\"])\n",
    "    df = pd.DataFrame(rows, columns=['id','x','y','deg']).sort_values('id')\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a887e",
   "metadata": {},
   "source": [
    "## UPDATED: Main Optimization with Growing Trees\n",
    "This cell shows how to integrate the new `sa_numba_growing` engine into your optimization workflow.\n",
    "The Growing Trees strategy uses soft physics (90% → 100% scaling) to escape local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82669178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_packing_with_growing_trees(trees, params, seeds=5):\n",
    "    \"\"\"Optimizes a list of trees with multi-seed Growing Trees using normalized params.\"\"\"\n",
    "    n = len(trees)\n",
    "    if n == 0:\n",
    "        return trees\n",
    "    xs = np.fromiter((t.center_x for t in trees), dtype=np.float64, count=n)\n",
    "    ys = np.fromiter((t.center_y for t in trees), dtype=np.float64, count=n)\n",
    "    angs = np.fromiter((t.angle for t in trees), dtype=np.float64, count=n)\n",
    "\n",
    "    p = normalize_sa_params(params or {})\n",
    "    T0, Tmin = p['T0'], p['Tmin']\n",
    "    move_scale, rot_scale = p['move_scale'], p['rot_scale']\n",
    "    compression = p['compression']\n",
    "    iterations = p['iterations']\n",
    "\n",
    "    best_side = float('inf')\n",
    "    best_state = (xs, ys, angs)\n",
    "\n",
    "    base_seed = p['base_seed']\n",
    "    ss = np.random.SeedSequence(base_seed)\n",
    "    seed_list = ss.spawn(seeds)\n",
    "\n",
    "    for child in seed_list:\n",
    "        seed = int(child.generate_state(1)[0] % (2**31 - 1))\n",
    "        bxs, bys, bangs, side = sa_numba_growing(\n",
    "            xs, ys, angs, n,\n",
    "            iterations=iterations,\n",
    "            T0=T0, Tmin=Tmin,\n",
    "            move_scale=move_scale, rot_scale=rot_scale,\n",
    "            seed=seed, compression=compression\n",
    "        )\n",
    "        if side < best_side - 1e-12:\n",
    "            best_side = side\n",
    "            best_state = (bxs.copy(), bys.copy(), bangs.copy())\n",
    "\n",
    "    bxs, bys, bangs = best_state\n",
    "    return [ChristmasTree(bxs[i], bys[i], bangs[i]) for i in range(n)]\n",
    "\n",
    "# optimized_trees = optimize_packing_with_growing_trees(candidate_trees, normalize_sa_params(get_sa_params(len(candidate_trees))), seeds=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ae47bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SANTA 2025 OPTIMIZATION PIPELINE\n",
      "======================================================================\n",
      "Loaded 200 reverse solutions. Score=39.013485\n",
      "Forward chain skipped.\n",
      "\n",
      "==================================================\n",
      "WORST-N REFINEMENT (top_k=30, seeds=20, iter=80000)\n",
      "==================================================\n",
      "Worst N selection:\n",
      "   1. N=  1 side=0.81317 contrib=0.66125\n",
      "   2. N=  2 side=0.94950 contrib=0.45078\n",
      "   3. N=  3 side=1.14203 contrib=0.43475\n",
      "   4. N=  8 side=1.75000 contrib=0.38281\n",
      "   5. N= 10 side=1.90000 contrib=0.36100\n",
      "   6. N=  7 side=1.57500 contrib=0.35438\n",
      "   7. N=  9 side=1.75000 contrib=0.34028\n",
      "   8. N=  4 side=1.15000 contrib=0.33062\n",
      "   9. N= 11 side=1.90000 contrib=0.32818\n",
      "  10. N= 12 side=1.90000 contrib=0.30083\n",
      "  11. N=  6 side=1.32500 contrib=0.29260\n",
      "  12. N=  5 side=1.20000 contrib=0.28800\n",
      "  13. N= 13 side=1.90000 contrib=0.27769\n",
      "  14. N= 18 side=2.20000 contrib=0.26889\n",
      "  15. N= 14 side=1.90000 contrib=0.25786\n",
      "  16. N= 21 side=2.32500 contrib=0.25741\n",
      "  17. N= 19 side=2.20000 contrib=0.25474\n",
      "  18. N= 15 side=1.95000 contrib=0.25350\n",
      "  19. N= 17 side=2.07500 contrib=0.25327\n",
      "  20. N= 22 side=2.32500 contrib=0.24571\n",
      "  21. N= 29 side=2.65000 contrib=0.24216\n",
      "  22. N= 20 side=2.20000 contrib=0.24200\n",
      "  23. N= 16 side=1.95000 contrib=0.23766\n",
      "  24. N= 23 side=2.32500 contrib=0.23503\n",
      "  25. N= 30 side=2.65000 contrib=0.23408\n",
      "  26. N= 32 side=2.70000 contrib=0.22781\n",
      "  27. N= 31 side=2.65000 contrib=0.22653\n",
      "  28. N= 24 side=2.32500 contrib=0.22523\n",
      "  29. N= 35 side=2.80000 contrib=0.22400\n",
      "  30. N= 33 side=2.70000 contrib=0.22091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5aab06abe1f4d47a3de109be465fd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Refining:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ N=  1 improved 0.81317 -> 0.73186 (-10.00%)\n",
      "  ✓ N=  2 improved 0.94950 -> 0.90685 (-4.49%)\n",
      "  ✓ N=  3 improved 1.14203 -> 1.08706 (-4.81%)\n",
      "  ✓ N=  8 improved 1.75000 -> 1.71000 (-2.29%)\n",
      "  ✓ N= 10 improved 1.90000 -> 1.80000 (-5.26%)\n",
      "  ✓ N=  7 improved 1.57500 -> 1.50500 (-4.44%)\n",
      "  ✓ N=  9 improved 1.75000 -> 1.71000 (-2.29%)\n",
      "  ✓ N=  4 improved 1.15000 -> 1.00500 (-12.61%)\n",
      "  ✓ N= 11 improved 1.90000 -> 1.80000 (-5.26%)\n",
      "  ✓ N= 12 improved 1.90000 -> 1.80000 (-5.26%)\n",
      "  ✓ N=  6 improved 1.32500 -> 1.25500 (-5.28%)\n",
      "  ✓ N=  5 improved 1.20000 -> 1.13000 (-5.83%)\n",
      "  ✓ N= 13 improved 1.90000 -> 1.80000 (-5.26%)\n",
      "  ✓ N= 18 improved 2.20000 -> 2.13000 (-3.18%)\n",
      "  ✓ N= 14 improved 1.90000 -> 1.80000 (-5.26%)\n",
      "  ✓ N= 21 improved 2.32500 -> 2.25500 (-3.01%)\n",
      "  ✓ N= 19 improved 2.20000 -> 2.13000 (-3.18%)\n",
      "  ✓ N= 15 improved 1.95000 -> 1.88000 (-3.59%)\n",
      "  ✓ N= 17 improved 2.07500 -> 2.00500 (-3.37%)\n",
      "  ✓ N= 22 improved 2.32500 -> 2.25500 (-3.01%)\n",
      "  ✓ N= 29 improved 2.65000 -> 2.61000 (-1.51%)\n",
      "  ✓ N= 20 improved 2.20000 -> 2.13000 (-3.18%)\n",
      "  ✓ N= 16 improved 1.95000 -> 1.88000 (-3.59%)\n",
      "  ✓ N= 23 improved 2.32500 -> 2.25500 (-3.01%)\n",
      "  ✓ N= 30 improved 2.65000 -> 2.61000 (-1.51%)\n",
      "  ✓ N= 32 improved 2.70000 -> 2.63000 (-2.59%)\n",
      "  ✓ N= 31 improved 2.65000 -> 2.61000 (-1.51%)\n",
      "  ✓ N= 24 improved 2.32500 -> 2.25500 (-3.01%)\n",
      "  ✓ N= 35 improved 2.80000 -> 2.70000 (-3.57%)\n",
      "  ✓ N= 33 improved 2.70000 -> 2.63000 (-2.59%)\n",
      "Refinement complete. Improved 30/30 targets.\n",
      "Refined score=39.013485 (time 51.5s)\n",
      "Merge stats:\n",
      "  Refined_Reverse wins=200/200 (100.0%)\n",
      "Merged total score: 39.013485 over 200 Ns\n",
      "Final merged score=39.013485\n",
      "Saved submission_ensemble.csv\n",
      "Saved submission.csv\n",
      "Pipeline time: 59.1s\n"
     ]
    }
   ],
   "source": [
    "# ========== MASTER WORKFLOW (Refactored & Robust) ==========\n",
    "import time\n",
    "WORKFLOW_CONFIG = {\n",
    "    'run_forward_chain': False,\n",
    "    'run_worst_n_refine': True,\n",
    "    'worst_n_count': 30,\n",
    "    'worst_n_seeds': 20,\n",
    "    'worst_n_iterations': 80000,\n",
    "    'forward_seeds': 3,\n",
    "    'forward_iterations': 25000,\n",
    "}\n",
    "\n",
    "start_global = time.time()\n",
    "print(f\"{'='*70}\\nSANTA 2025 OPTIMIZATION PIPELINE\\n{'='*70}\")\n",
    "\n",
    "reverse_solutions = all_solutions.copy() if 'all_solutions' in locals() else {}\n",
    "if reverse_solutions:\n",
    "    rev_score = sum((get_bounds(reverse_solutions[n])**2)/n for n in reverse_solutions)\n",
    "    print(f\"Loaded {len(reverse_solutions)} reverse solutions. Score={rev_score:.6f}\")\n",
    "else:\n",
    "    print(\"No reverse solutions found; run base search first.\")\n",
    "\n",
    "forward_solutions = {}\n",
    "if WORKFLOW_CONFIG['run_forward_chain']:\n",
    "    t0 = time.time()\n",
    "    forward_solutions = forward_chain_search(\n",
    "        max_n=200,\n",
    "        seeds_per_n=WORKFLOW_CONFIG['forward_seeds'],\n",
    "        iterations=WORKFLOW_CONFIG['forward_iterations']\n",
    "    )\n",
    "    fwd_score = sum((get_bounds(forward_solutions[n])**2)/n for n in forward_solutions)\n",
    "    print(f\"Forward chain score={fwd_score:.6f} (time {time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    print(\"Forward chain skipped.\")\n",
    "\n",
    "refined_solutions = reverse_solutions\n",
    "if WORKFLOW_CONFIG['run_worst_n_refine'] and reverse_solutions:\n",
    "    t0 = time.time()\n",
    "    refined_solutions = refine_worst_n(\n",
    "        reverse_solutions,\n",
    "        top_k=WORKFLOW_CONFIG['worst_n_count'],\n",
    "        seeds=WORKFLOW_CONFIG['worst_n_seeds'],\n",
    "        iterations=WORKFLOW_CONFIG['worst_n_iterations']\n",
    "    )\n",
    "    ref_score = sum((get_bounds(refined_solutions[n])**2)/n for n in refined_solutions)\n",
    "    print(f\"Refined score={ref_score:.6f} (time {time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    print(\"Worst-N refinement skipped.\")\n",
    "\n",
    "merge_inputs = [refined_solutions]\n",
    "merge_labels = [\"Refined_Reverse\"]\n",
    "if forward_solutions:\n",
    "    merge_inputs.append(forward_solutions)\n",
    "    merge_labels.append(\"Forward\")\n",
    "final_solutions = merge_solutions(*merge_inputs, labels=merge_labels)\n",
    "final_score = sum((get_bounds(final_solutions[n])**2)/n for n in final_solutions)\n",
    "print(f\"Final merged score={final_score:.6f}\")\n",
    "\n",
    "save_solution_to_csv(final_solutions, 'submission_ensemble.csv')\n",
    "save_solution_to_csv(final_solutions, 'submission.csv')\n",
    "all_solutions = final_solutions\n",
    "print(f\"Pipeline time: {time.time()-start_global:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb4f67",
   "metadata": {},
   "source": [
    "## Quick Test: Verify Growing Trees Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d3785a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Growing Trees optimization engine...\n",
      "--------------------------------------------------\n",
      "✓ Module loaded successfully\n",
      "✓ Test case created: 5 trees\n",
      "✓ Initial bounding box side: 3.150000\n",
      "\n",
      "Running sa_numba_growing (10k iterations)...\n",
      "✓ Optimization complete!\n",
      "  Initial side: 3.150000\n",
      "  Final side:   3.035000\n",
      "  Improvement:  3.65%\n",
      "\n",
      "✅ SUCCESS: Growing Trees engine is working correctly!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the new Growing Trees engine\n",
    "print(\"Testing Growing Trees optimization engine...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Reload the module to get latest changes\n",
    "    import importlib\n",
    "    import optimization_utils\n",
    "    importlib.reload(optimization_utils)\n",
    "    from optimization_utils import sa_numba_growing\n",
    "    \n",
    "    print(\"✓ Module loaded successfully\")\n",
    "    \n",
    "    # Create a simple test case with 5 trees\n",
    "    test_trees = [\n",
    "        ChristmasTree(0, 0, 0),\n",
    "        ChristmasTree(1, 0, 45),\n",
    "        ChristmasTree(0, 1, 90),\n",
    "        ChristmasTree(-1, 0, 135),\n",
    "        ChristmasTree(0, -1, 180)\n",
    "    ]\n",
    "    \n",
    "    n = len(test_trees)\n",
    "    xs = np.array([t.center_x for t in test_trees], dtype=np.float64)\n",
    "    ys = np.array([t.center_y for t in test_trees], dtype=np.float64)\n",
    "    angs = np.array([t.angle for t in test_trees], dtype=np.float64)\n",
    "    \n",
    "    print(f\"✓ Test case created: {n} trees\")\n",
    "    \n",
    "    # Get initial bounding box\n",
    "    from optimization_utils import get_poly, get_bbox, calc_side_cached\n",
    "    cached_bboxes = np.zeros((n, 4), dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        px, py = get_poly(xs[i], ys[i], angs[i], 1.0)\n",
    "        cached_bboxes[i] = get_bbox(px, py)\n",
    "    \n",
    "    initial_side = calc_side_cached(cached_bboxes, n)\n",
    "    print(f\"✓ Initial bounding box side: {initial_side:.6f}\")\n",
    "    \n",
    "    # Run optimization with Growing Trees\n",
    "    print(\"\\nRunning sa_numba_growing (10k iterations)...\")\n",
    "    bxs, bys, bangs, best_side = sa_numba_growing(\n",
    "        xs, ys, angs, n,\n",
    "        iterations=10000,\n",
    "        T0=1.0,\n",
    "        Tmin=0.001,\n",
    "        move_scale=0.5,\n",
    "        rot_scale=30.0,\n",
    "        seed=42,\n",
    "        compression=0.05\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Optimization complete!\")\n",
    "    print(f\"  Initial side: {initial_side:.6f}\")\n",
    "    print(f\"  Final side:   {best_side:.6f}\")\n",
    "    \n",
    "    improvement = ((initial_side - best_side) / initial_side) * 100\n",
    "    print(f\"  Improvement:  {improvement:.2f}%\")\n",
    "    \n",
    "    if best_side < initial_side:\n",
    "        print(\"\\n✅ SUCCESS: Growing Trees engine is working correctly!\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  WARNING: No improvement, but engine executed without errors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958dcd5",
   "metadata": {},
   "source": [
    "## Performance Benchmark\n",
    "Benchmark `sa_numba_growing` across varying N and iteration counts to estimate throughput (trees*iterations per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fb9a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from statistics import mean\n",
    "\n",
    "def benchmark_growing_trees(sample_ns=(5, 20, 50, 100), iterations=5000, repeats=3):\n",
    "    print(f\"Benchmark: iterations={iterations}, repeats={repeats}\")\n",
    "    rows = []\n",
    "    for n in sample_ns:\n",
    "        # Random initial layout\n",
    "        rng = np.random.default_rng(n*17 + 42)\n",
    "        xs = rng.uniform(-2, 2, size=n)\n",
    "        ys = rng.uniform(-2, 2, size=n)\n",
    "        angs = rng.uniform(0, 360, size=n)\n",
    "        params = get_sa_params(n)\n",
    "        T0 = params['initial_temp']; Tmin = params['min_temp']\n",
    "        move_scale = params['move_scale']; rot_scale = params['rot_scale']\n",
    "        compression = params.get('compression', 0.05)\n",
    "\n",
    "        times = []\n",
    "        for r in range(repeats):\n",
    "            t0 = time.time()\n",
    "            sa_numba_growing(xs, ys, angs, n, iterations, T0, Tmin, move_scale, rot_scale, seed=r + n*100, compression=compression)\n",
    "            times.append(time.time() - t0)\n",
    "        avg = mean(times)\n",
    "        it_per_sec = iterations / avg\n",
    "        tree_ops = (iterations * n) / avg\n",
    "        rows.append((n, avg, it_per_sec, tree_ops))\n",
    "        print(f\"N={n:3d} avg_time={avg:.3f}s  iter/s={it_per_sec:7.0f}  tree-iter/s={tree_ops:8.0f}\")\n",
    "    return rows\n",
    "\n",
    "# Run benchmark (uncomment to execute):\n",
    "# benchmark_growing_trees(sample_ns=(10, 40, 80, 160), iterations=8000, repeats=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f5689c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying submission.csv score...\n",
      "\n",
      "Validating 200 configurations...\n",
      "\n",
      "============================================================\n",
      "Verified Score: 39.0134852079\n",
      "Configurations: 200/200\n",
      "\n",
      "⚠️  COLLISION WARNINGS (197 configs affected):\n",
      "   N=4: 6 collision(s)\n",
      "   N=5: 10 collision(s)\n",
      "   N=6: 14 collision(s)\n",
      "   N=7: 17 collision(s)\n",
      "   N=8: 10 collision(s)\n",
      "   N=9: 12 collision(s)\n",
      "   N=10: 16 collision(s)\n",
      "   N=11: 22 collision(s)\n",
      "   N=12: 28 collision(s)\n",
      "   N=13: 33 collision(s)\n",
      "   ... and 187 more\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick verification: Check actual score of submission.csv\n",
    "print(\"Verifying submission.csv score...\")\n",
    "try:\n",
    "    verify_df = pd.read_csv('submission.csv')\n",
    "    \n",
    "    def parse_s(val):\n",
    "        return float(str(val).replace('s', ''))\n",
    "    \n",
    "    verify_df['x'] = verify_df['x'].apply(parse_s)\n",
    "    verify_df['y'] = verify_df['y'].apply(parse_s)\n",
    "    verify_df['deg'] = verify_df['deg'].apply(parse_s)\n",
    "    \n",
    "    verify_score = 0\n",
    "    verify_configs = {}\n",
    "    collision_summary = {}\n",
    "    \n",
    "    for _, row in verify_df.iterrows():\n",
    "        n_str = row['id'].split('_')[0]\n",
    "        n = int(n_str)\n",
    "        if n not in verify_configs:\n",
    "            verify_configs[n] = []\n",
    "        verify_configs[n].append(ChristmasTree(row['x'], row['y'], row['deg']))\n",
    "    \n",
    "    print(f\"\\nValidating {len(verify_configs)} configurations...\")\n",
    "    for n, trees in sorted(verify_configs.items()):\n",
    "        if len(trees) != n:\n",
    "            print(f\"⚠️ WARNING: N={n} has {len(trees)} trees (expected {n})\")\n",
    "            continue\n",
    "        \n",
    "        # Validate no collisions\n",
    "        is_valid, coll_count = validate_no_collisions(trees, verbose=False)\n",
    "        if coll_count > 0:\n",
    "            collision_summary[n] = coll_count\n",
    "        \n",
    "        side = get_bounds(trees)\n",
    "        verify_score += (side ** 2) / n\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Verified Score: {verify_score:.10f}\")\n",
    "    print(f\"Configurations: {len(verify_configs)}/200\")\n",
    "    \n",
    "    if collision_summary:\n",
    "        print(f\"\\n⚠️  COLLISION WARNINGS ({len(collision_summary)} configs affected):\")\n",
    "        for n in sorted(collision_summary.keys())[:10]:  # Show first 10\n",
    "            print(f\"   N={n}: {collision_summary[n]} collision(s)\")\n",
    "        if len(collision_summary) > 10:\n",
    "            print(f\"   ... and {len(collision_summary) - 10} more\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No collisions detected\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"submission.csv not found. Run the main processing cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8122a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating final score from submission.csv...\n",
      "Calculating scores for 200 configurations...\n",
      "\n",
      "============================================================\n",
      "Final Score: 39.0134852079\n",
      "Configurations found: 200/200\n",
      "\n",
      "⚠️  COLLISION ISSUES DETECTED!\n",
      "   197 configs with 82238 total collisions\n",
      "   Affected configs: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "   ... and 182 more\n",
      "\n",
      "   ⚠️  These collisions will cause Kaggle submission FAILURE!\n",
      "============================================================\n",
      "\n",
      "Original Score: 74.6567843348\n",
      "✓ SUCCESS: Score improved by 35.6432991269!\n",
      "\n",
      "⚠️  WARNING: Cannot submit due to 197 collision issues\n",
      "   Re-run optimization with longer iterations or stricter validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# Calculate final score from submission.csv directly (most reliable source)\n",
    "print(\"Calculating final score from submission.csv...\")\n",
    "try:\n",
    "    final_df = pd.read_csv('submission.csv')\n",
    "    \n",
    "    def parse_s(val):\n",
    "        return float(str(val).replace('s', ''))\n",
    "    \n",
    "    final_df['x'] = final_df['x'].apply(parse_s)\n",
    "    final_df['y'] = final_df['y'].apply(parse_s)\n",
    "    final_df['deg'] = final_df['deg'].apply(parse_s)\n",
    "    \n",
    "    final_score = 0\n",
    "    score_by_n = {}\n",
    "    collision_summary = {}\n",
    "    \n",
    "    for _, row in final_df.iterrows():\n",
    "        n_str = row['id'].split('_')[0]\n",
    "        n = int(n_str)\n",
    "        if n not in score_by_n:\n",
    "            score_by_n[n] = []\n",
    "        score_by_n[n].append(ChristmasTree(row['x'], row['y'], row['deg']))\n",
    "    \n",
    "    print(f\"Calculating scores for {len(score_by_n)} configurations...\")\n",
    "    \n",
    "    for n in sorted(score_by_n.keys()):\n",
    "        trees = score_by_n[n]\n",
    "        if len(trees) != n:\n",
    "            print(f\"⚠️ WARNING: N={n} has {len(trees)} trees, expected {n}\")\n",
    "            continue\n",
    "        \n",
    "        # Validate collisions\n",
    "        is_valid, coll_count = validate_no_collisions(trees, verbose=False)\n",
    "        if coll_count > 0:\n",
    "            collision_summary[n] = coll_count\n",
    "        \n",
    "        side = get_bounds(trees)\n",
    "        score_n = (side ** 2) / n\n",
    "        final_score += score_n\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final Score: {final_score:.10f}\")\n",
    "    print(f\"Configurations found: {len(score_by_n)}/200\")\n",
    "    \n",
    "    if collision_summary:\n",
    "        total_collisions = sum(collision_summary.values())\n",
    "        print(f\"\\n⚠️  COLLISION ISSUES DETECTED!\")\n",
    "        print(f\"   {len(collision_summary)} configs with {total_collisions} total collisions\")\n",
    "        print(f\"   Affected configs: {sorted(collision_summary.keys())[:15]}\")\n",
    "        if len(collision_summary) > 15:\n",
    "            print(f\"   ... and {len(collision_summary) - 15} more\")\n",
    "        print(f\"\\n   ⚠️  These collisions will cause Kaggle submission FAILURE!\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No collisions detected - submission is valid\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Compare with original\n",
    "    if 'total_test_score' in globals():\n",
    "        print(f\"Original Score: {total_test_score:.10f}\")\n",
    "        if final_score < total_test_score:\n",
    "            diff = total_test_score - final_score\n",
    "            print(f\"✓ SUCCESS: Score improved by {diff:.10f}!\")\n",
    "            \n",
    "            if collision_summary:\n",
    "                print(f\"\\n⚠️  WARNING: Cannot submit due to {len(collision_summary)} collision issues\")\n",
    "                print(\"   Re-run optimization with longer iterations or stricter validation\")\n",
    "            else:\n",
    "                # 1. Save copy with detailed name\n",
    "                os.makedirs('Data', exist_ok=True)\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                detailed_name = f\"Data/submission_score{final_score:.2f}_improved{diff:.2f}_{timestamp}.csv\"\n",
    "                final_df.to_csv(detailed_name, index=False)\n",
    "                print(f\"\\nSaved backup: {detailed_name}\")\n",
    "                \n",
    "                # 2. Overwrite test.csv\n",
    "                final_df.to_csv('test.csv', index=False)\n",
    "                print(\"Overwrote test.csv\")\n",
    "                \n",
    "                # 3. Submit to Kaggle\n",
    "                message = f\"Improved score {final_score:.6f} (was {total_test_score:.6f})\"\n",
    "                print(\"Submitting to Kaggle...\")\n",
    "                !kaggle competitions submit -c santa-2025 -f submission.csv -m \"{message}\"\n",
    "\n",
    "                # 4. Git Commit and Push\n",
    "                print(\"Committing and pushing to Git...\")\n",
    "                !git add .\n",
    "                !git commit -m \"{message}\"\n",
    "                !git push\n",
    "            \n",
    "        else:\n",
    "            improvement = final_score - total_test_score\n",
    "            print(f\"⚠ No improvement (Score change: {improvement:+.10f})\")\n",
    "            print(f\"  Current: {final_score:.10f}\")\n",
    "            print(f\"  Original: {total_test_score:.10f}\")\n",
    "    else:\n",
    "        print(\"⚠ Original score not found. Run the cell loading test.csv baseline first.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"submission.csv not found. Run the main processing cell first.\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a41c63",
   "metadata": {},
   "source": [
    "## 🔍 CRITICAL DIAGNOSIS: Score Discrepancy Analysis\n",
    "\n",
    "**PROBLEM**: Local score (44.79) vs Kaggle score (~75) = 30 point difference\n",
    "\n",
    "**HYPOTHESIS**: Massive collision penalties on Kaggle side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1504345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE COLLISION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Checking 200 configurations...\n",
      "\n",
      "  N=  4: Trees 0-1 overlap (area=6.0852e-02)\n",
      "  N=  5: Trees 0-1 overlap (area=6.0852e-02)\n",
      "  N=  6: Trees 0-1 overlap (area=6.0852e-02)\n",
      "  N=  7: Trees 0-1 overlap (area=6.0852e-02)\n",
      "  N=  8: Trees 0-1 overlap (area=6.0852e-02)\n",
      "  N=  9: Trees 0-1 overlap (area=6.0852e-02)\n",
      "  N= 10: Trees 0-1 overlap (area=6.0852e-02)\n",
      "  N= 11: Trees 0-1 overlap (area=6.0852e-02)\n",
      "  N= 12: Trees 0-1 overlap (area=6.0852e-02)\n",
      "  N= 13: Trees 0-1 overlap (area=6.0852e-02)\n",
      "\n",
      "======================================================================\n",
      "COLLISION SUMMARY:\n",
      "  Configurations with collisions: 197/200\n",
      "  Total collision pairs: 82238\n",
      "======================================================================\n",
      "\n",
      "WORST OFFENDERS (Top 20):\n",
      "  N=200:  875 collisions\n",
      "  N=199:  870 collisions\n",
      "  N=198:  865 collisions\n",
      "  N=197:  859 collisions\n",
      "  N=196:  852 collisions\n",
      "  N=195:  845 collisions\n",
      "  N=194:  838 collisions\n",
      "  N=193:  831 collisions\n",
      "  N=192:  824 collisions\n",
      "  N=191:  817 collisions\n",
      "  N=190:  810 collisions\n",
      "  N=189:  803 collisions\n",
      "  N=188:  796 collisions\n",
      "  N=187:  789 collisions\n",
      "  N=186:  782 collisions\n",
      "  N=185:  775 collisions\n",
      "  N=184:  768 collisions\n",
      "  N=183:  761 collisions\n",
      "  N=182:  756 collisions\n",
      "  N=181:  753 collisions\n",
      "\n",
      "⚠️  KAGGLE PENALTY ESTIMATE:\n",
      "   If Kaggle adds +0.05 penalty per collision:\n",
      "   Penalty = 82238 × 0.05 = 4111.90\n",
      "   Expected Kaggle score = 44.79 + 4111.90 = 4156.69\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC: Validate submission.csv for collisions\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE COLLISION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    diag_df = pd.read_csv('submission.csv')\n",
    "    diag_df['x'] = diag_df['x'].apply(lambda v: float(str(v).replace('s', '')))\n",
    "    diag_df['y'] = diag_df['y'].apply(lambda v: float(str(v).replace('s', '')))\n",
    "    diag_df['deg'] = diag_df['deg'].apply(lambda v: float(str(v).replace('s', '')))\n",
    "    \n",
    "    diag_configs = {}\n",
    "    for _, row in diag_df.iterrows():\n",
    "        n = int(row['id'].split('_')[0])\n",
    "        if n not in diag_configs:\n",
    "            diag_configs[n] = []\n",
    "        diag_configs[n].append(ChristmasTree(row['x'], row['y'], row['deg']))\n",
    "    \n",
    "    total_collisions = 0\n",
    "    configs_with_collisions = 0\n",
    "    worst_offenders = []\n",
    "    \n",
    "    print(f\"\\nChecking {len(diag_configs)} configurations...\\n\")\n",
    "    \n",
    "    for n in sorted(diag_configs.keys()):\n",
    "        trees = diag_configs[n]\n",
    "        is_valid, coll_count = validate_no_collisions(trees, verbose=False)\n",
    "        \n",
    "        if coll_count > 0:\n",
    "            configs_with_collisions += 1\n",
    "            total_collisions += coll_count\n",
    "            worst_offenders.append((n, coll_count))\n",
    "            \n",
    "            # Show first collision pair for this config\n",
    "            if len(worst_offenders) <= 10:\n",
    "                for i in range(len(trees)):\n",
    "                    for j in range(i+1, len(trees)):\n",
    "                        if trees[i].polygon.intersects(trees[j].polygon):\n",
    "                            area = trees[i].polygon.intersection(trees[j].polygon).area\n",
    "                            print(f\"  N={n:3d}: Trees {i}-{j} overlap (area={area:.4e})\")\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "    \n",
    "    worst_offenders.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COLLISION SUMMARY:\")\n",
    "    print(f\"  Configurations with collisions: {configs_with_collisions}/200\")\n",
    "    print(f\"  Total collision pairs: {total_collisions}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if worst_offenders:\n",
    "        print(f\"\\nWORST OFFENDERS (Top 20):\")\n",
    "        for n, coll in worst_offenders[:20]:\n",
    "            print(f\"  N={n:3d}: {coll:4d} collisions\")\n",
    "        \n",
    "        print(f\"\\n⚠️  KAGGLE PENALTY ESTIMATE:\")\n",
    "        print(f\"   If Kaggle adds +0.05 penalty per collision:\")\n",
    "        print(f\"   Penalty = {total_collisions} × 0.05 = {total_collisions * 0.05:.2f}\")\n",
    "        print(f\"   Expected Kaggle score = 44.79 + {total_collisions * 0.05:.2f} = {44.79 + total_collisions * 0.05:.2f}\")\n",
    "    else:\n",
    "        print(\"\\n✓ NO COLLISIONS - Submission is valid!\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e8158",
   "metadata": {},
   "source": [
    "# Simulated Annealing (SA) Tips\n",
    "\n",
    "Simulated annealing is a versatile metaheuristic used widely in heuristic competitions (e.g., AtCoder Heuristic Contests). Here are practical, reusable tips:\n",
    "\n",
    "- Make the loop fast: more iterations yield better solutions. Prototype in Python, but move hot loops to faster paths (Numba, vectorized numpy/shapely ops, or C++/Rust) when chasing leaderboard performance.\n",
    "- Avoid recomputation: after moving one tree `i`, only recheck pairs involving `i` (Θ(N) vs Θ(N²)). With N=200, that’s ~100× fewer checks.\n",
    "- Temperature matters:\n",
    "  - High `T` encourages exploration; accept worse moves with nontrivial probability.\n",
    "  - Low `T` focuses on fine-tuning near a good state.\n",
    "- Cooling schedule: geometric schedules are robust. For progress `p ∈ [0,1]`, initial `T0`, final `T1`:\n",
    "  - `T(p) = T0 * (T1 / T0) ** p`\n",
    "  - Acceptance probability for uphill move `Δ > 0`: `P = exp(-Δ / T)`\n",
    "- Design good neighborhoods: mix move types (translate, rotate, swap), adaptive step sizes, and occasional larger jumps to escape plateaus.\n",
    "- Track best-so-far: keep a separate best solution; the final state isn’t guaranteed to be the best visited.\n",
    "- Multi-run: try multiple random seeds and take the best.\n",
    "\n",
    "Below, we add small helper functions to prototype schedules and acceptance behavior, plus a submission validator to prevent failed uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5afee8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric schedule samples:\n",
      " p=0.00 → T=1.000000\n",
      " p=0.20 → T=0.251189\n",
      " p=0.40 → T=0.063096\n",
      " p=0.60 → T=0.015849\n",
      " p=0.80 → T=0.003981\n",
      " p=1.00 → T=0.001000\n",
      "\n",
      "Acceptance samples (Δ=0.01,0.1,1.0 at mid temperature):\n",
      " Δ=0.01 at T=0.031623 → P=0.728893\n",
      " Δ=0.10 at T=0.031623 → P=0.042329\n",
      " Δ=1.00 at T=0.031623 → P=0.000000\n"
     ]
    }
   ],
   "source": [
    "# SA helpers: geometric schedule and acceptance demo\n",
    "import math\n",
    "from typing import Iterable\n",
    "\n",
    "def geometric_temperature(p: float, T0: float, T1: float) -> float:\n",
    "    \"\"\"Geometric schedule: T(p) = T0 * (T1/T0)**p, where p ∈ [0,1].\"\"\"\n",
    "    p = min(max(p, 0.0), 1.0)\n",
    "    return T0 * ((T1 / T0) ** p)\n",
    "\n",
    "def acceptance_probability(delta: float, T: float) -> float:\n",
    "    \"\"\"Return exp(-Δ/T) for Δ>0, else accept with prob 1.\"\"\"\n",
    "    if delta <= 0:\n",
    "        return 1.0\n",
    "    if T <= 1e-15:\n",
    "        return 0.0\n",
    "    return math.exp(-delta / T)\n",
    "\n",
    "def demo_schedule(T0: float = 1.0, T1: float = 1e-3, steps: int = 5) -> None:\n",
    "    print(\"Geometric schedule samples:\")\n",
    "    for k in range(steps + 1):\n",
    "        p = k / steps\n",
    "        print(f\" p={p:0.2f} → T={geometric_temperature(p, T0, T1):.6f}\")\n",
    "    print()\n",
    "    print(\"Acceptance samples (Δ=0.01,0.1,1.0 at mid temperature):\")\n",
    "    Tmid = geometric_temperature(0.5, T0, T1)\n",
    "    for delta in (0.01, 0.1, 1.0):\n",
    "        print(f\" Δ={delta:0.2f} at T={Tmid:.6f} → P={acceptance_probability(delta, Tmid):.6f}\")\n",
    "\n",
    "demo_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b5d8fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating submission file: submission.csv\n",
      "✗ {'x': 20100, 'y': 20100, 'deg': 20100} values missing 's' suffix.\n",
      "  Example offending rows:\n",
      "      id               x               y              deg\n",
      "0  001_0   s0.1590990258  s-0.1590990258   s45.0000000000\n",
      "1  002_0  s-0.1540970283  s-0.2614593508   s23.6293826015\n",
      "2  002_1   s0.1540969086   s0.2614593526  s203.6293919594\n",
      "3  003_0   s0.3577227974   s0.2503589809   s66.3704810000\n",
      "4  003_1  s-0.2346198622   s0.1548201282  s155.1341740000\n",
      "Submission has issues\n"
     ]
    }
   ],
   "source": [
    "# Submission Validator: format and collision checks\n",
    "import pandas as pd\n",
    "\n",
    "def validate_submission(path: str = 'submission.csv') -> bool:\n",
    "    print(f\"Validating submission file: {path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to read CSV: {e}\")\n",
    "        return False\n",
    "    \n",
    "    required_cols = {'id', 'x', 'y', 'deg'}\n",
    "    if set(df.columns) != required_cols:\n",
    "        print(f\"✗ Columns mismatch. Found: {list(df.columns)} Expected: {sorted(required_cols)}\")\n",
    "        return False\n",
    "    \n",
    "    # Values must end with 's' per competition format (string values like '12.34s')\n",
    "    bad_suffix = {}\n",
    "    for col in ['x', 'y', 'deg']:\n",
    "        bad = df[~df[col].astype(str).str.endswith('s')]\n",
    "        if len(bad) > 0:\n",
    "            bad_suffix[col] = len(bad)\n",
    "    if bad_suffix:\n",
    "        print(f\"✗ {bad_suffix} values missing 's' suffix.\")\n",
    "        print(\"  Example offending rows:\")\n",
    "        print(df.iloc[:5])\n",
    "        return False\n",
    "    \n",
    "    # Check `id` structure and per-N counts\n",
    "    try:\n",
    "        df['n'] = df['id'].str.split('_').str[0].astype(int)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Unable to parse 'id' to extract N: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check there are exactly 200 configurations and each has N rows\n",
    "    ns = sorted(df['n'].unique())\n",
    "    missing_ns = [n for n in range(1, 201) if n not in ns]\n",
    "    if missing_ns:\n",
    "        print(f\"✗ Missing configurations for N={missing_ns[:10]}{' ...' if len(missing_ns)>10 else ''}\")\n",
    "        return False\n",
    "    \n",
    "    wrong_counts = [(n, c) for n, c in df['n'].value_counts().items() if c != n]\n",
    "    if wrong_counts:\n",
    "        print(\"✗ Per-N row counts incorrect (expected N rows per N):\")\n",
    "        print(wrong_counts[:10])\n",
    "        return False\n",
    "    \n",
    "    # Duplicate IDs\n",
    "    dup_count = int(df['id'].duplicated().sum())\n",
    "    if dup_count > 0:\n",
    "        print(f\"✗ {dup_count} duplicated IDs found.\")\n",
    "        return False\n",
    "    \n",
    "    # Collision + score verification (uses existing helpers)\n",
    "    def parse_s(val):\n",
    "        return float(str(val).replace('s', ''))\n",
    "    df['x_f'] = df['x'].apply(parse_s)\n",
    "    df['y_f'] = df['y'].apply(parse_s)\n",
    "    df['deg_f'] = df['deg'].apply(parse_s)\n",
    "    \n",
    "    verify_configs = {}\n",
    "    for _, row in df.iterrows():\n",
    "        n = int(row['n'])\n",
    "        verify_configs.setdefault(n, []).append(ChristmasTree(row['x_f'], row['y_f'], row['deg_f']))\n",
    "    \n",
    "    collision_summary = {}\n",
    "    verify_score = 0.0\n",
    "    for n, trees in sorted(verify_configs.items()):\n",
    "        # Validate no collisions\n",
    "        is_valid, coll_count = validate_no_collisions(trees, verbose=False)\n",
    "        if coll_count > 0:\n",
    "            collision_summary[n] = coll_count\n",
    "        side = get_bounds(trees)\n",
    "        verify_score += (side ** 2) / n\n",
    "    \n",
    "    print(\"✓ Basic format checks passed.\")\n",
    "    print(f\"Verified Score: {verify_score:.10f}\")\n",
    "    if collision_summary:\n",
    "        print(f\"⚠️ Collision warnings in {len(collision_summary)} configs (showing up to 10):\")\n",
    "        for n in sorted(collision_summary.keys())[:10]:\n",
    "            print(f\"   N={n}: {collision_summary[n]} collision(s)\")\n",
    "    else:\n",
    "        print(\"✓ No collisions detected.\")\n",
    "    \n",
    "    return len(collision_summary) == 0\n",
    "    \n",
    "# Run validator on default file\n",
    "_ok = validate_submission('submission.csv')\n",
    "print(\"Submission looks good\" if _ok else \"Submission has issues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
